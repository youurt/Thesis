% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.1 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{none/global//global/global}
    \entry{Rosenblatt}{report}{}
      \name{author}{1}{}{%
        {{hash=1750cd87a34d44119e7a4aab9b8012c6}{%
           family={Rosenblatt},
           familyi={R\bibinitperiod},
           given={F},
           giveni={F\bibinitperiod}}}%
      }
      \strng{namehash}{1750cd87a34d44119e7a4aab9b8012c6}
      \strng{fullhash}{1750cd87a34d44119e7a4aab9b8012c6}
      \strng{bibnamehash}{1750cd87a34d44119e7a4aab9b8012c6}
      \strng{authorbibnamehash}{1750cd87a34d44119e7a4aab9b8012c6}
      \strng{authornamehash}{1750cd87a34d44119e7a4aab9b8012c6}
      \strng{authorfullhash}{1750cd87a34d44119e7a4aab9b8012c6}
      \field{sortinit}{1}
      \field{sortinithash}{50c6687d7fc80f50136d75228e3c59ba}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Psychological Review}
      \field{number}{6}
      \field{title}{{THE PERCEPTRON: A PROBABILISTIC MODEL FOR INFORMATION STORAGE AND ORGANIZATION IN THE BRAIN 1}}
      \field{type}{techreport}
      \field{volume}{65}
      \field{pages}{19\bibrangedash 27}
      \range{pages}{9}
    \endentry
    \entry{Werbos1990}{article}{}
      \name{author}{1}{}{%
        {{hash=3f6ba0fa8bc8542632efe7f31c9ee2e8}{%
           family={Werbos},
           familyi={W\bibinitperiod},
           given={Paul\bibnamedelima J.},
           giveni={P\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \strng{namehash}{3f6ba0fa8bc8542632efe7f31c9ee2e8}
      \strng{fullhash}{3f6ba0fa8bc8542632efe7f31c9ee2e8}
      \strng{bibnamehash}{3f6ba0fa8bc8542632efe7f31c9ee2e8}
      \strng{authorbibnamehash}{3f6ba0fa8bc8542632efe7f31c9ee2e8}
      \strng{authornamehash}{3f6ba0fa8bc8542632efe7f31c9ee2e8}
      \strng{authorfullhash}{3f6ba0fa8bc8542632efe7f31c9ee2e8}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Backpropagation is now the most widely used tool in the field of artificial neural networks. At the core of backpropagation is a method for calculating derivatives exactly and efficiently in any large system made up of elementary subsystems or calculations which are represented by known, differentiable functions; thus, backpropagation has many applications which do not involve neural networks as such. This paper first reviews basic backpropagation, a simple method which is now being widely used in areas like pattern recognition and fault diagnosis. Next, it presents the basic equations for back-propagation through time, and discusses applications to areas like pattern recognition involving dynamic systems, systems identification, and control. Finally, it describes further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations or true recurrent networks, and other practical issues which arise with this method. Pseudocode is provided to clarify the algorithms. The chain rule forordered derivatives—the theorem which underlies backpropagation—is briefly discussed. {©} 1990, IEEE}
      \field{issn}{15582256}
      \field{journaltitle}{Proceedings of the IEEE}
      \field{number}{10}
      \field{title}{{Backpropagation Through Time: What It Does and How to Do It}}
      \field{volume}{78}
      \field{year}{1990}
      \field{pages}{1550\bibrangedash 1560}
      \range{pages}{11}
      \verb{doi}
      \verb 10.1109/5.58337
      \endverb
    \endentry
    \entry{Hinton}{report}{}
      \name{author}{2}{}{%
        {{hash=813bd95fe553e6079cd53a567b238287}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey\bibnamedelima E},
           giveni={G\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=643f92e8f89f2746a4c1aa077d225755}{%
           family={Osindero},
           familyi={O\bibinitperiod},
           given={Simon},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{cdfb09be61c835ae48863e4d278a5539}
      \strng{fullhash}{cdfb09be61c835ae48863e4d278a5539}
      \strng{bibnamehash}{cdfb09be61c835ae48863e4d278a5539}
      \strng{authorbibnamehash}{cdfb09be61c835ae48863e4d278a5539}
      \strng{authornamehash}{cdfb09be61c835ae48863e4d278a5539}
      \strng{authorfullhash}{cdfb09be61c835ae48863e4d278a5539}
      \field{sortinit}{3}
      \field{sortinithash}{a37a8ef248a93c322189792c34fc68c9}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associa-tive memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.}
      \field{title}{{A Fast Learning Algorithm for Deep Belief Nets Yee-Whye Teh}}
      \field{type}{techreport}
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Hinton, Osindero - Unknown - A Fast Learning Algorithm for Deep Belief Nets Yee-Whye Teh.pdf:pdf
      \endverb
    \endentry
    \entry{Schmidhuber2014}{report}{}
      \name{author}{1}{}{%
        {{hash=288bdbcfe1b91ad7484d7a24f74f99ed}{%
           family={Schmidhuber},
           familyi={S\bibinitperiod},
           given={Jürgen},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{288bdbcfe1b91ad7484d7a24f74f99ed}
      \strng{fullhash}{288bdbcfe1b91ad7484d7a24f74f99ed}
      \strng{bibnamehash}{288bdbcfe1b91ad7484d7a24f74f99ed}
      \strng{authorbibnamehash}{288bdbcfe1b91ad7484d7a24f74f99ed}
      \strng{authornamehash}{288bdbcfe1b91ad7484d7a24f74f99ed}
      \strng{authorfullhash}{288bdbcfe1b91ad7484d7a24f74f99ed}
      \field{sortinit}{4}
      \field{sortinithash}{e071e0bcb44634fab398d68ad04e69f4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks. LATEX source: Preface This is the preprint of an invited Deep Learning (DL) overview. One of its goals is to assign credit to those who contributed to the present state of the art. I acknowledge the limitations of attempting to achieve this goal. The DL research community itself may be viewed as a continually evolving, deep network of scientists who have influenced each other in complex ways. Starting from recent DL results, I tried to trace back the origins of relevant ideas through the past half century and beyond, sometimes using "local search" to follow citations of citations backwards in time. Since not all DL publications properly acknowledge earlier relevant work, additional global search strategies were employed , aided by consulting numerous neural network experts. As a result, the present preprint mostly consists of references. Nevertheless, through an expert selection bias I may have missed important work. A related bias was surely introduced by my special familiarity with the work of my own DL research group in the past quarter-century. For these reasons, this work should be viewed as merely a snapshot of an ongoing credit assignment process. To help improve it, please do not hesitate to send corrections and suggestions to juergen@idsia.ch.}
      \field{eprinttype}{arXiv}
      \field{title}{{Deep Learning in Neural Networks: An Overview}}
      \field{type}{techreport}
      \field{year}{2014}
      \verb{eprint}
      \verb 1404.7828v4
      \endverb
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Schmidhuber - 2014 - Deep Learning in Neural Networks An Overview.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb http://www.idsia.ch/˜juergen/DeepLearning8Oct2014.texCompleteBIBTEXfile
      \endverb
      \verb{url}
      \verb http://www.idsia.ch/%CB%9Cjuergen/DeepLearning8Oct2014.texCompleteBIBTEXfile
      \endverb
    \endentry
    \entry{Rumelhart1986}{article}{}
      \name{author}{3}{}{%
        {{hash=55cd5380bb5b15032a6d5a2015f56e3f}{%
           family={Rumelhart},
           familyi={R\bibinitperiod},
           given={David\bibnamedelima E.},
           giveni={D\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=813bd95fe553e6079cd53a567b238287}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey\bibnamedelima E.},
           giveni={G\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=6cbc29ad7fd57ffdb9ed4728418fd988}{%
           family={Williams},
           familyi={W\bibinitperiod},
           given={Ronald\bibnamedelima J.},
           giveni={R\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \strng{namehash}{fd75cea68d6982b503a0cd84f7cc7b51}
      \strng{fullhash}{fd75cea68d6982b503a0cd84f7cc7b51}
      \strng{bibnamehash}{fd75cea68d6982b503a0cd84f7cc7b51}
      \strng{authorbibnamehash}{fd75cea68d6982b503a0cd84f7cc7b51}
      \strng{authornamehash}{fd75cea68d6982b503a0cd84f7cc7b51}
      \strng{authorfullhash}{fd75cea68d6982b503a0cd84f7cc7b51}
      \field{sortinit}{6}
      \field{sortinithash}{7851c86048328b027313775d8fbd2131}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure 1. {©} 1986 Nature Publishing Group.}
      \field{issn}{00280836}
      \field{journaltitle}{Nature}
      \field{number}{6088}
      \field{title}{{Learning representations by back-propagating errors}}
      \field{volume}{323}
      \field{year}{1986}
      \verb{doi}
      \verb 10.1038/323533a0
      \endverb
    \endentry
    \entry{Herculano-Houzel2009}{misc}{}
      \name{author}{1}{}{%
        {{hash=844747f22b37db160509d1c43124abcb}{%
           family={Herculano-Houzel},
           familyi={H\bibinithyphendelim H\bibinitperiod},
           given={Suzana},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{844747f22b37db160509d1c43124abcb}
      \strng{fullhash}{844747f22b37db160509d1c43124abcb}
      \strng{bibnamehash}{844747f22b37db160509d1c43124abcb}
      \strng{authorbibnamehash}{844747f22b37db160509d1c43124abcb}
      \strng{authornamehash}{844747f22b37db160509d1c43124abcb}
      \strng{authorfullhash}{844747f22b37db160509d1c43124abcb}
      \field{sortinit}{7}
      \field{sortinithash}{f615fb9c6fba11c6f962fb3fd599810e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The human brain has often been viewed as outstanding among mammalian brains: the most cognitively able, the largest-than-expected from body size, endowed with an overdeveloped cerebral cortex that represents over 80{\%} of brain mass, and purportedly containing 100 billion neurons and 10× more glial cells. Such uniqueness was seemingly necessary to justify the superior cognitive abilities of humans over larger-brained mammals such as elephants and whales. However, our recent studies using a novel method to determine the cellular composition of the brain of humans and other primates as well as of rodents and insectivores show that, since different cellular scaling rules apply to the brains within these orders, brain size can no longer be considered a proxy for the number of neurons in the brain. These studies also showed that the human brain is not exceptional in its cellular composition, as it was found to contain as many neuronal and non-neuronal cells as would be expected of a primate brain of its size. Additionally, the so-called overdeveloped human cerebral cortex holds only 19{\%} of all brain neurons, a fraction that is similar to that found in other mammals. In what regards absolute numbers of neurons, however, the human brain does have two advantages compared to other mammalian brains: compared to rodents, and probably to whales and elephants as well, it is built according to the very economical, space-saving scaling rules that apply to other primates; and, among economically built primate brains, it is the largest, hence containing the most neurons. These findings argue in favor of a view of cognitive abilities that is centered on absolute numbers of neurons, rather than on body size or encephalization, and call for a re-examination of several concepts related to the exceptionality of the human brain. {©} 2009 Herculano-Houzel.}
      \field{booktitle}{Frontiers in Human Neuroscience}
      \field{issn}{16625161}
      \field{number}{NOV}
      \field{title}{{The human brain in numbers: A linearly scaled-up primate brain}}
      \field{volume}{3}
      \field{year}{2009}
      \verb{doi}
      \verb 10.3389/neuro.09.031.2009
      \endverb
    \endentry
    \entry{Nielsen2015}{article}{}
      \name{author}{1}{}{%
        {{hash=17279793dcf098cfda8570111ee5cfd9}{%
           family={Nielsen},
           familyi={N\bibinitperiod},
           given={MA},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{17279793dcf098cfda8570111ee5cfd9}
      \strng{fullhash}{17279793dcf098cfda8570111ee5cfd9}
      \strng{bibnamehash}{17279793dcf098cfda8570111ee5cfd9}
      \strng{authorbibnamehash}{17279793dcf098cfda8570111ee5cfd9}
      \strng{authornamehash}{17279793dcf098cfda8570111ee5cfd9}
      \strng{authorfullhash}{17279793dcf098cfda8570111ee5cfd9}
      \field{sortinit}{8}
      \field{sortinithash}{1b24cab5087933ef0826a7cd3b99e994}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{{Neural networks and deep learning}}
      \field{year}{2015}
    \endentry
    \entry{Taylor2017}{book}{}
      \name{author}{1}{}{%
        {{hash=5efe73c0ef8f0e873d520c4ea41f807e}{%
           family={Taylor},
           familyi={T\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{5efe73c0ef8f0e873d520c4ea41f807e}
      \strng{fullhash}{5efe73c0ef8f0e873d520c4ea41f807e}
      \strng{bibnamehash}{5efe73c0ef8f0e873d520c4ea41f807e}
      \strng{authorbibnamehash}{5efe73c0ef8f0e873d520c4ea41f807e}
      \strng{authornamehash}{5efe73c0ef8f0e873d520c4ea41f807e}
      \strng{authorfullhash}{5efe73c0ef8f0e873d520c4ea41f807e}
      \field{sortinit}{9}
      \field{sortinithash}{54047ffb55bdefa0694bbd554c1b11a0}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{{The Math of Neural Networks}}
      \field{year}{2017}
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Taylor - 2017 - The Math of Neural Networks.pdf:pdf
      \endverb
    \endentry
    \entry{Chollet2017}{book}{}
      \name{author}{1}{}{%
        {{hash=5836fc1fa037e340c8b5591da3207608}{%
           family={Chollet},
           familyi={C\bibinitperiod},
           given={Francois},
           giveni={F\bibinitperiod}}}%
      }
      \strng{namehash}{5836fc1fa037e340c8b5591da3207608}
      \strng{fullhash}{5836fc1fa037e340c8b5591da3207608}
      \strng{bibnamehash}{5836fc1fa037e340c8b5591da3207608}
      \strng{authorbibnamehash}{5836fc1fa037e340c8b5591da3207608}
      \strng{authornamehash}{5836fc1fa037e340c8b5591da3207608}
      \strng{authorfullhash}{5836fc1fa037e340c8b5591da3207608}
      \field{sortinit}{1}
      \field{sortinithash}{50c6687d7fc80f50136d75228e3c59ba}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{isbn}{9780996452762}
      \field{title}{{Deep learning with Python}}
      \field{year}{2017}
      \verb{doi}
      \verb 10.23919/ICIF.2018.8455530
      \endverb
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Chollet - 2017 - Deep learning with Python.pdf:pdf
      \endverb
    \endentry
    \entry{AntonioGuili;AmitaKapoor;SujitPal2019}{book}{}
      \name{author}{1}{}{%
        {{hash=5be3f653849a463589c05d6840a52369}{%
           family={{Antonio Guili; Amita Kapoor; Sujit Pal}},
           familyi={A\bibinitperiod}}}%
      }
      \strng{namehash}{5be3f653849a463589c05d6840a52369}
      \strng{fullhash}{5be3f653849a463589c05d6840a52369}
      \strng{bibnamehash}{5be3f653849a463589c05d6840a52369}
      \strng{authorbibnamehash}{5be3f653849a463589c05d6840a52369}
      \strng{authornamehash}{5be3f653849a463589c05d6840a52369}
      \strng{authorfullhash}{5be3f653849a463589c05d6840a52369}
      \field{sortinit}{1}
      \field{sortinithash}{50c6687d7fc80f50136d75228e3c59ba}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{2nd ed. Predicting house price using linear regression Deep Learning with TensorFlow 2 and Keras, Second Edition teaches deep learning techniques alongside TensorFlow (TF) and Keras. The book introduces neural networks with TensorFlow, runs through the main applications, covers two working example apps, and then dives into TF and cloudin production, TF mobile, and using TensorFlow with AutoML. Cover -- Copyright -- Packt Page -- Contributors -- Table of Contents -- Preface -- Chapter 1: Neural Network Foundations with TensorFlow 2.0 -- What is TensorFlow (TF)? -- What is Keras? -- What are the most important changes in TensorFlow 2.0? -- Introduction to neural networks -- Perceptron -- A first example of TensorFlow 2.0 code -- Multi-layer perceptron -- our first example of a network -- Problems in training the perceptron and their solutions -- Activation function -- sigmoid -- Activation function -- tanh -- Activation function -- ReLU Two additional activation functions -- ELU and LeakyReLU -- Activation functions -- In short -- what are neural networks after all? -- A real example -- recognizing handwritten digits -- One-hot encoding (OHE) -- Defining a simple neural network in TensorFlow 2.0 -- Running a simple TensorFlow 2.0 net and establishing a baseline -- Improving the simple net in TensorFlow 2.0 with hidden layers -- Further improving the simple net in TensorFlow with Dropout -- Testing different optimizers in TensorFlow 2.0 -- Increasing the number of epochs -- Controlling the optimizer learning rate Increasing the number of internal hidden neurons -- Increasing the size of batch computation -- Summarizing experiments run for recognizing handwritten charts -- Regularization -- Adopting regularization to avoid overfitting -- Understanding BatchNormalization -- Playing with Google Colab -- CPUs, GPUs, and TPUs -- Sentiment analysis -- Hyperparameter tuning and AutoML -- Predicting output -- A practical overview of backpropagation -- What have we learned so far? -- Towards a deep learning approach -- References -- Chapter 2: TensorFlow 1.x and 2.x -- Understanding TensorFlow 1.x TensorFlow 1.x computational graph program structure -- Computational graphs -- Working with constants, variables, and placeholders -- Examples of operations -- Constants -- Sequences -- Random tensors -- Variables -- An example of TensorFlow 1.x in TensorFlow 2.x -- Understanding TensorFlow 2.x -- Eager execution -- AutoGraph -- Keras APIs -- three programming models -- Sequential API -- Functional API -- Model subclassing -- Callbacks -- Saving a model and weights -- Training from tf.data.datasets -- tf.keras or Estimators? -- Ragged tensors -- Custom training Distributed training in TensorFlow 2.x -- Multiple GPUs -- MultiWorkerMirroredStrategy -- TPUStrategy -- ParameterServerStrategy -- Changes in namespaces -- Converting from 1.x to 2.x -- Using TensorFlow 2.x effectively -- The TensorFlow 2.x ecosystem -- Language bindings -- Keras or tf.keras? -- Summary -- Chapter 3: Regression -- What is regression? -- Prediction using linear regression -- Simple linear regression -- Multiple linear regression -- Multivariate linear regression -- TensorFlow Estimators -- Feature columns -- Input functions -- MNIST using TensorFlow Estimator API}
      \field{isbn}{9781838823412}
      \field{title}{{Deep Learning with TensorFlow 2 and Keras: Regression, ConvNets, GANs, RNNs, NLP, and More with TensorFlow 2 and the Keras API}}
      \field{year}{2019}
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Antonio Guili Amita Kapoor Sujit Pal - 2019 - Deep Learning with TensorFlow 2 and Keras Regression, ConvNets, GANs, RNNs, NLP, and More.pdf:pdf
      \endverb
    \endentry
    \entry{IanGoodfellowYoshuaBengio2016}{book}{}
      \name{author}{1}{}{%
        {{hash=66d6169bd5781eda3520264ffcff3d02}{%
           family={{Ian Goodfellow, Yoshua Bengio}},
           familyi={I\bibinitperiod},
           given={Aaron\bibnamedelima Courville},
           giveni={A\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
      }
      \strng{namehash}{66d6169bd5781eda3520264ffcff3d02}
      \strng{fullhash}{66d6169bd5781eda3520264ffcff3d02}
      \strng{bibnamehash}{66d6169bd5781eda3520264ffcff3d02}
      \strng{authorbibnamehash}{66d6169bd5781eda3520264ffcff3d02}
      \strng{authornamehash}{66d6169bd5781eda3520264ffcff3d02}
      \strng{authorfullhash}{66d6169bd5781eda3520264ffcff3d02}
      \field{sortinit}{1}
      \field{sortinithash}{50c6687d7fc80f50136d75228e3c59ba}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Prmu}
      \field{title}{{Deep Learning}}
      \field{year}{2016}
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Ian Goodfellow, Yoshua Bengio - 2016 - Deep Learning.pdf:pdf
      \endverb
    \endentry
    \entry{Patterson2019}{book}{}
      \name{author}{2}{}{%
        {{hash=b437b3540ca6290b1af286f81e1918f1}{%
           family={Patterson},
           familyi={P\bibinitperiod},
           given={Josh},
           giveni={J\bibinitperiod}}}%
        {{hash=6cc15cc3c2db75f3b76f461a16e7acd5}{%
           family={Gibson},
           familyi={G\bibinitperiod},
           given={Adam},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{57d3e134e29f75914b76c446039ab8bf}
      \strng{fullhash}{57d3e134e29f75914b76c446039ab8bf}
      \strng{bibnamehash}{57d3e134e29f75914b76c446039ab8bf}
      \strng{authorbibnamehash}{57d3e134e29f75914b76c446039ab8bf}
      \strng{authornamehash}{57d3e134e29f75914b76c446039ab8bf}
      \strng{authorfullhash}{57d3e134e29f75914b76c446039ab8bf}
      \field{sortinit}{1}
      \field{sortinithash}{50c6687d7fc80f50136d75228e3c59ba}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{"Deep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and video games. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors"--Page 4 of cover. Introduction -- Applied math and machine learning basics. Linear algebra -- Probability and information theory -- Numerical computation -- Machine learning basics -- Deep networks: modern practices. Deep feedforward networks -- Regularization for deep learning -- Optimization for training deep models -- Convolutional networks -- Sequence modeling: recurrent and recursive nets -- Practical methodology -- Applications -- Deep learning research. Linear factor models -- Autoencoders -- Representation learning -- Structured probabilistic models for deep learning -- Monte Carlo methods -- Confronting the partition function -- Approximate inference -- Deep generative models.}
      \field{booktitle}{Nature}
      \field{isbn}{3463353563306}
      \field{number}{7553}
      \field{title}{{Deep Learning a Practitioner'S Approach}}
      \field{volume}{29}
      \field{year}{2019}
      \field{pages}{1\bibrangedash 73}
      \range{pages}{73}
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Patterson, Gibson - 2019 - Deep Learning a Practitioner'S Approach.pdf:pdf
      \endverb
    \endentry
    \entry{Ruder2016}{article}{}
      \name{author}{1}{}{%
        {{hash=b468248a20d75c52ee742f4592c2569f}{%
           family={Ruder},
           familyi={R\bibinitperiod},
           given={Sebastian},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{b468248a20d75c52ee742f4592c2569f}
      \strng{fullhash}{b468248a20d75c52ee742f4592c2569f}
      \strng{bibnamehash}{b468248a20d75c52ee742f4592c2569f}
      \strng{authorbibnamehash}{b468248a20d75c52ee742f4592c2569f}
      \strng{authornamehash}{b468248a20d75c52ee742f4592c2569f}
      \strng{authorfullhash}{b468248a20d75c52ee742f4592c2569f}
      \field{sortinit}{1}
      \field{sortinithash}{50c6687d7fc80f50136d75228e3c59ba}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.}
      \field{eprinttype}{arXiv}
      \field{month}{9}
      \field{title}{{An overview of gradient descent optimization algorithms}}
      \field{year}{2016}
      \verb{eprint}
      \verb 1609.04747
      \endverb
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Ruder - 2016 - An overview of gradient descent optimization algorithms.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1609.04747
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1609.04747
      \endverb
    \endentry
    \entry{StanfordUniversityCoursecs231n2018}{article}{}
      \name{author}{1}{}{%
        {{hash=9fa88a35c4186a2afcb31d783ca1890c}{%
           family={{Stanford University Course cs231n}},
           familyi={S\bibinitperiod}}}%
      }
      \strng{namehash}{9fa88a35c4186a2afcb31d783ca1890c}
      \strng{fullhash}{9fa88a35c4186a2afcb31d783ca1890c}
      \strng{bibnamehash}{9fa88a35c4186a2afcb31d783ca1890c}
      \strng{authorbibnamehash}{9fa88a35c4186a2afcb31d783ca1890c}
      \strng{authornamehash}{9fa88a35c4186a2afcb31d783ca1890c}
      \strng{authorfullhash}{9fa88a35c4186a2afcb31d783ca1890c}
      \field{extraname}{1}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Convolutional Neural Networks are very similar to ordinary Neural Networks from the previous chapter: they are made up of neurons that have learnable weights and biases. Each neuron receives some inputs, performs a dot product and optionally follows it with a non-linearity. The whole network still expresses a single differentiable score function: from the raw image pixels on one end to class scores at the other. And they still have a loss function (e.g. SVM/Softmax) on the last (fully-connected) layer and all the tips/tricks we developed for learning regular Neural Networks still apply. So what does change? ConvNet architectures make the explicit assumption that the inputs are images, which allows us to encode certain properties into the architecture. These then make the forward function more efficient to implement and vastly reduce the amount of parameters in the network.}
      \field{journaltitle}{Stanford University Course cs231n}
      \field{title}{{CS231n Convolutional Neural Networks for Visual Recognition}}
      \field{year}{2018}
      \field{pages}{30}
      \range{pages}{1}
      \verb{urlraw}
      \verb https://cs231n.github.io/neural-networks-3/ http://cs231n.github.io/convolutional-networks/{\%}0Ahttp://cs231n.github.io/neural-networks-3/
      \endverb
      \verb{url}
      \verb https://cs231n.github.io/neural-networks-3/%20http://cs231n.github.io/convolutional-networks/%7B%5C%%7D0Ahttp://cs231n.github.io/neural-networks-3/
      \endverb
    \endentry
    \entry{Srivastava2014}{report}{}
      \name{author}{4}{}{%
        {{hash=6a147afa4569ce6cf23c0436e65d8486}{%
           family={Srivastava},
           familyi={S\bibinitperiod},
           given={Nitish},
           giveni={N\bibinitperiod}}}%
        {{hash=9a8750ccdb2a4cf14d2655face1ce016}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey},
           giveni={G\bibinitperiod}}}%
        {{hash=c5e3a676e2ac1164b3afcd539c131fc9}{%
           family={Krizhevsky},
           familyi={K\bibinitperiod},
           given={Alex},
           giveni={A\bibinitperiod}}}%
        {{hash=bd2be300d445e9f6db7808f9533e66cb}{%
           family={Salakhutdinov},
           familyi={S\bibinitperiod},
           given={Ruslan},
           giveni={R\bibinitperiod}}}%
      }
      \strng{namehash}{8eeaede68a96eb301b76575348cd4b07}
      \strng{fullhash}{a4e8289d0cb36166764c7993dfb7ec92}
      \strng{bibnamehash}{a4e8289d0cb36166764c7993dfb7ec92}
      \strng{authorbibnamehash}{a4e8289d0cb36166764c7993dfb7ec92}
      \strng{authornamehash}{8eeaede68a96eb301b76575348cd4b07}
      \strng{authorfullhash}{a4e8289d0cb36166764c7993dfb7ec92}
      \field{sortinit}{3}
      \field{sortinithash}{a37a8ef248a93c322189792c34fc68c9}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.}
      \field{booktitle}{Journal of Machine Learning Research}
      \field{title}{{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}}
      \field{type}{techreport}
      \field{volume}{15}
      \field{year}{2014}
      \field{pages}{1929\bibrangedash 1958}
      \range{pages}{30}
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf
      \endverb
      \keyw{deep learning,model combination,neural networks,regularization}
    \endentry
    \entry{Aggarwal2018}{book}{}
      \name{author}{1}{}{%
        {{hash=86d11a07b1bfeac58dc68f7a419b3039}{%
           family={Aggarwal},
           familyi={A\bibinitperiod},
           given={Charu\bibnamedelima C.},
           giveni={C\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer International Publishing}%
      }
      \strng{namehash}{86d11a07b1bfeac58dc68f7a419b3039}
      \strng{fullhash}{86d11a07b1bfeac58dc68f7a419b3039}
      \strng{bibnamehash}{86d11a07b1bfeac58dc68f7a419b3039}
      \strng{authorbibnamehash}{86d11a07b1bfeac58dc68f7a419b3039}
      \strng{authornamehash}{86d11a07b1bfeac58dc68f7a419b3039}
      \strng{authorfullhash}{86d11a07b1bfeac58dc68f7a419b3039}
      \field{sortinit}{4}
      \field{sortinithash}{e071e0bcb44634fab398d68ad04e69f4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This book covers both classical and modern models in deep learning. The primary focus is on the theory and algorithms of deep learning. The theory and algorithms of neural networks are particularly important for understanding important concepts, so that one can understand the important design concepts of neural architectures in different applications. Why do neural networks work? When do they work better than off-the-shelf machine-learning models? When is depth useful? Why is training neural networks so hard? What are the pitfalls? The book is also rich in discussing different applications in order to give the practitioner a flavor of how neural architectures are designed for different types of problems. Applications associated with many different areas like recommender systems, machine translation, image captioning, image classification, reinforcement-learning based gaming, and text analytics are covered. The chapters of this book span three categories: The basics of neural networks: Many traditional machine learning models can be understood as special cases of neural networks. An emphasis is placed in the first two chapters on understanding the relationship between traditional machine learning and neural networks. Support vector machines, linear/logistic regression, singular value decomposition, matrix factorization, and recommender systems are shown to be special cases of neural networks. These methods are studied together with recent feature engineering methods like word2vec. Fundamentals of neural networks: A detailed discussion of training and regularization is provided in Chapters 3 and 4. Chapters 5 and 6 present radial-basis function (RBF) networks and restricted Boltzmann machines. Advanced topics in neural networks: Chapters 7 and 8 discuss recurrent neural networks and convolutional neural networks. Several advanced topics like deep reinforcement learning, neural Turing machines, Kohonen self-organizing maps, and generative adversarial networks are introduced in Chapters 9 and 10. The book is written for graduate students, researchers, and practitioners. Numerous exercises are available along with a solution manual to aid in classroom teaching. Where possible, an application-centric view is highlighted in order to provide an understanding of the practical uses of each class of techniques.}
      \field{booktitle}{Neural Networks and Deep Learning}
      \field{title}{{Neural Networks and Deep Learning}}
      \field{year}{2018}
      \verb{doi}
      \verb 10.1007/978-3-319-94463-0
      \endverb
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Aggarwal - 2018 - Neural Networks and Deep Learning.pdf:pdf
      \endverb
    \endentry
    \entry{StanfordUniversityCoursecs231n2018a}{article}{}
      \name{author}{1}{}{%
        {{hash=9fa88a35c4186a2afcb31d783ca1890c}{%
           family={{Stanford University Course cs231n}},
           familyi={S\bibinitperiod}}}%
      }
      \strng{namehash}{9fa88a35c4186a2afcb31d783ca1890c}
      \strng{fullhash}{9fa88a35c4186a2afcb31d783ca1890c}
      \strng{bibnamehash}{9fa88a35c4186a2afcb31d783ca1890c}
      \strng{authorbibnamehash}{9fa88a35c4186a2afcb31d783ca1890c}
      \strng{authornamehash}{9fa88a35c4186a2afcb31d783ca1890c}
      \strng{authorfullhash}{9fa88a35c4186a2afcb31d783ca1890c}
      \field{extraname}{2}
      \field{sortinit}{4}
      \field{sortinithash}{e071e0bcb44634fab398d68ad04e69f4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Convolutional Neural Networks are very similar to ordinary Neural Networks from the previous chapter: they are made up of neurons that have learnable weights and biases. Each neuron receives some inputs, performs a dot product and optionally follows it with a non-linearity. The whole network still expresses a single differentiable score function: from the raw image pixels on one end to class scores at the other. And they still have a loss function (e.g. SVM/Softmax) on the last (fully-connected) layer and all the tips/tricks we developed for learning regular Neural Networks still apply. So what does change? ConvNet architectures make the explicit assumption that the inputs are images, which allows us to encode certain properties into the architecture. These then make the forward function more efficient to implement and vastly reduce the amount of parameters in the network.}
      \field{journaltitle}{Stanford University Course cs231n}
      \field{title}{{CS231n Convolutional Neural Networks for Visual Recognition}}
      \field{year}{2018}
      \field{pages}{30}
      \range{pages}{1}
      \verb{urlraw}
      \verb https://cs231n.github.io/convolutional-networks/
      \endverb
      \verb{url}
      \verb https://cs231n.github.io/convolutional-networks/
      \endverb
    \endentry
    \entry{Zhang}{report}{}
      \name{author}{2}{}{%
        {{hash=5ddec46e090404e1224890533cbaa882}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Ye},
           giveni={Y\bibinitperiod}}}%
        {{hash=dcf3fece96fc55df5a003eccf64d2961}{%
           family={Wallace},
           familyi={W\bibinitperiod},
           given={Byron\bibnamedelima C},
           giveni={B\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
      }
      \strng{namehash}{615367a133b779975ce17728b05f6a32}
      \strng{fullhash}{615367a133b779975ce17728b05f6a32}
      \strng{bibnamehash}{615367a133b779975ce17728b05f6a32}
      \strng{authorbibnamehash}{615367a133b779975ce17728b05f6a32}
      \strng{authornamehash}{615367a133b779975ce17728b05f6a32}
      \strng{authorfullhash}{615367a133b779975ce17728b05f6a32}
      \field{sortinit}{5}
      \field{sortinithash}{5dd416adbafacc8226114bc0202d5fdd}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Convolutional Neural Networks (CNNs) have recently achieved remarkably strong performance on the practically important task of sentence classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2014). However, these models require practitioners to specify an exact model architecture and set accompanying hyperparameters, including the filter region size, regularization parameters, and so on. It is currently unknown how sensitive model performance is to changes in these configurations for the task of sentence classification. We thus conduct a sensitivity analysis of one-layer CNNs to explore the effect of architecture components on model performance; our aim is to distinguish between important and comparatively inconsequential design decisions for sentence classification. We focus on one-layer CNNs (to the exclusion of more complex models) due to their comparative simplicity and strong empirical performance, which makes it a modern standard baseline method akin to Support Vector Machine (SVMs) and logistic regression. We derive practical advice from our extensive empirical results for those interested in getting the most out of CNNs for sentence classification in real world settings.}
      \field{eprinttype}{arXiv}
      \field{title}{{A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification}}
      \field{type}{techreport}
      \verb{eprint}
      \verb 1510.03820v4
      \endverb
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Wallace - Unknown - A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classifica.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb http://nlp.stanford.edu/projects/
      \endverb
      \verb{url}
      \verb http://nlp.stanford.edu/projects/
      \endverb
    \endentry
    \entry{widlml}{misc}{}
      \name{author}{2}{}{%
        {{hash=2823227040db0fb9953a68c781f0565c}{%
           family={{Francois Chaubard}},
           familyi={F\bibinitperiod},
           given={Rohit},
           giveni={R\bibinitperiod}}}%
        {{hash=769cda2e6c89ce1142647f575d271fcb}{%
           family={Mundra},
           familyi={M\bibinitperiod},
           given={Richard\bibnamedelima Socher},
           giveni={R\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
      }
      \strng{namehash}{c20057159afdd8aec0393f6e86c0710b}
      \strng{fullhash}{c20057159afdd8aec0393f6e86c0710b}
      \strng{bibnamehash}{c20057159afdd8aec0393f6e86c0710b}
      \strng{authorbibnamehash}{c20057159afdd8aec0393f6e86c0710b}
      \strng{authornamehash}{c20057159afdd8aec0393f6e86c0710b}
      \strng{authorfullhash}{c20057159afdd8aec0393f6e86c0710b}
      \field{sortinit}{5}
      \field{sortinithash}{5dd416adbafacc8226114bc0202d5fdd}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{{Understanding Convolutional Neural Networks for NLP – WildML}}
      \field{urlday}{15}
      \field{urlmonth}{12}
      \field{urlyear}{2020}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/
      \endverb
      \verb{url}
      \verb http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/
      \endverb
    \endentry
    \entry{Jurafskya}{report}{}
      \name{author}{2}{}{%
        {{hash=bd33d7a70284b5ead9c7985cb5c9929c}{%
           family={Jurafsky},
           familyi={J\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod}}}%
        {{hash=9ed8bee488b2703da341cded66bf83cb}{%
           family={Martin},
           familyi={M\bibinitperiod},
           given={James\bibnamedelima H},
           giveni={J\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
      }
      \strng{namehash}{b5e6a2accdc2023c51f42c6ec010d19b}
      \strng{fullhash}{b5e6a2accdc2023c51f42c6ec010d19b}
      \strng{bibnamehash}{b5e6a2accdc2023c51f42c6ec010d19b}
      \strng{authorbibnamehash}{b5e6a2accdc2023c51f42c6ec010d19b}
      \strng{authornamehash}{b5e6a2accdc2023c51f42c6ec010d19b}
      \strng{authorfullhash}{b5e6a2accdc2023c51f42c6ec010d19b}
      \field{sortinit}{6}
      \field{sortinithash}{7851c86048328b027313775d8fbd2131}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{{Speech and Language Processing An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition Third Edition draft}}
      \field{type}{techreport}
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Jurafsky, Martin - Unknown - Speech and Language Processing An Introduction to Natural Language Processing, Computational Linguistics, a.pdf:pdf
      \endverb
    \endentry
    \entry{wordemdgood}{misc}{}
      \field{sortinit}{6}
      \field{sortinithash}{7851c86048328b027313775d8fbd2131}
      \field{labeltitlesource}{title}
      \field{title}{{Embeddings: Translating to a Lower-Dimensional Space}}
      \field{urlday}{15}
      \field{urlmonth}{12}
      \field{urlyear}{2020}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space
      \endverb
      \verb{url}
      \verb https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space
      \endverb
    \endentry
    \entry{Mikolov2013}{report}{}
      \name{author}{4}{}{%
        {{hash=a2d359b12ca2fadf0b40136a73f021bb}{%
           family={Mikolov},
           familyi={M\bibinitperiod},
           given={Tomas},
           giveni={T\bibinitperiod}}}%
        {{hash=ee3f7d7b96add98106db907e189d6c13}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Kai},
           giveni={K\bibinitperiod}}}%
        {{hash=3d11e6f2a0d0a1183b2cf62996525afc}{%
           family={Corrado},
           familyi={C\bibinitperiod},
           given={Greg},
           giveni={G\bibinitperiod}}}%
        {{hash=4aecfb0cc2e1e3b7899129fa2a94e2b8}{%
           family={Dean},
           familyi={D\bibinitperiod},
           given={Jeffrey},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{ed4d154f6ad724ee86e4b498231228f3}
      \strng{fullhash}{f24c60896b6daa69474b40efb61f4e88}
      \strng{bibnamehash}{f24c60896b6daa69474b40efb61f4e88}
      \strng{authorbibnamehash}{f24c60896b6daa69474b40efb61f4e88}
      \strng{authornamehash}{ed4d154f6ad724ee86e4b498231228f3}
      \strng{authorfullhash}{f24c60896b6daa69474b40efb61f4e88}
      \field{extraname}{1}
      \field{sortinit}{6}
      \field{sortinithash}{7851c86048328b027313775d8fbd2131}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.}
      \field{eprinttype}{arXiv}
      \field{title}{{Distributed Representations of Words and Phrases and their Compositionality}}
      \field{type}{techreport}
      \field{year}{2013}
      \verb{eprint}
      \verb 1310.4546v1
      \endverb
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf
      \endverb
      \keyw{()}
    \endentry
    \entry{Mikolov}{report}{}
      \name{author}{4}{}{%
        {{hash=a2d359b12ca2fadf0b40136a73f021bb}{%
           family={Mikolov},
           familyi={M\bibinitperiod},
           given={Tomas},
           giveni={T\bibinitperiod}}}%
        {{hash=ee3f7d7b96add98106db907e189d6c13}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Kai},
           giveni={K\bibinitperiod}}}%
        {{hash=3d11e6f2a0d0a1183b2cf62996525afc}{%
           family={Corrado},
           familyi={C\bibinitperiod},
           given={Greg},
           giveni={G\bibinitperiod}}}%
        {{hash=4aecfb0cc2e1e3b7899129fa2a94e2b8}{%
           family={Dean},
           familyi={D\bibinitperiod},
           given={Jeffrey},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{ed4d154f6ad724ee86e4b498231228f3}
      \strng{fullhash}{f24c60896b6daa69474b40efb61f4e88}
      \strng{bibnamehash}{f24c60896b6daa69474b40efb61f4e88}
      \strng{authorbibnamehash}{f24c60896b6daa69474b40efb61f4e88}
      \strng{authornamehash}{ed4d154f6ad724ee86e4b498231228f3}
      \strng{authorfullhash}{f24c60896b6daa69474b40efb61f4e88}
      \field{extraname}{2}
      \field{sortinit}{6}
      \field{sortinithash}{7851c86048328b027313775d8fbd2131}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.}
      \field{eprinttype}{arXiv}
      \field{title}{{Efficient Estimation of Word Representations in Vector Space}}
      \field{type}{techreport}
      \verb{eprint}
      \verb 1301.3781v3
      \endverb
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Mikolov et al. - Unknown - Efficient Estimation of Word Representations in Vector Space.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb http://ronan.collobert.com/senna/
      \endverb
      \verb{url}
      \verb http://ronan.collobert.com/senna/
      \endverb
    \endentry
  \enddatalist
\endrefsection
\endinput

