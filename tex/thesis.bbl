% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.1 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{none/global//global/global}
    \entry{Rosenblatt}{report}{}
      \name{author}{1}{}{%
        {{hash=1750cd87a34d44119e7a4aab9b8012c6}{%
           family={Rosenblatt},
           familyi={R\bibinitperiod},
           given={F},
           giveni={F\bibinitperiod}}}%
      }
      \strng{namehash}{1750cd87a34d44119e7a4aab9b8012c6}
      \strng{fullhash}{1750cd87a34d44119e7a4aab9b8012c6}
      \strng{bibnamehash}{1750cd87a34d44119e7a4aab9b8012c6}
      \strng{authorbibnamehash}{1750cd87a34d44119e7a4aab9b8012c6}
      \strng{authornamehash}{1750cd87a34d44119e7a4aab9b8012c6}
      \strng{authorfullhash}{1750cd87a34d44119e7a4aab9b8012c6}
      \field{sortinit}{1}
      \field{sortinithash}{50c6687d7fc80f50136d75228e3c59ba}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Psychological Review}
      \field{number}{6}
      \field{title}{{THE PERCEPTRON: A PROBABILISTIC MODEL FOR INFORMATION STORAGE AND ORGANIZATION IN THE BRAIN 1}}
      \field{type}{techreport}
      \field{volume}{65}
      \field{pages}{19\bibrangedash 27}
      \range{pages}{9}
    \endentry
    \entry{Werbos1990}{article}{}
      \name{author}{1}{}{%
        {{hash=3f6ba0fa8bc8542632efe7f31c9ee2e8}{%
           family={Werbos},
           familyi={W\bibinitperiod},
           given={Paul\bibnamedelima J.},
           giveni={P\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \strng{namehash}{3f6ba0fa8bc8542632efe7f31c9ee2e8}
      \strng{fullhash}{3f6ba0fa8bc8542632efe7f31c9ee2e8}
      \strng{bibnamehash}{3f6ba0fa8bc8542632efe7f31c9ee2e8}
      \strng{authorbibnamehash}{3f6ba0fa8bc8542632efe7f31c9ee2e8}
      \strng{authornamehash}{3f6ba0fa8bc8542632efe7f31c9ee2e8}
      \strng{authorfullhash}{3f6ba0fa8bc8542632efe7f31c9ee2e8}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Backpropagation is now the most widely used tool in the field of artificial neural networks. At the core of backpropagation is a method for calculating derivatives exactly and efficiently in any large system made up of elementary subsystems or calculations which are represented by known, differentiable functions; thus, backpropagation has many applications which do not involve neural networks as such. This paper first reviews basic backpropagation, a simple method which is now being widely used in areas like pattern recognition and fault diagnosis. Next, it presents the basic equations for back-propagation through time, and discusses applications to areas like pattern recognition involving dynamic systems, systems identification, and control. Finally, it describes further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations or true recurrent networks, and other practical issues which arise with this method. Pseudocode is provided to clarify the algorithms. The chain rule forordered derivatives—the theorem which underlies backpropagation—is briefly discussed. © 1990, IEEE}
      \field{issn}{15582256}
      \field{journaltitle}{Proceedings of the IEEE}
      \field{number}{10}
      \field{title}{{Backpropagation Through Time: What It Does and How to Do It}}
      \field{volume}{78}
      \field{year}{1990}
      \field{pages}{1550\bibrangedash 1560}
      \range{pages}{11}
      \verb{doi}
      \verb 10.1109/5.58337
      \endverb
    \endentry
    \entry{Hinton}{report}{}
      \name{author}{2}{}{%
        {{hash=813bd95fe553e6079cd53a567b238287}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey\bibnamedelima E},
           giveni={G\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=643f92e8f89f2746a4c1aa077d225755}{%
           family={Osindero},
           familyi={O\bibinitperiod},
           given={Simon},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{cdfb09be61c835ae48863e4d278a5539}
      \strng{fullhash}{cdfb09be61c835ae48863e4d278a5539}
      \strng{bibnamehash}{cdfb09be61c835ae48863e4d278a5539}
      \strng{authorbibnamehash}{cdfb09be61c835ae48863e4d278a5539}
      \strng{authornamehash}{cdfb09be61c835ae48863e4d278a5539}
      \strng{authorfullhash}{cdfb09be61c835ae48863e4d278a5539}
      \field{sortinit}{3}
      \field{sortinithash}{a37a8ef248a93c322189792c34fc68c9}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associa-tive memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.}
      \field{title}{{A Fast Learning Algorithm for Deep Belief Nets Yee-Whye Teh}}
      \field{type}{techreport}
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Hinton, Osindero - Unknown - A Fast Learning Algorithm for Deep Belief Nets Yee-Whye Teh.pdf:pdf
      \endverb
    \endentry
    \entry{Schmidhuber2014}{report}{}
      \name{author}{1}{}{%
        {{hash=288bdbcfe1b91ad7484d7a24f74f99ed}{%
           family={Schmidhuber},
           familyi={S\bibinitperiod},
           given={J{ü}rgen},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{288bdbcfe1b91ad7484d7a24f74f99ed}
      \strng{fullhash}{288bdbcfe1b91ad7484d7a24f74f99ed}
      \strng{bibnamehash}{288bdbcfe1b91ad7484d7a24f74f99ed}
      \strng{authorbibnamehash}{288bdbcfe1b91ad7484d7a24f74f99ed}
      \strng{authornamehash}{288bdbcfe1b91ad7484d7a24f74f99ed}
      \strng{authorfullhash}{288bdbcfe1b91ad7484d7a24f74f99ed}
      \field{sortinit}{4}
      \field{sortinithash}{e071e0bcb44634fab398d68ad04e69f4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks. LATEX source: Preface This is the preprint of an invited Deep Learning (DL) overview. One of its goals is to assign credit to those who contributed to the present state of the art. I acknowledge the limitations of attempting to achieve this goal. The DL research community itself may be viewed as a continually evolving, deep network of scientists who have influenced each other in complex ways. Starting from recent DL results, I tried to trace back the origins of relevant ideas through the past half century and beyond, sometimes using "local search" to follow citations of citations backwards in time. Since not all DL publications properly acknowledge earlier relevant work, additional global search strategies were employed , aided by consulting numerous neural network experts. As a result, the present preprint mostly consists of references. Nevertheless, through an expert selection bias I may have missed important work. A related bias was surely introduced by my special familiarity with the work of my own DL research group in the past quarter-century. For these reasons, this work should be viewed as merely a snapshot of an ongoing credit assignment process. To help improve it, please do not hesitate to send corrections and suggestions to juergen@idsia.ch.}
      \field{eprinttype}{arXiv}
      \field{title}{{Deep Learning in Neural Networks: An Overview}}
      \field{type}{techreport}
      \field{year}{2014}
      \verb{eprint}
      \verb 1404.7828v4
      \endverb
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Schmidhuber - 2014 - Deep Learning in Neural Networks An Overview.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb http://www.idsia.ch/˜juergen/DeepLearning8Oct2014.texCompleteBIBTEXfile
      \endverb
      \verb{url}
      \verb http://www.idsia.ch/%CB%9Cjuergen/DeepLearning8Oct2014.texCompleteBIBTEXfile
      \endverb
    \endentry
    \entry{Rumelhart1986}{article}{}
      \name{author}{3}{}{%
        {{hash=55cd5380bb5b15032a6d5a2015f56e3f}{%
           family={Rumelhart},
           familyi={R\bibinitperiod},
           given={David\bibnamedelima E.},
           giveni={D\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=813bd95fe553e6079cd53a567b238287}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey\bibnamedelima E.},
           giveni={G\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=6cbc29ad7fd57ffdb9ed4728418fd988}{%
           family={Williams},
           familyi={W\bibinitperiod},
           given={Ronald\bibnamedelima J.},
           giveni={R\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \strng{namehash}{fd75cea68d6982b503a0cd84f7cc7b51}
      \strng{fullhash}{fd75cea68d6982b503a0cd84f7cc7b51}
      \strng{bibnamehash}{fd75cea68d6982b503a0cd84f7cc7b51}
      \strng{authorbibnamehash}{fd75cea68d6982b503a0cd84f7cc7b51}
      \strng{authornamehash}{fd75cea68d6982b503a0cd84f7cc7b51}
      \strng{authorfullhash}{fd75cea68d6982b503a0cd84f7cc7b51}
      \field{sortinit}{6}
      \field{sortinithash}{7851c86048328b027313775d8fbd2131}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure 1. © 1986 Nature Publishing Group.}
      \field{issn}{00280836}
      \field{journaltitle}{Nature}
      \field{number}{6088}
      \field{title}{{Learning representations by back-propagating errors}}
      \field{volume}{323}
      \field{year}{1986}
      \verb{doi}
      \verb 10.1038/323533a0
      \endverb
    \endentry
    \entry{Herculano-Houzel2009}{misc}{}
      \name{author}{1}{}{%
        {{hash=844747f22b37db160509d1c43124abcb}{%
           family={Herculano-Houzel},
           familyi={H\bibinithyphendelim H\bibinitperiod},
           given={Suzana},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{844747f22b37db160509d1c43124abcb}
      \strng{fullhash}{844747f22b37db160509d1c43124abcb}
      \strng{bibnamehash}{844747f22b37db160509d1c43124abcb}
      \strng{authorbibnamehash}{844747f22b37db160509d1c43124abcb}
      \strng{authornamehash}{844747f22b37db160509d1c43124abcb}
      \strng{authorfullhash}{844747f22b37db160509d1c43124abcb}
      \field{sortinit}{7}
      \field{sortinithash}{f615fb9c6fba11c6f962fb3fd599810e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The human brain has often been viewed as outstanding among mammalian brains: the most cognitively able, the largest-than-expected from body size, endowed with an overdeveloped cerebral cortex that represents over 80{\%} of brain mass, and purportedly containing 100 billion neurons and 10× more glial cells. Such uniqueness was seemingly necessary to justify the superior cognitive abilities of humans over larger-brained mammals such as elephants and whales. However, our recent studies using a novel method to determine the cellular composition of the brain of humans and other primates as well as of rodents and insectivores show that, since different cellular scaling rules apply to the brains within these orders, brain size can no longer be considered a proxy for the number of neurons in the brain. These studies also showed that the human brain is not exceptional in its cellular composition, as it was found to contain as many neuronal and non-neuronal cells as would be expected of a primate brain of its size. Additionally, the so-called overdeveloped human cerebral cortex holds only 19{\%} of all brain neurons, a fraction that is similar to that found in other mammals. In what regards absolute numbers of neurons, however, the human brain does have two advantages compared to other mammalian brains: compared to rodents, and probably to whales and elephants as well, it is built according to the very economical, space-saving scaling rules that apply to other primates; and, among economically built primate brains, it is the largest, hence containing the most neurons. These findings argue in favor of a view of cognitive abilities that is centered on absolute numbers of neurons, rather than on body size or encephalization, and call for a re-examination of several concepts related to the exceptionality of the human brain. © 2009 Herculano-Houzel.}
      \field{booktitle}{Frontiers in Human Neuroscience}
      \field{issn}{16625161}
      \field{number}{NOV}
      \field{title}{{The human brain in numbers: A linearly scaled-up primate brain}}
      \field{volume}{3}
      \field{year}{2009}
      \verb{doi}
      \verb 10.3389/neuro.09.031.2009
      \endverb
    \endentry
    \entry{IanGoodfellowYoshuaBengio2016}{book}{}
      \name{author}{1}{}{%
        {{hash=66d6169bd5781eda3520264ffcff3d02}{%
           family={{Ian Goodfellow, Yoshua Bengio}},
           familyi={I\bibinitperiod},
           given={Aaron\bibnamedelima Courville},
           giveni={A\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
      }
      \strng{namehash}{66d6169bd5781eda3520264ffcff3d02}
      \strng{fullhash}{66d6169bd5781eda3520264ffcff3d02}
      \strng{bibnamehash}{66d6169bd5781eda3520264ffcff3d02}
      \strng{authorbibnamehash}{66d6169bd5781eda3520264ffcff3d02}
      \strng{authornamehash}{66d6169bd5781eda3520264ffcff3d02}
      \strng{authorfullhash}{66d6169bd5781eda3520264ffcff3d02}
      \field{sortinit}{8}
      \field{sortinithash}{1b24cab5087933ef0826a7cd3b99e994}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Prmu}
      \field{title}{{Deep Learning}}
      \field{year}{2016}
      \field{pages}{1\bibrangedash 10}
      \range{pages}{10}
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Ian Goodfellow, Yoshua Bengio - 2016 - Deep Learning.pdf:pdf
      \endverb
    \endentry
    \entry{Suah2017}{article}{}
      \name{author}{1}{}{%
        {{hash=e78141b9fd51644bcf4ac1c177789c9d}{%
           family={Suah},
           familyi={S\bibinitperiod},
           given={Faiz\bibnamedelimb Bukhari\bibnamedelima Mohd},
           giveni={F\bibinitperiod\bibinitdelim B\bibinitperiod\bibinitdelim M\bibinitperiod}}}%
      }
      \strng{namehash}{e78141b9fd51644bcf4ac1c177789c9d}
      \strng{fullhash}{e78141b9fd51644bcf4ac1c177789c9d}
      \strng{bibnamehash}{e78141b9fd51644bcf4ac1c177789c9d}
      \strng{authorbibnamehash}{e78141b9fd51644bcf4ac1c177789c9d}
      \strng{authornamehash}{e78141b9fd51644bcf4ac1c177789c9d}
      \strng{authorfullhash}{e78141b9fd51644bcf4ac1c177789c9d}
      \field{sortinit}{9}
      \field{sortinithash}{54047ffb55bdefa0694bbd554c1b11a0}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A greener analytical procedure based on automated flow through system with an optical sensor is proposed for determination of Co(II). The flow through system consisted of polymer inclusion membrane (PIM) containing potassium thiocyanate (KSCN) that was placed between the measuring cell and fixed with optical sensor probe as an optical sensor for monitoring of Co(II) at 625 nm. In the presence of Co(II) ions, the colourless membrane changes to blue. The sensing membrane was prepared by incorporating SCN into a non plasticized PIM. The prepared PIM were found to be homogenous, transparent and mechanically stable. The optode shows reversible optical response in the range of 1.00 × 10−6 – 1.00 × 10−3 mol L−1 with detection limit of 6.10 × 10−7 mol L−1. The optode can be regenerated by using 0.1 mol L−1 of ethylenediaminetetraacetic acid (EDTA). The main parameters of the computer controlled flow system incorporating the flow-through optode, a multi-port selection valve and peristaltic pump were optimized too. The calculated Relative Standard Deviation (R.S.D) of the repeatability and reproducibility of the method are 0.76{\%} and 4.73{\%}, respectively. This green system has been applied to the determination of Co(II) in wastewater samples with reduced reagents and samples consumption and minimum waste generation.}
      \field{issn}{22141812}
      \field{journaltitle}{Analytical Chemistry Research}
      \field{title}{{Preparation and characterization of a novel Co(II) optode based on polymer inclusion membrane}}
      \field{volume}{12}
      \field{year}{2017}
      \field{pages}{40\bibrangedash 46}
      \range{pages}{7}
      \verb{doi}
      \verb 10.1016/j.ancr.2017.02.001
      \endverb
      \verb{file}
      \verb :Users/ugurtigu/Downloads/Zeiler-Fergus2014{\_}Chapter{\_}VisualizingAndUnderstandingCon.pdf:pdf
      \endverb
      \keyw{Aliquat 336,Cobalt(II),Flow through system,Green analytical chemistry,Optode,Polymer inclusion membrane}
    \endentry
    \entry{Nielsen2015}{article}{}
      \name{author}{1}{}{%
        {{hash=17279793dcf098cfda8570111ee5cfd9}{%
           family={Nielsen},
           familyi={N\bibinitperiod},
           given={MA},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{17279793dcf098cfda8570111ee5cfd9}
      \strng{fullhash}{17279793dcf098cfda8570111ee5cfd9}
      \strng{bibnamehash}{17279793dcf098cfda8570111ee5cfd9}
      \strng{authorbibnamehash}{17279793dcf098cfda8570111ee5cfd9}
      \strng{authornamehash}{17279793dcf098cfda8570111ee5cfd9}
      \strng{authorfullhash}{17279793dcf098cfda8570111ee5cfd9}
      \field{sortinit}{1}
      \field{sortinithash}{50c6687d7fc80f50136d75228e3c59ba}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{{Neural networks and deep learning}}
      \field{year}{2015}
    \endentry
    \entry{Taylor2017}{book}{}
      \name{author}{1}{}{%
        {{hash=5efe73c0ef8f0e873d520c4ea41f807e}{%
           family={Taylor},
           familyi={T\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{5efe73c0ef8f0e873d520c4ea41f807e}
      \strng{fullhash}{5efe73c0ef8f0e873d520c4ea41f807e}
      \strng{bibnamehash}{5efe73c0ef8f0e873d520c4ea41f807e}
      \strng{authorbibnamehash}{5efe73c0ef8f0e873d520c4ea41f807e}
      \strng{authornamehash}{5efe73c0ef8f0e873d520c4ea41f807e}
      \strng{authorfullhash}{5efe73c0ef8f0e873d520c4ea41f807e}
      \field{sortinit}{1}
      \field{sortinithash}{50c6687d7fc80f50136d75228e3c59ba}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{{The Math of Neural Networks}}
      \field{year}{2017}
      \verb{file}
      \verb :Users/ugurtigu/Downloads/The Math of Neural Networks by Michael Taylor, Mark Koning (z-lib.org).epub.pdf:pdf
      \endverb
    \endentry
    \entry{Chollet2017}{book}{}
      \name{author}{1}{}{%
        {{hash=5836fc1fa037e340c8b5591da3207608}{%
           family={Chollet},
           familyi={C\bibinitperiod},
           given={Francois},
           giveni={F\bibinitperiod}}}%
      }
      \strng{namehash}{5836fc1fa037e340c8b5591da3207608}
      \strng{fullhash}{5836fc1fa037e340c8b5591da3207608}
      \strng{bibnamehash}{5836fc1fa037e340c8b5591da3207608}
      \strng{authorbibnamehash}{5836fc1fa037e340c8b5591da3207608}
      \strng{authornamehash}{5836fc1fa037e340c8b5591da3207608}
      \strng{authorfullhash}{5836fc1fa037e340c8b5591da3207608}
      \field{sortinit}{1}
      \field{sortinithash}{50c6687d7fc80f50136d75228e3c59ba}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{isbn}{9780996452762}
      \field{title}{{Deep learning with Python}}
      \field{year}{2017}
      \verb{doi}
      \verb 10.23919/ICIF.2018.8455530
      \endverb
      \verb{file}
      \verb :Users/ugurtigu/Downloads/Deep learning with Python by Chollet, Francois (z-lib.org).pdf:pdf
      \endverb
    \endentry
    \entry{AntonioGuili;AmitaKapoor;SujitPal2019}{book}{}
      \name{author}{1}{}{%
        {{hash=5be3f653849a463589c05d6840a52369}{%
           family={{Antonio Guili; Amita Kapoor; Sujit Pal}},
           familyi={A\bibinitperiod}}}%
      }
      \strng{namehash}{5be3f653849a463589c05d6840a52369}
      \strng{fullhash}{5be3f653849a463589c05d6840a52369}
      \strng{bibnamehash}{5be3f653849a463589c05d6840a52369}
      \strng{authorbibnamehash}{5be3f653849a463589c05d6840a52369}
      \strng{authornamehash}{5be3f653849a463589c05d6840a52369}
      \strng{authorfullhash}{5be3f653849a463589c05d6840a52369}
      \field{sortinit}{1}
      \field{sortinithash}{50c6687d7fc80f50136d75228e3c59ba}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{2nd ed. Predicting house price using linear regression Deep Learning with TensorFlow 2 and Keras, Second Edition teaches deep learning techniques alongside TensorFlow (TF) and Keras. The book introduces neural networks with TensorFlow, runs through the main applications, covers two working example apps, and then dives into TF and cloudin production, TF mobile, and using TensorFlow with AutoML. Cover -- Copyright -- Packt Page -- Contributors -- Table of Contents -- Preface -- Chapter 1: Neural Network Foundations with TensorFlow 2.0 -- What is TensorFlow (TF)? -- What is Keras? -- What are the most important changes in TensorFlow 2.0? -- Introduction to neural networks -- Perceptron -- A first example of TensorFlow 2.0 code -- Multi-layer perceptron -- our first example of a network -- Problems in training the perceptron and their solutions -- Activation function -- sigmoid -- Activation function -- tanh -- Activation function -- ReLU Two additional activation functions -- ELU and LeakyReLU -- Activation functions -- In short -- what are neural networks after all? -- A real example -- recognizing handwritten digits -- One-hot encoding (OHE) -- Defining a simple neural network in TensorFlow 2.0 -- Running a simple TensorFlow 2.0 net and establishing a baseline -- Improving the simple net in TensorFlow 2.0 with hidden layers -- Further improving the simple net in TensorFlow with Dropout -- Testing different optimizers in TensorFlow 2.0 -- Increasing the number of epochs -- Controlling the optimizer learning rate Increasing the number of internal hidden neurons -- Increasing the size of batch computation -- Summarizing experiments run for recognizing handwritten charts -- Regularization -- Adopting regularization to avoid overfitting -- Understanding BatchNormalization -- Playing with Google Colab -- CPUs, GPUs, and TPUs -- Sentiment analysis -- Hyperparameter tuning and AutoML -- Predicting output -- A practical overview of backpropagation -- What have we learned so far? -- Towards a deep learning approach -- References -- Chapter 2: TensorFlow 1.x and 2.x -- Understanding TensorFlow 1.x TensorFlow 1.x computational graph program structure -- Computational graphs -- Working with constants, variables, and placeholders -- Examples of operations -- Constants -- Sequences -- Random tensors -- Variables -- An example of TensorFlow 1.x in TensorFlow 2.x -- Understanding TensorFlow 2.x -- Eager execution -- AutoGraph -- Keras APIs -- three programming models -- Sequential API -- Functional API -- Model subclassing -- Callbacks -- Saving a model and weights -- Training from tf.data.datasets -- tf.keras or Estimators? -- Ragged tensors -- Custom training Distributed training in TensorFlow 2.x -- Multiple GPUs -- MultiWorkerMirroredStrategy -- TPUStrategy -- ParameterServerStrategy -- Changes in namespaces -- Converting from 1.x to 2.x -- Using TensorFlow 2.x effectively -- The TensorFlow 2.x ecosystem -- Language bindings -- Keras or tf.keras? -- Summary -- Chapter 3: Regression -- What is regression? -- Prediction using linear regression -- Simple linear regression -- Multiple linear regression -- Multivariate linear regression -- TensorFlow Estimators -- Feature columns -- Input functions -- MNIST using TensorFlow Estimator API}
      \field{isbn}{9781838823412}
      \field{title}{{Deep Learning with TensorFlow 2 and Keras: Regression, ConvNets, GANs, RNNs, NLP, and More with TensorFlow 2 and the Keras API}}
      \field{year}{2019}
      \verb{file}
      \verb :Users/ugurtigu/Downloads/Deep Learning with TensorFlow 2.0 and Keras Regression, ConvNets, GANs, RNNs, NLP more with TF 2.0 and the Keras API by Antonio Gulli, Amita Kapoor, Sujit Pal (z-lib.org).pdf:pdf
      \endverb
    \endentry
    \entry{Patterson2019}{book}{}
      \name{author}{2}{}{%
        {{hash=b437b3540ca6290b1af286f81e1918f1}{%
           family={Patterson},
           familyi={P\bibinitperiod},
           given={Josh},
           giveni={J\bibinitperiod}}}%
        {{hash=6cc15cc3c2db75f3b76f461a16e7acd5}{%
           family={Gibson},
           familyi={G\bibinitperiod},
           given={Adam},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{57d3e134e29f75914b76c446039ab8bf}
      \strng{fullhash}{57d3e134e29f75914b76c446039ab8bf}
      \strng{bibnamehash}{57d3e134e29f75914b76c446039ab8bf}
      \strng{authorbibnamehash}{57d3e134e29f75914b76c446039ab8bf}
      \strng{authornamehash}{57d3e134e29f75914b76c446039ab8bf}
      \strng{authorfullhash}{57d3e134e29f75914b76c446039ab8bf}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{"Deep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and video games. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors"--Page 4 of cover. Introduction -- Applied math and machine learning basics. Linear algebra -- Probability and information theory -- Numerical computation -- Machine learning basics -- Deep networks: modern practices. Deep feedforward networks -- Regularization for deep learning -- Optimization for training deep models -- Convolutional networks -- Sequence modeling: recurrent and recursive nets -- Practical methodology -- Applications -- Deep learning research. Linear factor models -- Autoencoders -- Representation learning -- Structured probabilistic models for deep learning -- Monte Carlo methods -- Confronting the partition function -- Approximate inference -- Deep generative models.}
      \field{booktitle}{Nature}
      \field{isbn}{3463353563306}
      \field{number}{7553}
      \field{title}{{Deep Learning a Practitioner'S Approach}}
      \field{volume}{29}
      \field{year}{2019}
      \field{pages}{1\bibrangedash 73}
      \range{pages}{73}
      \verb{file}
      \verb :Users/ugurtigu/Downloads/Deep Learning A Practitioners Approach by J. Patterson, A. Gibson (z-lib.org).pdf:pdf
      \endverb
    \endentry
    \entry{Ruder2016}{article}{}
      \name{author}{1}{}{%
        {{hash=b468248a20d75c52ee742f4592c2569f}{%
           family={Ruder},
           familyi={R\bibinitperiod},
           given={Sebastian},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{b468248a20d75c52ee742f4592c2569f}
      \strng{fullhash}{b468248a20d75c52ee742f4592c2569f}
      \strng{bibnamehash}{b468248a20d75c52ee742f4592c2569f}
      \strng{authorbibnamehash}{b468248a20d75c52ee742f4592c2569f}
      \strng{authornamehash}{b468248a20d75c52ee742f4592c2569f}
      \strng{authorfullhash}{b468248a20d75c52ee742f4592c2569f}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.}
      \field{eprinttype}{arXiv}
      \field{month}{9}
      \field{title}{{An overview of gradient descent optimization algorithms}}
      \field{year}{2016}
      \verb{eprint}
      \verb 1609.04747
      \endverb
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Ruder - 2016 - An overview of gradient descent optimization algorithms.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1609.04747
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1609.04747
      \endverb
    \endentry
    \entry{StanfordUniversityCoursecs231n2018}{article}{}
      \name{author}{1}{}{%
        {{hash=9fa88a35c4186a2afcb31d783ca1890c}{%
           family={{Stanford University Course cs231n}},
           familyi={S\bibinitperiod}}}%
      }
      \strng{namehash}{9fa88a35c4186a2afcb31d783ca1890c}
      \strng{fullhash}{9fa88a35c4186a2afcb31d783ca1890c}
      \strng{bibnamehash}{9fa88a35c4186a2afcb31d783ca1890c}
      \strng{authorbibnamehash}{9fa88a35c4186a2afcb31d783ca1890c}
      \strng{authornamehash}{9fa88a35c4186a2afcb31d783ca1890c}
      \strng{authorfullhash}{9fa88a35c4186a2afcb31d783ca1890c}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Convolutional Neural Networks are very similar to ordinary Neural Networks from the previous chapter: they are made up of neurons that have learnable weights and biases. Each neuron receives some inputs, performs a dot product and optionally follows it with a non-linearity. The whole network still expresses a single differentiable score function: from the raw image pixels on one end to class scores at the other. And they still have a loss function (e.g. SVM/Softmax) on the last (fully-connected) layer and all the tips/tricks we developed for learning regular Neural Networks still apply. So what does change? ConvNet architectures make the explicit assumption that the inputs are images, which allows us to encode certain properties into the architecture. These then make the forward function more efficient to implement and vastly reduce the amount of parameters in the network.}
      \field{journaltitle}{Stanford University Course cs231n}
      \field{title}{{CS231n Convolutional Neural Networks for Visual Recognition}}
      \field{year}{2018}
      \field{pages}{30}
      \range{pages}{1}
      \verb{urlraw}
      \verb https://cs231n.github.io/neural-networks-3/ http://cs231n.github.io/convolutional-networks/{\%}0Ahttp://cs231n.github.io/neural-networks-3/
      \endverb
      \verb{url}
      \verb https://cs231n.github.io/neural-networks-3/%20http://cs231n.github.io/convolutional-networks/%7B%5C%%7D0Ahttp://cs231n.github.io/neural-networks-3/
      \endverb
    \endentry
    \entry{Srivastava2014}{report}{}
      \name{author}{4}{}{%
        {{hash=6a147afa4569ce6cf23c0436e65d8486}{%
           family={Srivastava},
           familyi={S\bibinitperiod},
           given={Nitish},
           giveni={N\bibinitperiod}}}%
        {{hash=9a8750ccdb2a4cf14d2655face1ce016}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey},
           giveni={G\bibinitperiod}}}%
        {{hash=c5e3a676e2ac1164b3afcd539c131fc9}{%
           family={Krizhevsky},
           familyi={K\bibinitperiod},
           given={Alex},
           giveni={A\bibinitperiod}}}%
        {{hash=bd2be300d445e9f6db7808f9533e66cb}{%
           family={Salakhutdinov},
           familyi={S\bibinitperiod},
           given={Ruslan},
           giveni={R\bibinitperiod}}}%
      }
      \strng{namehash}{8eeaede68a96eb301b76575348cd4b07}
      \strng{fullhash}{a4e8289d0cb36166764c7993dfb7ec92}
      \strng{bibnamehash}{a4e8289d0cb36166764c7993dfb7ec92}
      \strng{authorbibnamehash}{a4e8289d0cb36166764c7993dfb7ec92}
      \strng{authornamehash}{8eeaede68a96eb301b76575348cd4b07}
      \strng{authorfullhash}{a4e8289d0cb36166764c7993dfb7ec92}
      \field{sortinit}{3}
      \field{sortinithash}{a37a8ef248a93c322189792c34fc68c9}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.}
      \field{booktitle}{Journal of Machine Learning Research}
      \field{title}{{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}}
      \field{type}{techreport}
      \field{volume}{15}
      \field{year}{2014}
      \field{pages}{1929\bibrangedash 1958}
      \range{pages}{30}
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf
      \endverb
      \keyw{deep learning,model combination,neural networks,regularization}
    \endentry
  \enddatalist
\endrefsection
\endinput

