% $ biblatex auxiliary file $
% $ biblatex bbl format version 3.1 $
% Do not modify the above lines!
%
% This is an auxiliary file used by the 'biblatex' package.
% This file may safely be deleted. It will be recreated by
% biber as required.
%
\begingroup
\makeatletter
\@ifundefined{ver@biblatex.sty}
  {\@latex@error
     {Missing 'biblatex' package}
     {The bibliography requires the 'biblatex' package.}
      \aftergroup\endinput}
  {}
\endgroup


\refsection{0}
  \datalist[entry]{none/global//global/global}
    \entry{Rosenblatt}{report}{}
      \name{author}{1}{}{%
        {{hash=1750cd87a34d44119e7a4aab9b8012c6}{%
           family={Rosenblatt},
           familyi={R\bibinitperiod},
           given={F},
           giveni={F\bibinitperiod}}}%
      }
      \strng{namehash}{1750cd87a34d44119e7a4aab9b8012c6}
      \strng{fullhash}{1750cd87a34d44119e7a4aab9b8012c6}
      \strng{bibnamehash}{1750cd87a34d44119e7a4aab9b8012c6}
      \strng{authorbibnamehash}{1750cd87a34d44119e7a4aab9b8012c6}
      \strng{authornamehash}{1750cd87a34d44119e7a4aab9b8012c6}
      \strng{authorfullhash}{1750cd87a34d44119e7a4aab9b8012c6}
      \field{sortinit}{1}
      \field{sortinithash}{50c6687d7fc80f50136d75228e3c59ba}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Psychological Review}
      \field{number}{6}
      \field{title}{{THE PERCEPTRON: A PROBABILISTIC MODEL FOR INFORMATION STORAGE AND ORGANIZATION IN THE BRAIN 1}}
      \field{type}{techreport}
      \field{volume}{65}
      \field{pages}{19\bibrangedash 27}
      \range{pages}{9}
    \endentry
    \entry{Werbos1990}{article}{}
      \name{author}{1}{}{%
        {{hash=3f6ba0fa8bc8542632efe7f31c9ee2e8}{%
           family={Werbos},
           familyi={W\bibinitperiod},
           given={Paul\bibnamedelima J.},
           giveni={P\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \strng{namehash}{3f6ba0fa8bc8542632efe7f31c9ee2e8}
      \strng{fullhash}{3f6ba0fa8bc8542632efe7f31c9ee2e8}
      \strng{bibnamehash}{3f6ba0fa8bc8542632efe7f31c9ee2e8}
      \strng{authorbibnamehash}{3f6ba0fa8bc8542632efe7f31c9ee2e8}
      \strng{authornamehash}{3f6ba0fa8bc8542632efe7f31c9ee2e8}
      \strng{authorfullhash}{3f6ba0fa8bc8542632efe7f31c9ee2e8}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Backpropagation is now the most widely used tool in the field of artificial neural networks. At the core of backpropagation is a method for calculating derivatives exactly and efficiently in any large system made up of elementary subsystems or calculations which are represented by known, differentiable functions; thus, backpropagation has many applications which do not involve neural networks as such. This paper first reviews basic backpropagation, a simple method which is now being widely used in areas like pattern recognition and fault diagnosis. Next, it presents the basic equations for back-propagation through time, and discusses applications to areas like pattern recognition involving dynamic systems, systems identification, and control. Finally, it describes further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations or true recurrent networks, and other practical issues which arise with this method. Pseudocode is provided to clarify the algorithms. The chain rule forordered derivatives—the theorem which underlies backpropagation—is briefly discussed. {©} 1990, IEEE}
      \field{issn}{15582256}
      \field{journaltitle}{Proceedings of the IEEE}
      \field{number}{10}
      \field{title}{{Backpropagation Through Time: What It Does and How to Do It}}
      \field{volume}{78}
      \field{year}{1990}
      \field{pages}{1550\bibrangedash 1560}
      \range{pages}{11}
      \verb{doi}
      \verb 10.1109/5.58337
      \endverb
    \endentry
    \entry{Hinton}{report}{}
      \name{author}{2}{}{%
        {{hash=813bd95fe553e6079cd53a567b238287}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey\bibnamedelima E},
           giveni={G\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=643f92e8f89f2746a4c1aa077d225755}{%
           family={Osindero},
           familyi={O\bibinitperiod},
           given={Simon},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{cdfb09be61c835ae48863e4d278a5539}
      \strng{fullhash}{cdfb09be61c835ae48863e4d278a5539}
      \strng{bibnamehash}{cdfb09be61c835ae48863e4d278a5539}
      \strng{authorbibnamehash}{cdfb09be61c835ae48863e4d278a5539}
      \strng{authornamehash}{cdfb09be61c835ae48863e4d278a5539}
      \strng{authorfullhash}{cdfb09be61c835ae48863e4d278a5539}
      \field{sortinit}{3}
      \field{sortinithash}{a37a8ef248a93c322189792c34fc68c9}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associa-tive memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.}
      \field{title}{{A Fast Learning Algorithm for Deep Belief Nets Yee-Whye Teh}}
      \field{type}{techreport}
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Hinton, Osindero - Unknown - A Fast Learning Algorithm for Deep Belief Nets Yee-Whye Teh.pdf:pdf
      \endverb
    \endentry
    \entry{Lecun2015}{article}{}
      \name{author}{3}{}{%
        {{hash=d0ab8cbca75df7aa3e0b6024d60ac5f8}{%
           family={Lecun},
           familyi={L\bibinitperiod},
           given={Yann},
           giveni={Y\bibinitperiod}}}%
        {{hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Yoshua},
           giveni={Y\bibinitperiod}}}%
        {{hash=9a8750ccdb2a4cf14d2655face1ce016}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey},
           giveni={G\bibinitperiod}}}%
      }
      \strng{namehash}{df147ee3262b14a19bae8c3ffe34845e}
      \strng{fullhash}{df147ee3262b14a19bae8c3ffe34845e}
      \strng{bibnamehash}{df147ee3262b14a19bae8c3ffe34845e}
      \strng{authorbibnamehash}{df147ee3262b14a19bae8c3ffe34845e}
      \strng{authornamehash}{df147ee3262b14a19bae8c3ffe34845e}
      \strng{authorfullhash}{df147ee3262b14a19bae8c3ffe34845e}
      \field{sortinit}{4}
      \field{sortinithash}{e071e0bcb44634fab398d68ad04e69f4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.}
      \field{issn}{14764687}
      \field{journaltitle}{Nature}
      \field{number}{7553}
      \field{title}{{Deep learning}}
      \field{volume}{521}
      \field{year}{2015}
      \field{pages}{436\bibrangedash 444}
      \range{pages}{9}
      \verb{doi}
      \verb 10.1038/nature14539
      \endverb
      \verb{file}
      \verb :Users/ugurtigu/Downloads/nature14539.pdf:pdf
      \endverb
    \endentry
    \entry{Nielsen2015}{article}{}
      \name{author}{1}{}{%
        {{hash=17279793dcf098cfda8570111ee5cfd9}{%
           family={Nielsen},
           familyi={N\bibinitperiod},
           given={MA},
           giveni={M\bibinitperiod}}}%
      }
      \strng{namehash}{17279793dcf098cfda8570111ee5cfd9}
      \strng{fullhash}{17279793dcf098cfda8570111ee5cfd9}
      \strng{bibnamehash}{17279793dcf098cfda8570111ee5cfd9}
      \strng{authorbibnamehash}{17279793dcf098cfda8570111ee5cfd9}
      \strng{authornamehash}{17279793dcf098cfda8570111ee5cfd9}
      \strng{authorfullhash}{17279793dcf098cfda8570111ee5cfd9}
      \field{sortinit}{6}
      \field{sortinithash}{7851c86048328b027313775d8fbd2131}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{{Neural networks and deep learning}}
      \field{year}{2015}
    \endentry
    \entry{Gardner1998}{article}{}
      \name{author}{2}{}{%
        {{hash=5707ebdded50437381e5df9123e740bd}{%
           family={Gardner},
           familyi={G\bibinitperiod},
           given={M.\bibnamedelimi W.},
           giveni={M\bibinitperiod\bibinitdelim W\bibinitperiod}}}%
        {{hash=1c61530acf4dd19fcef40271ea17a5f0}{%
           family={Dorling},
           familyi={D\bibinitperiod},
           given={S.\bibnamedelimi R.},
           giveni={S\bibinitperiod\bibinitdelim R\bibinitperiod}}}%
      }
      \strng{namehash}{d3b069cf54786f3af7509cc758fbdd6c}
      \strng{fullhash}{d3b069cf54786f3af7509cc758fbdd6c}
      \strng{bibnamehash}{d3b069cf54786f3af7509cc758fbdd6c}
      \strng{authorbibnamehash}{d3b069cf54786f3af7509cc758fbdd6c}
      \strng{authornamehash}{d3b069cf54786f3af7509cc758fbdd6c}
      \strng{authorfullhash}{d3b069cf54786f3af7509cc758fbdd6c}
      \field{sortinit}{7}
      \field{sortinithash}{f615fb9c6fba11c6f962fb3fd599810e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Artificial neural networks are appearing as useful alternatives to traditional statistical modelling techniques in many scientific disciplines. This paper presents a general introduction and discussion of recent applications of the multilayer perceptron, one type of artificial neural network, in the atmospheric sciences.}
      \field{issn}{13522310}
      \field{journaltitle}{Atmospheric Environment}
      \field{number}{14-15}
      \field{title}{{Artificial neural networks (the multilayer perceptron) - a review of applications in the atmospheric sciences}}
      \field{volume}{32}
      \field{year}{1998}
      \field{pages}{2627\bibrangedash 2636}
      \range{pages}{10}
      \verb{doi}
      \verb 10.1016/S1352-2310(97)00447-0
      \endverb
      \verb{file}
      \verb :Users/ugurtigu/Downloads/s1352-2310(97)00447-0.pdf:pdf
      \endverb
      \keyw{Artificial intelligence,Backpropagation,Neural network,Statistical modelling}
    \endentry
    \entry{Chollet2017}{book}{}
      \name{author}{1}{}{%
        {{hash=5836fc1fa037e340c8b5591da3207608}{%
           family={Chollet},
           familyi={C\bibinitperiod},
           given={Francois},
           giveni={F\bibinitperiod}}}%
      }
      \strng{namehash}{5836fc1fa037e340c8b5591da3207608}
      \strng{fullhash}{5836fc1fa037e340c8b5591da3207608}
      \strng{bibnamehash}{5836fc1fa037e340c8b5591da3207608}
      \strng{authorbibnamehash}{5836fc1fa037e340c8b5591da3207608}
      \strng{authornamehash}{5836fc1fa037e340c8b5591da3207608}
      \strng{authorfullhash}{5836fc1fa037e340c8b5591da3207608}
      \field{sortinit}{1}
      \field{sortinithash}{50c6687d7fc80f50136d75228e3c59ba}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{isbn}{9780996452762}
      \field{title}{{Deep learning with Python}}
      \field{year}{2017}
      \verb{doi}
      \verb 10.23919/ICIF.2018.8455530
      \endverb
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Chollet - 2017 - Deep learning with Python.pdf:pdf
      \endverb
    \endentry
    \entry{AntonioGuili;AmitaKapoor;SujitPal2019}{book}{}
      \name{author}{1}{}{%
        {{hash=5be3f653849a463589c05d6840a52369}{%
           family={{Antonio Guili; Amita Kapoor; Sujit Pal}},
           familyi={A\bibinitperiod}}}%
      }
      \strng{namehash}{5be3f653849a463589c05d6840a52369}
      \strng{fullhash}{5be3f653849a463589c05d6840a52369}
      \strng{bibnamehash}{5be3f653849a463589c05d6840a52369}
      \strng{authorbibnamehash}{5be3f653849a463589c05d6840a52369}
      \strng{authornamehash}{5be3f653849a463589c05d6840a52369}
      \strng{authorfullhash}{5be3f653849a463589c05d6840a52369}
      \field{sortinit}{1}
      \field{sortinithash}{50c6687d7fc80f50136d75228e3c59ba}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{2nd ed. Predicting house price using linear regression Deep Learning with TensorFlow 2 and Keras, Second Edition teaches deep learning techniques alongside TensorFlow (TF) and Keras. The book introduces neural networks with TensorFlow, runs through the main applications, covers two working example apps, and then dives into TF and cloudin production, TF mobile, and using TensorFlow with AutoML. Cover -- Copyright -- Packt Page -- Contributors -- Table of Contents -- Preface -- Chapter 1: Neural Network Foundations with TensorFlow 2.0 -- What is TensorFlow (TF)? -- What is Keras? -- What are the most important changes in TensorFlow 2.0? -- Introduction to neural networks -- Perceptron -- A first example of TensorFlow 2.0 code -- Multi-layer perceptron -- our first example of a network -- Problems in training the perceptron and their solutions -- Activation function -- sigmoid -- Activation function -- tanh -- Activation function -- ReLU Two additional activation functions -- ELU and LeakyReLU -- Activation functions -- In short -- what are neural networks after all? -- A real example -- recognizing handwritten digits -- One-hot encoding (OHE) -- Defining a simple neural network in TensorFlow 2.0 -- Running a simple TensorFlow 2.0 net and establishing a baseline -- Improving the simple net in TensorFlow 2.0 with hidden layers -- Further improving the simple net in TensorFlow with Dropout -- Testing different optimizers in TensorFlow 2.0 -- Increasing the number of epochs -- Controlling the optimizer learning rate Increasing the number of internal hidden neurons -- Increasing the size of batch computation -- Summarizing experiments run for recognizing handwritten charts -- Regularization -- Adopting regularization to avoid overfitting -- Understanding BatchNormalization -- Playing with Google Colab -- CPUs, GPUs, and TPUs -- Sentiment analysis -- Hyperparameter tuning and AutoML -- Predicting output -- A practical overview of backpropagation -- What have we learned so far? -- Towards a deep learning approach -- References -- Chapter 2: TensorFlow 1.x and 2.x -- Understanding TensorFlow 1.x TensorFlow 1.x computational graph program structure -- Computational graphs -- Working with constants, variables, and placeholders -- Examples of operations -- Constants -- Sequences -- Random tensors -- Variables -- An example of TensorFlow 1.x in TensorFlow 2.x -- Understanding TensorFlow 2.x -- Eager execution -- AutoGraph -- Keras APIs -- three programming models -- Sequential API -- Functional API -- Model subclassing -- Callbacks -- Saving a model and weights -- Training from tf.data.datasets -- tf.keras or Estimators? -- Ragged tensors -- Custom training Distributed training in TensorFlow 2.x -- Multiple GPUs -- MultiWorkerMirroredStrategy -- TPUStrategy -- ParameterServerStrategy -- Changes in namespaces -- Converting from 1.x to 2.x -- Using TensorFlow 2.x effectively -- The TensorFlow 2.x ecosystem -- Language bindings -- Keras or tf.keras? -- Summary -- Chapter 3: Regression -- What is regression? -- Prediction using linear regression -- Simple linear regression -- Multiple linear regression -- Multivariate linear regression -- TensorFlow Estimators -- Feature columns -- Input functions -- MNIST using TensorFlow Estimator API}
      \field{isbn}{9781838823412}
      \field{title}{{Deep Learning with TensorFlow 2 and Keras: Regression, ConvNets, GANs, RNNs, NLP, and More with TensorFlow 2 and the Keras API}}
      \field{year}{2019}
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Antonio Guili Amita Kapoor Sujit Pal - 2019 - Deep Learning with TensorFlow 2 and Keras Regression, ConvNets, GANs, RNNs, NLP, and More.pdf:pdf
      \endverb
    \endentry
    \entry{IanGoodfellowYoshuaBengio2016}{book}{}
      \name{author}{1}{}{%
        {{hash=66d6169bd5781eda3520264ffcff3d02}{%
           family={{Ian Goodfellow, Yoshua Bengio}},
           familyi={I\bibinitperiod},
           given={Aaron\bibnamedelima Courville},
           giveni={A\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
      }
      \strng{namehash}{66d6169bd5781eda3520264ffcff3d02}
      \strng{fullhash}{66d6169bd5781eda3520264ffcff3d02}
      \strng{bibnamehash}{66d6169bd5781eda3520264ffcff3d02}
      \strng{authorbibnamehash}{66d6169bd5781eda3520264ffcff3d02}
      \strng{authornamehash}{66d6169bd5781eda3520264ffcff3d02}
      \strng{authorfullhash}{66d6169bd5781eda3520264ffcff3d02}
      \field{sortinit}{1}
      \field{sortinithash}{50c6687d7fc80f50136d75228e3c59ba}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{Prmu}
      \field{title}{{Deep Learning}}
      \field{year}{2016}
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Ian Goodfellow, Yoshua Bengio - 2016 - Deep Learning.pdf:pdf
      \endverb
    \endentry
    \entry{Ruder2016}{article}{}
      \name{author}{1}{}{%
        {{hash=b468248a20d75c52ee742f4592c2569f}{%
           family={Ruder},
           familyi={R\bibinitperiod},
           given={Sebastian},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{b468248a20d75c52ee742f4592c2569f}
      \strng{fullhash}{b468248a20d75c52ee742f4592c2569f}
      \strng{bibnamehash}{b468248a20d75c52ee742f4592c2569f}
      \strng{authorbibnamehash}{b468248a20d75c52ee742f4592c2569f}
      \strng{authornamehash}{b468248a20d75c52ee742f4592c2569f}
      \strng{authorfullhash}{b468248a20d75c52ee742f4592c2569f}
      \field{sortinit}{1}
      \field{sortinithash}{50c6687d7fc80f50136d75228e3c59ba}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.}
      \field{eprinttype}{arXiv}
      \field{month}{9}
      \field{title}{{An overview of gradient descent optimization algorithms}}
      \field{year}{2016}
      \verb{eprint}
      \verb 1609.04747
      \endverb
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Ruder - 2016 - An overview of gradient descent optimization algorithms.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb http://arxiv.org/abs/1609.04747
      \endverb
      \verb{url}
      \verb http://arxiv.org/abs/1609.04747
      \endverb
    \endentry
    \entry{hackernoon}{misc}{}
      \field{sortinit}{1}
      \field{sortinithash}{50c6687d7fc80f50136d75228e3c59ba}
      \field{labeltitlesource}{title}
      \field{title}{{Gradient Descent: All You Need to Know | Hacker Noon}}
      \field{urlday}{24}
      \field{urlmonth}{12}
      \field{urlyear}{2020}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb https://hackernoon.com/gradient-descent-aynk-7cbe95a778da
      \endverb
      \verb{url}
      \verb https://hackernoon.com/gradient-descent-aynk-7cbe95a778da
      \endverb
    \endentry
    \entry{Patterson2019}{book}{}
      \name{author}{2}{}{%
        {{hash=b437b3540ca6290b1af286f81e1918f1}{%
           family={Patterson},
           familyi={P\bibinitperiod},
           given={Josh},
           giveni={J\bibinitperiod}}}%
        {{hash=6cc15cc3c2db75f3b76f461a16e7acd5}{%
           family={Gibson},
           familyi={G\bibinitperiod},
           given={Adam},
           giveni={A\bibinitperiod}}}%
      }
      \strng{namehash}{57d3e134e29f75914b76c446039ab8bf}
      \strng{fullhash}{57d3e134e29f75914b76c446039ab8bf}
      \strng{bibnamehash}{57d3e134e29f75914b76c446039ab8bf}
      \strng{authorbibnamehash}{57d3e134e29f75914b76c446039ab8bf}
      \strng{authornamehash}{57d3e134e29f75914b76c446039ab8bf}
      \strng{authorfullhash}{57d3e134e29f75914b76c446039ab8bf}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{"Deep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and video games. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors"--Page 4 of cover. Introduction -- Applied math and machine learning basics. Linear algebra -- Probability and information theory -- Numerical computation -- Machine learning basics -- Deep networks: modern practices. Deep feedforward networks -- Regularization for deep learning -- Optimization for training deep models -- Convolutional networks -- Sequence modeling: recurrent and recursive nets -- Practical methodology -- Applications -- Deep learning research. Linear factor models -- Autoencoders -- Representation learning -- Structured probabilistic models for deep learning -- Monte Carlo methods -- Confronting the partition function -- Approximate inference -- Deep generative models.}
      \field{booktitle}{Nature}
      \field{isbn}{3463353563306}
      \field{number}{7553}
      \field{title}{{Deep Learning a Practitioner'S Approach}}
      \field{volume}{29}
      \field{year}{2019}
      \field{pages}{1\bibrangedash 73}
      \range{pages}{73}
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Patterson, Gibson - 2019 - Deep Learning a Practitioner'S Approach.pdf:pdf
      \endverb
    \endentry
    \entry{StanfordUniversityCoursecs231n2018}{article}{}
      \name{author}{1}{}{%
        {{hash=9fa88a35c4186a2afcb31d783ca1890c}{%
           family={{Stanford University Course cs231n}},
           familyi={S\bibinitperiod}}}%
      }
      \strng{namehash}{9fa88a35c4186a2afcb31d783ca1890c}
      \strng{fullhash}{9fa88a35c4186a2afcb31d783ca1890c}
      \strng{bibnamehash}{9fa88a35c4186a2afcb31d783ca1890c}
      \strng{authorbibnamehash}{9fa88a35c4186a2afcb31d783ca1890c}
      \strng{authornamehash}{9fa88a35c4186a2afcb31d783ca1890c}
      \strng{authorfullhash}{9fa88a35c4186a2afcb31d783ca1890c}
      \field{extraname}{1}
      \field{sortinit}{2}
      \field{sortinithash}{ed39bb39cf854d5250e95b1c1f94f4ed}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Convolutional Neural Networks are very similar to ordinary Neural Networks from the previous chapter: they are made up of neurons that have learnable weights and biases. Each neuron receives some inputs, performs a dot product and optionally follows it with a non-linearity. The whole network still expresses a single differentiable score function: from the raw image pixels on one end to class scores at the other. And they still have a loss function (e.g. SVM/Softmax) on the last (fully-connected) layer and all the tips/tricks we developed for learning regular Neural Networks still apply. So what does change? ConvNet architectures make the explicit assumption that the inputs are images, which allows us to encode certain properties into the architecture. These then make the forward function more efficient to implement and vastly reduce the amount of parameters in the network.}
      \field{journaltitle}{Stanford University Course cs231n}
      \field{title}{{CS231n Convolutional Neural Networks for Visual Recognition}}
      \field{year}{2018}
      \field{pages}{30}
      \range{pages}{1}
      \verb{urlraw}
      \verb https://cs231n.github.io/neural-networks-3/ http://cs231n.github.io/convolutional-networks/{\%}0Ahttp://cs231n.github.io/neural-networks-3/
      \endverb
      \verb{url}
      \verb https://cs231n.github.io/neural-networks-3/%20http://cs231n.github.io/convolutional-networks/%7B%5C%%7D0Ahttp://cs231n.github.io/neural-networks-3/
      \endverb
    \endentry
    \entry{Srivastava2014}{report}{}
      \name{author}{4}{}{%
        {{hash=6a147afa4569ce6cf23c0436e65d8486}{%
           family={Srivastava},
           familyi={S\bibinitperiod},
           given={Nitish},
           giveni={N\bibinitperiod}}}%
        {{hash=9a8750ccdb2a4cf14d2655face1ce016}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey},
           giveni={G\bibinitperiod}}}%
        {{hash=c5e3a676e2ac1164b3afcd539c131fc9}{%
           family={Krizhevsky},
           familyi={K\bibinitperiod},
           given={Alex},
           giveni={A\bibinitperiod}}}%
        {{hash=bd2be300d445e9f6db7808f9533e66cb}{%
           family={Salakhutdinov},
           familyi={S\bibinitperiod},
           given={Ruslan},
           giveni={R\bibinitperiod}}}%
      }
      \strng{namehash}{8eeaede68a96eb301b76575348cd4b07}
      \strng{fullhash}{a4e8289d0cb36166764c7993dfb7ec92}
      \strng{bibnamehash}{a4e8289d0cb36166764c7993dfb7ec92}
      \strng{authorbibnamehash}{a4e8289d0cb36166764c7993dfb7ec92}
      \strng{authornamehash}{8eeaede68a96eb301b76575348cd4b07}
      \strng{authorfullhash}{a4e8289d0cb36166764c7993dfb7ec92}
      \field{sortinit}{3}
      \field{sortinithash}{a37a8ef248a93c322189792c34fc68c9}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.}
      \field{booktitle}{Journal of Machine Learning Research}
      \field{title}{{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}}
      \field{type}{techreport}
      \field{volume}{15}
      \field{year}{2014}
      \field{pages}{1929\bibrangedash 1958}
      \range{pages}{30}
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf
      \endverb
      \keyw{deep learning,model combination,neural networks,regularization}
    \endentry
    \entry{Aggarwal2018}{book}{}
      \name{author}{1}{}{%
        {{hash=86d11a07b1bfeac58dc68f7a419b3039}{%
           family={Aggarwal},
           familyi={A\bibinitperiod},
           given={Charu\bibnamedelima C.},
           giveni={C\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {Springer International Publishing}%
      }
      \strng{namehash}{86d11a07b1bfeac58dc68f7a419b3039}
      \strng{fullhash}{86d11a07b1bfeac58dc68f7a419b3039}
      \strng{bibnamehash}{86d11a07b1bfeac58dc68f7a419b3039}
      \strng{authorbibnamehash}{86d11a07b1bfeac58dc68f7a419b3039}
      \strng{authornamehash}{86d11a07b1bfeac58dc68f7a419b3039}
      \strng{authorfullhash}{86d11a07b1bfeac58dc68f7a419b3039}
      \field{sortinit}{4}
      \field{sortinithash}{e071e0bcb44634fab398d68ad04e69f4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This book covers both classical and modern models in deep learning. The primary focus is on the theory and algorithms of deep learning. The theory and algorithms of neural networks are particularly important for understanding important concepts, so that one can understand the important design concepts of neural architectures in different applications. Why do neural networks work? When do they work better than off-the-shelf machine-learning models? When is depth useful? Why is training neural networks so hard? What are the pitfalls? The book is also rich in discussing different applications in order to give the practitioner a flavor of how neural architectures are designed for different types of problems. Applications associated with many different areas like recommender systems, machine translation, image captioning, image classification, reinforcement-learning based gaming, and text analytics are covered. The chapters of this book span three categories: The basics of neural networks: Many traditional machine learning models can be understood as special cases of neural networks. An emphasis is placed in the first two chapters on understanding the relationship between traditional machine learning and neural networks. Support vector machines, linear/logistic regression, singular value decomposition, matrix factorization, and recommender systems are shown to be special cases of neural networks. These methods are studied together with recent feature engineering methods like word2vec. Fundamentals of neural networks: A detailed discussion of training and regularization is provided in Chapters 3 and 4. Chapters 5 and 6 present radial-basis function (RBF) networks and restricted Boltzmann machines. Advanced topics in neural networks: Chapters 7 and 8 discuss recurrent neural networks and convolutional neural networks. Several advanced topics like deep reinforcement learning, neural Turing machines, Kohonen self-organizing maps, and generative adversarial networks are introduced in Chapters 9 and 10. The book is written for graduate students, researchers, and practitioners. Numerous exercises are available along with a solution manual to aid in classroom teaching. Where possible, an application-centric view is highlighted in order to provide an understanding of the practical uses of each class of techniques.}
      \field{booktitle}{Neural Networks and Deep Learning}
      \field{title}{{Neural Networks and Deep Learning}}
      \field{year}{2018}
      \verb{doi}
      \verb 10.1007/978-3-319-94463-0
      \endverb
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Aggarwal - 2018 - Neural Networks and Deep Learning.pdf:pdf
      \endverb
    \endentry
    \entry{StanfordUniversityCoursecs231n2018a}{article}{}
      \name{author}{1}{}{%
        {{hash=9fa88a35c4186a2afcb31d783ca1890c}{%
           family={{Stanford University Course cs231n}},
           familyi={S\bibinitperiod}}}%
      }
      \strng{namehash}{9fa88a35c4186a2afcb31d783ca1890c}
      \strng{fullhash}{9fa88a35c4186a2afcb31d783ca1890c}
      \strng{bibnamehash}{9fa88a35c4186a2afcb31d783ca1890c}
      \strng{authorbibnamehash}{9fa88a35c4186a2afcb31d783ca1890c}
      \strng{authornamehash}{9fa88a35c4186a2afcb31d783ca1890c}
      \strng{authorfullhash}{9fa88a35c4186a2afcb31d783ca1890c}
      \field{extraname}{2}
      \field{sortinit}{4}
      \field{sortinithash}{e071e0bcb44634fab398d68ad04e69f4}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Convolutional Neural Networks are very similar to ordinary Neural Networks from the previous chapter: they are made up of neurons that have learnable weights and biases. Each neuron receives some inputs, performs a dot product and optionally follows it with a non-linearity. The whole network still expresses a single differentiable score function: from the raw image pixels on one end to class scores at the other. And they still have a loss function (e.g. SVM/Softmax) on the last (fully-connected) layer and all the tips/tricks we developed for learning regular Neural Networks still apply. So what does change? ConvNet architectures make the explicit assumption that the inputs are images, which allows us to encode certain properties into the architecture. These then make the forward function more efficient to implement and vastly reduce the amount of parameters in the network.}
      \field{journaltitle}{Stanford University Course cs231n}
      \field{title}{{CS231n Convolutional Neural Networks for Visual Recognition}}
      \field{year}{2018}
      \field{pages}{30}
      \range{pages}{1}
      \verb{urlraw}
      \verb https://cs231n.github.io/convolutional-networks/
      \endverb
      \verb{url}
      \verb https://cs231n.github.io/convolutional-networks/
      \endverb
    \endentry
    \entry{Liddy}{report}{}
      \name{author}{1}{}{%
        {{hash=f58f8bb3262004b24e666c5ff9ed930f}{%
           family={Liddy},
           familyi={L\bibinitperiod},
           given={Elizabeth\bibnamedelima D},
           giveni={E\bibinitperiod\bibinitdelim D\bibinitperiod}}}%
      }
      \strng{namehash}{f58f8bb3262004b24e666c5ff9ed930f}
      \strng{fullhash}{f58f8bb3262004b24e666c5ff9ed930f}
      \strng{bibnamehash}{f58f8bb3262004b24e666c5ff9ed930f}
      \strng{authorbibnamehash}{f58f8bb3262004b24e666c5ff9ed930f}
      \strng{authornamehash}{f58f8bb3262004b24e666c5ff9ed930f}
      \strng{authorfullhash}{f58f8bb3262004b24e666c5ff9ed930f}
      \field{sortinit}{5}
      \field{sortinithash}{5dd416adbafacc8226114bc0202d5fdd}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{{SURFACE SURFACE Center for Natural Language Processing School of Information Studies (iSchool) 2001 Natural Language Processing Natural Language Processing Natural Language Processing 1}}
      \field{type}{techreport}
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Liddy - Unknown - SURFACE SURFACE Center for Natural Language Processing School of Information Studies (iSchool) 2001 Natural Language P.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://surface.syr.edu/cnlp
      \endverb
      \verb{url}
      \verb https://surface.syr.edu/cnlp
      \endverb
    \endentry
    \entry{Bengio2003}{report}{}
      \name{author}{9}{}{%
        {{hash=40a8e4774982146adc2688546f54efb2}{%
           family={Bengio},
           familyi={B\bibinitperiod},
           given={Yoshua},
           giveni={Y\bibinitperiod}}}%
        {{hash=dcb65deeb8d2de03d00f0aeeb3499771}{%
           family={Ducharme},
           familyi={D\bibinitperiod},
           given={Réjean},
           giveni={R\bibinitperiod}}}%
        {{hash=da21e966c02c3cfd33d74369c7435c1a}{%
           family={Vincent},
           familyi={V\bibinitperiod},
           given={Pascal},
           giveni={P\bibinitperiod}}}%
        {{hash=bd9126e06687017f43910a8eb056bb0a}{%
           family={Jauvin},
           familyi={J\bibinitperiod},
           given={Christian},
           giveni={C\bibinitperiod}}}%
        {{hash=fd22dc084c72d3665935e5c76cd1579a}{%
           family={Ca},
           familyi={C\bibinitperiod},
           given={Jauvinc@iro\bibnamedelima Umontreal},
           giveni={J\bibinitperiod\bibinitdelim U\bibinitperiod}}}%
        {{hash=7a76df59366b2cd67950e21661526bdf}{%
           family={Kandola},
           familyi={K\bibinitperiod},
           given={Jaz},
           giveni={J\bibinitperiod}}}%
        {{hash=ee42cfed2ff3020233d2de97c356eaee}{%
           family={Hofmann},
           familyi={H\bibinitperiod},
           given={Thomas},
           giveni={T\bibinitperiod}}}%
        {{hash=f65eec80d26b2ab02398d1e78643c9a6}{%
           family={Poggio},
           familyi={P\bibinitperiod},
           given={Tomaso},
           giveni={T\bibinitperiod}}}%
        {{hash=8feae4ec394a5f6905c74b1a7abff875}{%
           family={Shawe-Taylor},
           familyi={S\bibinithyphendelim T\bibinitperiod},
           given={John},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{725bfe7fa99eb3477fe935c2ef46ed6a}
      \strng{fullhash}{68e68897d128ca68159c0a4396171233}
      \strng{bibnamehash}{68e68897d128ca68159c0a4396171233}
      \strng{authorbibnamehash}{68e68897d128ca68159c0a4396171233}
      \strng{authornamehash}{725bfe7fa99eb3477fe935c2ef46ed6a}
      \strng{authorfullhash}{68e68897d128ca68159c0a4396171233}
      \field{sortinit}{6}
      \field{sortinithash}{7851c86048328b027313775d8fbd2131}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.}
      \field{booktitle}{Journal of Machine Learning Research}
      \field{title}{{A Neural Probabilistic Language Model}}
      \field{type}{techreport}
      \field{volume}{3}
      \field{year}{2003}
      \field{pages}{1137\bibrangedash 1155}
      \range{pages}{19}
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Bengio et al. - 2003 - A Neural Probabilistic Language Model.pdf:pdf
      \endverb
      \keyw{Statistical language modeling,artificial neural networks,curse of dimensionality,distributed representation}
    \endentry
    \entry{Rumelhart1986}{article}{}
      \name{author}{3}{}{%
        {{hash=55cd5380bb5b15032a6d5a2015f56e3f}{%
           family={Rumelhart},
           familyi={R\bibinitperiod},
           given={David\bibnamedelima E.},
           giveni={D\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=813bd95fe553e6079cd53a567b238287}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey\bibnamedelima E.},
           giveni={G\bibinitperiod\bibinitdelim E\bibinitperiod}}}%
        {{hash=6cbc29ad7fd57ffdb9ed4728418fd988}{%
           family={Williams},
           familyi={W\bibinitperiod},
           given={Ronald\bibnamedelima J.},
           giveni={R\bibinitperiod\bibinitdelim J\bibinitperiod}}}%
      }
      \strng{namehash}{fd75cea68d6982b503a0cd84f7cc7b51}
      \strng{fullhash}{fd75cea68d6982b503a0cd84f7cc7b51}
      \strng{bibnamehash}{fd75cea68d6982b503a0cd84f7cc7b51}
      \strng{authorbibnamehash}{fd75cea68d6982b503a0cd84f7cc7b51}
      \strng{authornamehash}{fd75cea68d6982b503a0cd84f7cc7b51}
      \strng{authorfullhash}{fd75cea68d6982b503a0cd84f7cc7b51}
      \field{sortinit}{6}
      \field{sortinithash}{7851c86048328b027313775d8fbd2131}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure 1. {©} 1986 Nature Publishing Group.}
      \field{issn}{00280836}
      \field{journaltitle}{Nature}
      \field{number}{6088}
      \field{title}{{Learning representations by back-propagating errors}}
      \field{volume}{323}
      \field{year}{1986}
      \verb{doi}
      \verb 10.1038/323533a0
      \endverb
    \endentry
    \entry{Jurafskya}{report}{}
      \name{author}{2}{}{%
        {{hash=bd33d7a70284b5ead9c7985cb5c9929c}{%
           family={Jurafsky},
           familyi={J\bibinitperiod},
           given={Daniel},
           giveni={D\bibinitperiod}}}%
        {{hash=9ed8bee488b2703da341cded66bf83cb}{%
           family={Martin},
           familyi={M\bibinitperiod},
           given={James\bibnamedelima H},
           giveni={J\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
      }
      \strng{namehash}{b5e6a2accdc2023c51f42c6ec010d19b}
      \strng{fullhash}{b5e6a2accdc2023c51f42c6ec010d19b}
      \strng{bibnamehash}{b5e6a2accdc2023c51f42c6ec010d19b}
      \strng{authorbibnamehash}{b5e6a2accdc2023c51f42c6ec010d19b}
      \strng{authornamehash}{b5e6a2accdc2023c51f42c6ec010d19b}
      \strng{authorfullhash}{b5e6a2accdc2023c51f42c6ec010d19b}
      \field{sortinit}{6}
      \field{sortinithash}{7851c86048328b027313775d8fbd2131}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{{Speech and Language Processing An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition Third Edition draft}}
      \field{type}{techreport}
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Jurafsky, Martin - Unknown - Speech and Language Processing An Introduction to Natural Language Processing, Computational Linguistics, a.pdf:pdf
      \endverb
    \endentry
    \entry{wordemdgood}{misc}{}
      \field{sortinit}{6}
      \field{sortinithash}{7851c86048328b027313775d8fbd2131}
      \field{labeltitlesource}{title}
      \field{title}{{Embeddings: Translating to a Lower-Dimensional Space}}
      \field{urlday}{15}
      \field{urlmonth}{12}
      \field{urlyear}{2020}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space
      \endverb
      \verb{url}
      \verb https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space
      \endverb
    \endentry
    \entry{Mikolov2013}{report}{}
      \name{author}{4}{}{%
        {{hash=a2d359b12ca2fadf0b40136a73f021bb}{%
           family={Mikolov},
           familyi={M\bibinitperiod},
           given={Tomas},
           giveni={T\bibinitperiod}}}%
        {{hash=ee3f7d7b96add98106db907e189d6c13}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Kai},
           giveni={K\bibinitperiod}}}%
        {{hash=3d11e6f2a0d0a1183b2cf62996525afc}{%
           family={Corrado},
           familyi={C\bibinitperiod},
           given={Greg},
           giveni={G\bibinitperiod}}}%
        {{hash=4aecfb0cc2e1e3b7899129fa2a94e2b8}{%
           family={Dean},
           familyi={D\bibinitperiod},
           given={Jeffrey},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{ed4d154f6ad724ee86e4b498231228f3}
      \strng{fullhash}{f24c60896b6daa69474b40efb61f4e88}
      \strng{bibnamehash}{f24c60896b6daa69474b40efb61f4e88}
      \strng{authorbibnamehash}{f24c60896b6daa69474b40efb61f4e88}
      \strng{authornamehash}{ed4d154f6ad724ee86e4b498231228f3}
      \strng{authorfullhash}{f24c60896b6daa69474b40efb61f4e88}
      \field{extraname}{1}
      \field{sortinit}{6}
      \field{sortinithash}{7851c86048328b027313775d8fbd2131}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.}
      \field{eprinttype}{arXiv}
      \field{title}{{Distributed Representations of Words and Phrases and their Compositionality}}
      \field{type}{techreport}
      \field{year}{2013}
      \verb{eprint}
      \verb 1310.4546v1
      \endverb
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf
      \endverb
      \keyw{()}
    \endentry
    \entry{Mikolov}{report}{}
      \name{author}{4}{}{%
        {{hash=a2d359b12ca2fadf0b40136a73f021bb}{%
           family={Mikolov},
           familyi={M\bibinitperiod},
           given={Tomas},
           giveni={T\bibinitperiod}}}%
        {{hash=ee3f7d7b96add98106db907e189d6c13}{%
           family={Chen},
           familyi={C\bibinitperiod},
           given={Kai},
           giveni={K\bibinitperiod}}}%
        {{hash=3d11e6f2a0d0a1183b2cf62996525afc}{%
           family={Corrado},
           familyi={C\bibinitperiod},
           given={Greg},
           giveni={G\bibinitperiod}}}%
        {{hash=4aecfb0cc2e1e3b7899129fa2a94e2b8}{%
           family={Dean},
           familyi={D\bibinitperiod},
           given={Jeffrey},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{ed4d154f6ad724ee86e4b498231228f3}
      \strng{fullhash}{f24c60896b6daa69474b40efb61f4e88}
      \strng{bibnamehash}{f24c60896b6daa69474b40efb61f4e88}
      \strng{authorbibnamehash}{f24c60896b6daa69474b40efb61f4e88}
      \strng{authornamehash}{ed4d154f6ad724ee86e4b498231228f3}
      \strng{authorfullhash}{f24c60896b6daa69474b40efb61f4e88}
      \field{extraname}{2}
      \field{sortinit}{7}
      \field{sortinithash}{f615fb9c6fba11c6f962fb3fd599810e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.}
      \field{eprinttype}{arXiv}
      \field{title}{{Efficient Estimation of Word Representations in Vector Space}}
      \field{type}{techreport}
      \verb{eprint}
      \verb 1301.3781v3
      \endverb
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Mikolov et al. - Unknown - Efficient Estimation of Word Representations in Vector Space.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb http://ronan.collobert.com/senna/
      \endverb
      \verb{url}
      \verb http://ronan.collobert.com/senna/
      \endverb
    \endentry
    \entry{Zhang}{report}{}
      \name{author}{2}{}{%
        {{hash=5ddec46e090404e1224890533cbaa882}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Ye},
           giveni={Y\bibinitperiod}}}%
        {{hash=dcf3fece96fc55df5a003eccf64d2961}{%
           family={Wallace},
           familyi={W\bibinitperiod},
           given={Byron\bibnamedelima C},
           giveni={B\bibinitperiod\bibinitdelim C\bibinitperiod}}}%
      }
      \strng{namehash}{615367a133b779975ce17728b05f6a32}
      \strng{fullhash}{615367a133b779975ce17728b05f6a32}
      \strng{bibnamehash}{615367a133b779975ce17728b05f6a32}
      \strng{authorbibnamehash}{615367a133b779975ce17728b05f6a32}
      \strng{authornamehash}{615367a133b779975ce17728b05f6a32}
      \strng{authorfullhash}{615367a133b779975ce17728b05f6a32}
      \field{sortinit}{7}
      \field{sortinithash}{f615fb9c6fba11c6f962fb3fd599810e}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Convolutional Neural Networks (CNNs) have recently achieved remarkably strong performance on the practically important task of sentence classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2014). However, these models require practitioners to specify an exact model architecture and set accompanying hyperparameters, including the filter region size, regularization parameters, and so on. It is currently unknown how sensitive model performance is to changes in these configurations for the task of sentence classification. We thus conduct a sensitivity analysis of one-layer CNNs to explore the effect of architecture components on model performance; our aim is to distinguish between important and comparatively inconsequential design decisions for sentence classification. We focus on one-layer CNNs (to the exclusion of more complex models) due to their comparative simplicity and strong empirical performance, which makes it a modern standard baseline method akin to Support Vector Machine (SVMs) and logistic regression. We derive practical advice from our extensive empirical results for those interested in getting the most out of CNNs for sentence classification in real world settings.}
      \field{eprinttype}{arXiv}
      \field{title}{{A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification}}
      \field{type}{techreport}
      \verb{eprint}
      \verb 1510.03820v4
      \endverb
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Wallace - Unknown - A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classifica.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb http://nlp.stanford.edu/projects/
      \endverb
      \verb{url}
      \verb http://nlp.stanford.edu/projects/
      \endverb
    \endentry
    \entry{widlml}{misc}{}
      \name{author}{2}{}{%
        {{hash=2823227040db0fb9953a68c781f0565c}{%
           family={{Francois Chaubard}},
           familyi={F\bibinitperiod},
           given={Rohit},
           giveni={R\bibinitperiod}}}%
        {{hash=769cda2e6c89ce1142647f575d271fcb}{%
           family={Mundra},
           familyi={M\bibinitperiod},
           given={Richard\bibnamedelima Socher},
           giveni={R\bibinitperiod\bibinitdelim S\bibinitperiod}}}%
      }
      \strng{namehash}{c20057159afdd8aec0393f6e86c0710b}
      \strng{fullhash}{c20057159afdd8aec0393f6e86c0710b}
      \strng{bibnamehash}{c20057159afdd8aec0393f6e86c0710b}
      \strng{authorbibnamehash}{c20057159afdd8aec0393f6e86c0710b}
      \strng{authornamehash}{c20057159afdd8aec0393f6e86c0710b}
      \strng{authorfullhash}{c20057159afdd8aec0393f6e86c0710b}
      \field{sortinit}{8}
      \field{sortinithash}{1b24cab5087933ef0826a7cd3b99e994}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{title}{{Understanding Convolutional Neural Networks for NLP – WildML}}
      \field{urlday}{15}
      \field{urlmonth}{12}
      \field{urlyear}{2020}
      \field{urldateera}{ce}
      \verb{urlraw}
      \verb http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/
      \endverb
      \verb{url}
      \verb http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/
      \endverb
    \endentry
    \entry{Chakrabortya}{report}{}
      \name{author}{4}{}{%
        {{hash=0e8a2aad04b25dd40f944226d1bb7f15}{%
           family={Chakraborty},
           familyi={C\bibinitperiod},
           given={Abhijnan},
           giveni={A\bibinitperiod}}}%
        {{hash=d8e8976ad7ec5848c2f5557280aa5cdf}{%
           family={Paranjape},
           familyi={P\bibinitperiod},
           given={Bhargavi},
           giveni={B\bibinitperiod}}}%
        {{hash=ba7d487ccad20c25ee563dfa70d103b7}{%
           family={Kakarla},
           familyi={K\bibinitperiod},
           given={Sourya},
           giveni={S\bibinitperiod}}}%
        {{hash=fe765cd9cbf00c08505a78acccd90fb9}{%
           family={Ganguly},
           familyi={G\bibinitperiod},
           given={Niloy},
           giveni={N\bibinitperiod}}}%
      }
      \strng{namehash}{68658d1e359f50a29e3d1dc0800b5a38}
      \strng{fullhash}{a87ad7e2cb48cb880d3a44cd1a867d53}
      \strng{bibnamehash}{a87ad7e2cb48cb880d3a44cd1a867d53}
      \strng{authorbibnamehash}{a87ad7e2cb48cb880d3a44cd1a867d53}
      \strng{authornamehash}{68658d1e359f50a29e3d1dc0800b5a38}
      \strng{authorfullhash}{a87ad7e2cb48cb880d3a44cd1a867d53}
      \field{sortinit}{8}
      \field{sortinithash}{1b24cab5087933ef0826a7cd3b99e994}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Most of the online news media outlets rely heavily on the revenues generated from the clicks made by their readers, and due to the presence of numerous such outlets, they need to compete with each other for reader attention. To attract the readers to click on an article and subsequently visit the media site, the outlets often come up with catchy headlines accompanying the article links, which lure the readers to click on the link. Such headlines are known as Clickbaits. While these baits may trick the readers into clicking, in the long-run, clickbaits usually don't live up to the expectation of the readers, and leave them disappointed. In this work, we attempt to automatically detect clickbaits and then build a browser extension which warns the readers of different media sites about the possibility of being baited by such headlines. The extension also offers each reader an option to block clickbaits she doesn't want to see. Then, using such reader choices, the extension automatically blocks similar clickbaits during her future visits. We run extensive offline and online experiments across multiple media sites and find that the proposed clickbait detection and the personalized blocking approaches perform very well achieving 93{\%} accuracy in detecting and 89{\%} accuracy in blocking clickbaits.}
      \field{eprinttype}{arXiv}
      \field{title}{{Stop Clickbait: Detecting and Preventing Clickbaits in Online News Media}}
      \field{type}{techreport}
      \verb{eprint}
      \verb 1610.09786v1
      \endverb
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Chakraborty et al. - Unknown - Stop Clickbait Detecting and Preventing Clickbaits in Online News Media(2).pdf:pdf
      \endverb
    \endentry
    \entry{Potthast}{report}{}
      \name{author}{4}{}{%
        {{hash=c239bbb9437f221987397e74fbc81634}{%
           family={Potthast},
           familyi={P\bibinitperiod},
           given={Martin},
           giveni={M\bibinitperiod}}}%
        {{hash=3ee9c569072c8f977c582b71c3f3b743}{%
           family={Gollub},
           familyi={G\bibinitperiod},
           given={Tim},
           giveni={T\bibinitperiod}}}%
        {{hash=f951de76ce4fe85f370ee87d3b28235f}{%
           family={Hagen},
           familyi={H\bibinitperiod},
           given={Matthias},
           giveni={M\bibinitperiod}}}%
        {{hash=1ead8f029ff27a7eeedb027e8c7aaeb9}{%
           family={Stein},
           familyi={S\bibinitperiod},
           given={Benno},
           giveni={B\bibinitperiod}}}%
      }
      \strng{namehash}{52c27533ca68fed91d1d9e41ba2e1ec9}
      \strng{fullhash}{5576f353e5bff3f6073d7fb342634f89}
      \strng{bibnamehash}{5576f353e5bff3f6073d7fb342634f89}
      \strng{authorbibnamehash}{5576f353e5bff3f6073d7fb342634f89}
      \strng{authornamehash}{52c27533ca68fed91d1d9e41ba2e1ec9}
      \strng{authorfullhash}{5576f353e5bff3f6073d7fb342634f89}
      \field{sortinit}{8}
      \field{sortinithash}{1b24cab5087933ef0826a7cd3b99e994}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Clickbait has grown to become a nuisance to social media users and social media operators alike. Malicious content publishers misuse social media to manipulate as many users as possible to visit their websites using clickbait messages. Machine learning technology may help to handle this problem, giving rise to automatic clickbait detection. To accelerate progress in this direction, we organized the Clickbait Challenge 2017, a shared task inviting the submission of clickbait detectors for a comparative evaluation. A total of 13 detectors have been submitted, achieving significant improvements over the previous state of the art in terms of detection performance. Also, many of the submitted approaches have been published open source, rendering them reproducible, and a good starting point for newcomers. While the 2017 challenge has passed, we maintain the evaluation system and answer to new registrations in support of the ongoing research on better clickbait detectors.}
      \field{eprinttype}{arXiv}
      \field{title}{{The Clickbait Challenge 2017: Towards a Regression Model for Clickbait Strength}}
      \field{type}{techreport}
      \verb{eprint}
      \verb 1812.10847v1
      \endverb
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Potthast et al. - Unknown - The Clickbait Challenge 2017 Towards a Regression Model for Clickbait Strength.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://clickbait-challenge.org
      \endverb
      \verb{url}
      \verb https://clickbait-challenge.org
      \endverb
    \endentry
    \entry{Anand2019}{report}{}
      \name{author}{3}{}{%
        {{hash=e9c04884349329f54db6669b03ab4d4a}{%
           family={Anand},
           familyi={A\bibinitperiod},
           given={Ankesh},
           giveni={A\bibinitperiod}}}%
        {{hash=12ef4925945d218592da34e0e46c4eb4}{%
           family={Chakraborty},
           familyi={C\bibinitperiod},
           given={Tanmoy},
           giveni={T\bibinitperiod}}}%
        {{hash=fb037e7b379a2a88828b3b587b13a6fb}{%
           family={Park},
           familyi={P\bibinitperiod},
           given={Noseong},
           giveni={N\bibinitperiod}}}%
      }
      \strng{namehash}{58ddb4abd3a370936750034e899e674e}
      \strng{fullhash}{58ddb4abd3a370936750034e899e674e}
      \strng{bibnamehash}{58ddb4abd3a370936750034e899e674e}
      \strng{authorbibnamehash}{58ddb4abd3a370936750034e899e674e}
      \strng{authornamehash}{58ddb4abd3a370936750034e899e674e}
      \strng{authorfullhash}{58ddb4abd3a370936750034e899e674e}
      \field{sortinit}{8}
      \field{sortinithash}{1b24cab5087933ef0826a7cd3b99e994}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Online content publishers often use catchy headlines for their articles in order to attract users to their websites. These headlines, popularly known as clickbaits, exploit a user's curiosity gap and lure them to click on links that often disappoint them. Existing methods for automatically detecting clickbaits rely on heavy feature engineering and domain knowledge. Here, we introduce a neural network architecture based on Recurrent Neural Networks for detecting clickbaits. Our model relies on distributed word representations learned from a large unannotated corpora , and character embeddings learned via Convolutional Neural Networks. Experimental results on a dataset of news headlines show that our model outperforms existing techniques for clickbait detection with an accuracy of 0.98 with F1-score of 0.98 and ROC-AUC of 0.99.}
      \field{eprinttype}{arXiv}
      \field{title}{{We used Neural Networks to Detect Clickbaits: You won't believe what happened Next!}}
      \field{type}{techreport}
      \field{year}{2019}
      \verb{eprint}
      \verb 1612.01340v2
      \endverb
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Anand, Chakraborty, Park - 2019 - We used Neural Networks to Detect Clickbaits You won't believe what happened Next!.pdf:pdf
      \endverb
      \keyw{Clickbait Detection,Deep Learning,Neural Networks}
    \endentry
    \entry{Agrawal2017}{article}{}
      \name{author}{1}{}{%
        {{hash=79190e1726177769f6978ea191a2ab02}{%
           family={Agrawal},
           familyi={A\bibinitperiod},
           given={Amol},
           giveni={A\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{79190e1726177769f6978ea191a2ab02}
      \strng{fullhash}{79190e1726177769f6978ea191a2ab02}
      \strng{bibnamehash}{79190e1726177769f6978ea191a2ab02}
      \strng{authorbibnamehash}{79190e1726177769f6978ea191a2ab02}
      \strng{authornamehash}{79190e1726177769f6978ea191a2ab02}
      \strng{authorfullhash}{79190e1726177769f6978ea191a2ab02}
      \field{sortinit}{8}
      \field{sortinithash}{1b24cab5087933ef0826a7cd3b99e994}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Clickbaits, in social media, are exaggerated headlines whose main motive is to mislead the reader to 'click' on them. They create a nuisance in the online experience by creating a lure towards poor content. Online content creators are utilizing more of them to get increased page views and thereby more ad revenue without providing the backing content. This paper proposes a model for detection of clickbait by utilizing convolutional neural networks and presents a compiled clickbait corpus. We create a corpus using multiple social media platforms and utilize deep learning for learning features rather than undergoing the long and complex process of feature engineering. Our model achieves high performance in identification of clickbaits.}
      \field{isbn}{9781509032570}
      \field{journaltitle}{Proceedings on 2016 2nd International Conference on Next Generation Computing Technologies, NGCT 2016}
      \field{number}{October}
      \field{title}{{Clickbait detection using deep learning}}
      \field{year}{2017}
      \field{pages}{268\bibrangedash 272}
      \range{pages}{5}
      \verb{doi}
      \verb 10.1109/NGCT.2016.7877426
      \endverb
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Agrawal - 2017 - Clickbait detection using deep learning.pdf:pdf
      \endverb
      \keyw{Clickbait,Convolutional neural networks,Deep learning}
    \endentry
    \entry{Pujahari}{report}{}
      \name{author}{2}{}{%
        {{hash=c035160a519c9ca4614ee41f5521c26a}{%
           family={Pujahari},
           familyi={P\bibinitperiod},
           given={Abinash},
           giveni={A\bibinitperiod}}}%
        {{hash=88a38fbcf1ad9e30d957d2ff33466fb3}{%
           family={{Singh Sisodia}},
           familyi={S\bibinitperiod},
           given={Dilip},
           giveni={D\bibinitperiod}}}%
      }
      \strng{namehash}{3f6546d84578679b49027c9f41f8eda5}
      \strng{fullhash}{3f6546d84578679b49027c9f41f8eda5}
      \strng{bibnamehash}{3f6546d84578679b49027c9f41f8eda5}
      \strng{authorbibnamehash}{3f6546d84578679b49027c9f41f8eda5}
      \strng{authornamehash}{3f6546d84578679b49027c9f41f8eda5}
      \strng{authorfullhash}{3f6546d84578679b49027c9f41f8eda5}
      \field{sortinit}{8}
      \field{sortinithash}{1b24cab5087933ef0826a7cd3b99e994}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Clickbaits are online articles with deliberately designed misleading titles for luring more and more readers to open the intended web page. Clickbaits are used to tempted visitors to click on a particular link either to monetize the landing page or to spread the false news for sensationalization. The presence of clickbaits on any news aggregator portal may lead to unpleasant experience to readers. Automatic detection of clickbait headlines from news headlines has been a challenging issue for the machine learning community. A lot of methods have been proposed for preventing clickbait articles in recent past. However, the recent techniques available in detecting clickbaits are not much robust. This paper proposes a hybrid categorization technique for separating clickbait and non-clickbait articles by integrating different features, sentence structure, and clustering. During preliminary categorization, the headlines are separated using eleven features. After that, the headlines are recategorized using sentence formality, syntactic similarity measures. In the last phase, the headlines are again recategorized by applying clustering using word vector similarity based on t-Stochastic Neighbourhood Embedding (t-SNE) approach. After categorization of these headlines, machine learning models are applied to the data set to evaluate machine learning algorithms. The obtained experimental results indicate the proposed hybrid model is more robust, reliable and efficient than any individual categorization techniques for the real-world dataset we used.}
      \field{title}{{Clickbait Detection using Multiple Categorization Techniques}}
      \field{type}{techreport}
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Pujahari, Singh Sisodia - Unknown - Clickbait Detection using Multiple Categorization Techniques.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb www.clickbait-challenge.org
      \endverb
      \verb{url}
      \verb www.clickbait-challenge.org
      \endverb
      \keyw{Classification,Clickbait,Clustering,Sentence Structure,Word Vector}
    \endentry
    \entry{VanDerMaaten2008}{report}{}
      \name{author}{2}{}{%
        {{hash=a33ababcca5b83a7777452957fb2eef2}{%
           family={{Van Der Maaten}},
           familyi={V\bibinitperiod},
           given={Laurens},
           giveni={L\bibinitperiod}}}%
        {{hash=9a8750ccdb2a4cf14d2655face1ce016}{%
           family={Hinton},
           familyi={H\bibinitperiod},
           given={Geoffrey},
           giveni={G\bibinitperiod}}}%
      }
      \strng{namehash}{db0349d236640a0f89628c7e075d2cf7}
      \strng{fullhash}{db0349d236640a0f89628c7e075d2cf7}
      \strng{bibnamehash}{db0349d236640a0f89628c7e075d2cf7}
      \strng{authorbibnamehash}{db0349d236640a0f89628c7e075d2cf7}
      \strng{authornamehash}{db0349d236640a0f89628c7e075d2cf7}
      \strng{authorfullhash}{db0349d236640a0f89628c7e075d2cf7}
      \field{sortinit}{8}
      \field{sortinithash}{1b24cab5087933ef0826a7cd3b99e994}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{We present a new technique called "t-SNE" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualiza-tions produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.}
      \field{booktitle}{Journal of Machine Learning Research}
      \field{title}{{Visualizing Data using t-SNE}}
      \field{type}{techreport}
      \field{volume}{9}
      \field{year}{2008}
      \field{pages}{2579\bibrangedash 2605}
      \range{pages}{27}
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Van Der Maaten, Hinton - 2008 - Visualizing Data using t-SNE.pdf:pdf
      \endverb
      \keyw{dimensionality reduction,embedding algorithms,manifold learning,multidimensional scaling,visualization}
    \endentry
    \entry{chawda2019novel}{inproceedings}{}
      \name{author}{4}{}{%
        {{hash=fe07222e32a5a811c32955f89fac587b}{%
           family={Chawda},
           familyi={C\bibinitperiod},
           given={Sarjak},
           giveni={S\bibinitperiod}}}%
        {{hash=42ac31f9ebc43e5d93fe443e43338297}{%
           family={Patil},
           familyi={P\bibinitperiod},
           given={Aditi},
           giveni={A\bibinitperiod}}}%
        {{hash=ea1f2bcf60809ab80264614320310cb8}{%
           family={Singh},
           familyi={S\bibinitperiod},
           given={Abhishek},
           giveni={A\bibinitperiod}}}%
        {{hash=032ecee1189e315454bd5dba8ae0160a}{%
           family={Save},
           familyi={S\bibinitperiod},
           given={Ashwini},
           giveni={A\bibinitperiod}}}%
      }
      \list{organization}{1}{%
        {IEEE}%
      }
      \strng{namehash}{4a5f22eae5e09e34233d29237f1701e3}
      \strng{fullhash}{beae80dbb74d9cf7d185717281312cfd}
      \strng{bibnamehash}{beae80dbb74d9cf7d185717281312cfd}
      \strng{authorbibnamehash}{beae80dbb74d9cf7d185717281312cfd}
      \strng{authornamehash}{4a5f22eae5e09e34233d29237f1701e3}
      \strng{authorfullhash}{beae80dbb74d9cf7d185717281312cfd}
      \field{sortinit}{9}
      \field{sortinithash}{54047ffb55bdefa0694bbd554c1b11a0}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{booktitle}{2019 3rd International Conference on Trends in Electronics and Informatics (ICOEI)}
      \field{title}{{A Novel Approach for Clickbait Detection}}
      \field{year}{2019}
      \field{pages}{1318\bibrangedash 1321}
      \range{pages}{4}
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Chawda et al. - 2019 - A Novel Approach for Clickbait Detection.pdf:pdf
      \endverb
    \endentry
    \entry{Zannettou2018}{article}{}
      \name{author}{4}{}{%
        {{hash=db4a9460f26f889dd7daa7b06ea91b41}{%
           family={Zannettou},
           familyi={Z\bibinitperiod},
           given={Savvas},
           giveni={S\bibinitperiod}}}%
        {{hash=41cd510bcaefae46626ad4eb9434811b}{%
           family={Chatzis},
           familyi={C\bibinitperiod},
           given={Sotirios},
           giveni={S\bibinitperiod}}}%
        {{hash=71c3d31bd66362a5dc8e09b53b27421d}{%
           family={Papadamou},
           familyi={P\bibinitperiod},
           given={Kostantinos},
           giveni={K\bibinitperiod}}}%
        {{hash=f7de2fa60bed513a74926c6a4f5fb8fe}{%
           family={Sirivianos},
           familyi={S\bibinitperiod},
           given={Michael},
           giveni={M\bibinitperiod}}}%
      }
      \list{publisher}{1}{%
        {IEEE}%
      }
      \strng{namehash}{335a0d5bcde6e714e530affc5dfc4176}
      \strng{fullhash}{5f99ef5ce0af49b39491849781e10d3b}
      \strng{bibnamehash}{5f99ef5ce0af49b39491849781e10d3b}
      \strng{authorbibnamehash}{5f99ef5ce0af49b39491849781e10d3b}
      \strng{authornamehash}{335a0d5bcde6e714e530affc5dfc4176}
      \strng{authorfullhash}{5f99ef5ce0af49b39491849781e10d3b}
      \field{sortinit}{9}
      \field{sortinithash}{54047ffb55bdefa0694bbd554c1b11a0}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{The use of deceptive techniques in user-generated video portals is ubiquitous. Unscrupulous uploaders deliberately mislabel video descriptors aiming at increasing their views and subsequently their ad revenue. This problem, usually referred to as 'clickbait,' may severely undermine user experience. In this work, we study the clickbait problem on YouTube by collecting metadata for 206k videos. To address it, we devise a deep learning model based on variational autoencoders that supports the diverse modalities of data that videos include. The proposed model relies on a limited amount of manually labeled data to classify a large corpus of unlabeled data. Our evaluation indicates that the proposed model offers improved performance when compared to other conventional models. Our analysis of the collected data indicates that YouTube recommendation engine does not take into account clickbait. Thus, it is susceptible to recommending misleading videos to users.}
      \field{isbn}{9780769563497}
      \field{journaltitle}{Proceedings - 2018 IEEE Symposium on Security and Privacy Workshops, SPW 2018}
      \field{title}{{The good, the bad and the bait: Detecting and characterizing clickbait on youtube}}
      \field{year}{2018}
      \field{pages}{63\bibrangedash 69}
      \range{pages}{7}
      \verb{doi}
      \verb 10.1109/SPW.2018.00018
      \endverb
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Zannettou et al. - 2018 - The good, the bad and the bait Detecting and characterizing clickbait on youtube.pdf:pdf
      \endverb
      \keyw{Clickbait,Deep learning,YouTube}
    \endentry
    \entry{Kumar}{article}{}
      \name{author}{5}{}{%
        {{hash=5f153c3ab63f5af1fb30c4d4bcdc064f}{%
           family={Kumar},
           familyi={K\bibinitperiod},
           given={Vaibhav},
           giveni={V\bibinitperiod}}}%
        {{hash=3011966fb519bd6ec32378d7980f681d}{%
           family={Khattar},
           familyi={K\bibinitperiod},
           given={Dhruv},
           giveni={D\bibinitperiod}}}%
        {{hash=722ea42ea898b2e91b6b1b3d0125d32f}{%
           family={Gairola},
           familyi={G\bibinitperiod},
           given={Siddhartha},
           giveni={S\bibinitperiod}}}%
        {{hash=cff0407c9d230c6f889f7c424c113e3a}{%
           family={{Kumar Lal}},
           familyi={K\bibinitperiod},
           given={Yash},
           giveni={Y\bibinitperiod}}}%
        {{hash=2aa2acc782f22c7720d5b854573e0c6d}{%
           family={Varma},
           familyi={V\bibinitperiod},
           given={Vasudeva},
           giveni={V\bibinitperiod}}}%
      }
      \strng{namehash}{fd9a0f7d188404d583a71821fb161788}
      \strng{fullhash}{5c55f5535d8c04ed2305a717c85cf39f}
      \strng{bibnamehash}{5c55f5535d8c04ed2305a717c85cf39f}
      \strng{authorbibnamehash}{5c55f5535d8c04ed2305a717c85cf39f}
      \strng{authornamehash}{fd9a0f7d188404d583a71821fb161788}
      \strng{authorfullhash}{5c55f5535d8c04ed2305a717c85cf39f}
      \field{sortinit}{9}
      \field{sortinithash}{54047ffb55bdefa0694bbd554c1b11a0}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Online media outlets, in a bid to expand their reach and subsequently increase revenue through ad monetisation, have begun adopting clickbait techniques to lure readers to click on articles. The article fails to fulfill the promise made by the headline. Traditional methods for clickbait detection have relied heavily on feature engineering which, in turn, is dependent on the dataset it is built for. The application of neural networks for this task has only been explored partially. We propose a novel approach considering all information found in a social media post. We train a bidirectional LSTM with an attention mechanism to learn the extent to which a word contributes to the post's clickbait score in a differential manner. We also employ a Siamese net to capture the similarity between source and target information. Information gleaned from images has not been considered in previous approaches. We learn image embeddings from large amounts of data using Convolutional Neural Networks to add another layer of complexity to our model. Finally, we concatenate the outputs from the three separate components , serving it as input to a fully connected layer. We conduct experiments over a test corpus of 19538 social media posts, attaining an F1 score of 65.37{\%} on the dataset bettering the previous state-of-the-art, as well as other proposed approaches, feature engineering or otherwise.}
      \field{isbn}{9781450356572}
      \field{title}{{Identifying Clickbait: A Multi-Strategy Approach Using Neural Networks * Identifying Clickbait: A Multi-Strategy Approach Using}}
      \verb{doi}
      \verb 10.1145/3209978.3210144
      \endverb
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Kumar et al. - Unknown - Identifying Clickbait A Multi-Strategy Approach Using Neural Networks Identifying Clickbait A Multi-Strategy A.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb https://doi.org/10.1145/3209978.3210144
      \endverb
      \verb{url}
      \verb https://doi.org/10.1145/3209978.3210144
      \endverb
      \keyw{Attention-Mechanism,Clickbait,Image Embeddings,Neural Network,Siamese Network,Text Embeddings}
    \endentry
    \entry{Thomas}{report}{}
      \name{author}{1}{}{%
        {{hash=74e9a4e3a5f69b1ca55545ad18cdbabe}{%
           family={Thomas},
           familyi={T\bibinitperiod},
           given={Philippe},
           giveni={P\bibinitperiod}}}%
      }
      \strng{namehash}{74e9a4e3a5f69b1ca55545ad18cdbabe}
      \strng{fullhash}{74e9a4e3a5f69b1ca55545ad18cdbabe}
      \strng{bibnamehash}{74e9a4e3a5f69b1ca55545ad18cdbabe}
      \strng{authorbibnamehash}{74e9a4e3a5f69b1ca55545ad18cdbabe}
      \strng{authornamehash}{74e9a4e3a5f69b1ca55545ad18cdbabe}
      \strng{authorfullhash}{74e9a4e3a5f69b1ca55545ad18cdbabe}
      \field{sortinit}{9}
      \field{sortinithash}{54047ffb55bdefa0694bbd554c1b11a0}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper presents the results of our participation in the Clickbait Detection Challenge 2017. The system relies on a fusion of neural networks, incorporating different types of available informations. It does not require any linguistic preprocessing, and hence generalizes more easily to new domains and languages. The final combined model achieves a mean squared error of 0.0428, an accuracy of 0.826, and a F1 score of 0.564. According to the official evaluation metric the system ranked 6th of the 13 participating teams.}
      \field{eprinttype}{arXiv}
      \field{title}{{Clickbait Identification using Neural Networks The Whitebait Clickbait Detector at the Clickbait Challenge 2017}}
      \field{type}{techreport}
      \verb{eprint}
      \verb 1710.08721v1
      \endverb
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Thomas - Unknown - Clickbait Identification using Neural Networks The Whitebait Clickbait Detector at the Clickbait Challenge 2017.pdf:pdf
      \endverb
    \endentry
    \entry{Liao}{report}{}
      \name{author}{4}{}{%
        {{hash=c2abe015fee7402cc8afc288c24002c6}{%
           family={Liao},
           familyi={L\bibinitperiod},
           given={Feng},
           giveni={F\bibinitperiod}}}%
        {{hash=01af8b3e8feb9723557b0431c4dd4758}{%
           family={Zhuo},
           familyi={Z\bibinitperiod},
           given={Hankz\bibnamedelima Hankui},
           giveni={H\bibinitperiod\bibinitdelim H\bibinitperiod}}}%
        {{hash=5d032556ff2c4b762edabc4c71129e08}{%
           family={Huang},
           familyi={H\bibinitperiod},
           given={Xiaoling},
           giveni={X\bibinitperiod}}}%
        {{hash=9a4f4a1ff661cd600eb26523a5ba8bb4}{%
           family={Zhang},
           familyi={Z\bibinitperiod},
           given={Yu},
           giveni={Y\bibinitperiod}}}%
      }
      \strng{namehash}{4e072ed9f2dee2609fea6304d8d60144}
      \strng{fullhash}{d8a744eea96ac120ed66c4f17dbeae0c}
      \strng{bibnamehash}{d8a744eea96ac120ed66c4f17dbeae0c}
      \strng{authorbibnamehash}{d8a744eea96ac120ed66c4f17dbeae0c}
      \strng{authornamehash}{4e072ed9f2dee2609fea6304d8d60144}
      \strng{authorfullhash}{d8a744eea96ac120ed66c4f17dbeae0c}
      \field{sortinit}{9}
      \field{sortinithash}{54047ffb55bdefa0694bbd554c1b11a0}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Online media outlets adopt clickbait techniques to lure readers to click on articles in a bid to expand their reach and subsequently increase revenue through ad mone-tization. As the adverse effects of clickbait attract more and more attention, researchers have started to explore machine learning techniques to automatically detect click-baits. Previous work on clickbait detection assumes that all the training data is available locally during training. In many real-world applications, however, training data is generally distributedly stored by different parties (e.g., different parties maintain data with different feature spaces), and the parties cannot share their data with each other due to data privacy issues. It is challenging to build models of high-quality federally for detecting clickbaits effectively without data sharing. In this paper, we propose a fed-erated training framework, which is called federated hierarchical hybrid networks, to build clickbait detection models, where the titles and contents are stored by different parties, whose relationships must be exploited for clickbait detection. We empirically demonstrate that our approach is effective by comparing our approach to the state-of-the-art approaches using datasets from social media.}
      \field{eprinttype}{arXiv}
      \field{title}{{Federated Hierarchical Hybrid Networks for Clickbait Detection}}
      \field{type}{techreport}
      \verb{eprint}
      \verb 1906.00638v1
      \endverb
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Liao et al. - Unknown - Federated Hierarchical Hybrid Networks for Clickbait Detection.pdf:pdf
      \endverb
    \endentry
    \entry{Glenski}{report}{}
      \name{author}{4}{}{%
        {{hash=4f9ea86286cec2328fb1ea3b12f32a8d}{%
           family={Glenski},
           familyi={G\bibinitperiod},
           given={Maria},
           giveni={M\bibinitperiod}}}%
        {{hash=2d36510be7fedc7e30ef3a1d45f90049}{%
           family={Ayton},
           familyi={A\bibinitperiod},
           given={Ellyn},
           giveni={E\bibinitperiod}}}%
        {{hash=9161e019557560a91101f80a51e0d8e0}{%
           family={Arendt},
           familyi={A\bibinitperiod},
           given={Dustin},
           giveni={D\bibinitperiod}}}%
        {{hash=16bf4cd63e44f4b2847510f8ea40b0b1}{%
           family={Volkova},
           familyi={V\bibinitperiod},
           given={Svitlana},
           giveni={S\bibinitperiod}}}%
      }
      \strng{namehash}{98bc83c411a78d80467ec9751c1b5969}
      \strng{fullhash}{8479c529b15c525e50188ea5d1a34149}
      \strng{bibnamehash}{8479c529b15c525e50188ea5d1a34149}
      \strng{authorbibnamehash}{8479c529b15c525e50188ea5d1a34149}
      \strng{authornamehash}{98bc83c411a78d80467ec9751c1b5969}
      \strng{authorfullhash}{8479c529b15c525e50188ea5d1a34149}
      \field{sortinit}{9}
      \field{sortinithash}{54047ffb55bdefa0694bbd554c1b11a0}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{This paper presents the results and conclusions of our participation in the Clickbait Challenge 2017 1 on automatic clickbait detection in social media. We first describe linguistically-infused neural network models and identify informative representations to predict the level of clickbaiting present in Twitter posts. Our models allow to answer the question not only whether a post is a clickbait or not, but to what extent it is a clickbait post e.g., not at all, slightly, considerably, or heavily clickbaity using a score ranging from 0 to 1. We evaluate the predictive power of models trained on varied text and image representations extracted from tweets. Our best performing model that relies on the tweet text and linguistic markers of biased language extracted from the tweet and the corresponding page yields mean squared error (MSE) of 0.04, mean absolute error (MAE) of 0.16 and R2 of 0.43 on the held-out test data. For the binary classification setup (clickbait vs. non-clickbait), our model achieved F1 score of 0.69. We have not found that image representations combined with text yield significant performance improvement yet. Nevertheless, this work is the first to present preliminary analysis of objects extracted using Google Tensorflow object detection API from images in clickbait vs. non-clickbait Twitter posts. Finally, we outline several steps to improve model performance as a part of the future work.}
      \field{eprinttype}{arXiv}
      \field{title}{{Fishing for Clickbaits in Social Images and Texts with Linguistically-Infused Neural Network Models The Pineapplefish Clickbait Detector at the Clickbait Challenge 2017}}
      \field{type}{techreport}
      \verb{eprint}
      \verb 1710.06390v1
      \endverb
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Glenski et al. - Unknown - Fishing for Clickbaits in Social Images and Texts with Linguistically-Infused Neural Network Models The Pinea.pdf:pdf
      \endverb
      \verb{urlraw}
      \verb http://www.clickbait-challenge.org/
      \endverb
      \verb{url}
      \verb http://www.clickbait-challenge.org/
      \endverb
    \endentry
    \entry{Biyani2016}{article}{}
      \name{author}{3}{}{%
        {{hash=12f725fd2ad62d5d23ec06fc24c00a8a}{%
           family={Biyani},
           familyi={B\bibinitperiod},
           given={Prakhar},
           giveni={P\bibinitperiod}}}%
        {{hash=fd1b1ede623c04dd68ca701e123565eb}{%
           family={Tsioutsiouliklis},
           familyi={T\bibinitperiod},
           given={Kostas},
           giveni={K\bibinitperiod}}}%
        {{hash=ea217e5ce2ecdd8ea1749def8a67b4a3}{%
           family={Blackmer},
           familyi={B\bibinitperiod},
           given={John},
           giveni={J\bibinitperiod}}}%
      }
      \strng{namehash}{db4c3657618a41fc0777ec9d7e86512c}
      \strng{fullhash}{db4c3657618a41fc0777ec9d7e86512c}
      \strng{bibnamehash}{db4c3657618a41fc0777ec9d7e86512c}
      \strng{authorbibnamehash}{db4c3657618a41fc0777ec9d7e86512c}
      \strng{authornamehash}{db4c3657618a41fc0777ec9d7e86512c}
      \strng{authorfullhash}{db4c3657618a41fc0777ec9d7e86512c}
      \field{sortinit}{1}
      \field{sortinithash}{50c6687d7fc80f50136d75228e3c59ba}
      \field{labelnamesource}{author}
      \field{labeltitlesource}{title}
      \field{abstract}{Clickbaits are articles with misleading titles, exaggerating the content on the landing page. Their goal is to entice users to click on the title in order to monetize the landing page. The content on the landing page is usually of low quality. Their presence in user homepage stream of news aggregator sites (e.g., Yahoo news, Google news) may adversely impact user experience. Hence, it is important to identify and demote or block them on homepages. In this paper, we present a machine-learning model to detect clickbaits. We use a variety of features and show that the degree of informality of a web-page (as measured by different metrics) is a strong indicator of it being a clickbait. We conduct extensive experiments to evaluate our approach and analyze properties of clickbait and non-clickbait articles. Our model achieves high performance (74.9{\%} F-1 score) in predicting clickbaits.}
      \field{isbn}{9781577357605}
      \field{journaltitle}{Thirtieth AAAI Conference on Artificial Intelligence}
      \field{title}{{Detecting Clickbaits in News Streams Using Article Informality}}
      \field{year}{2016}
      \field{pages}{94\bibrangedash 100}
      \range{pages}{7}
      \verb{file}
      \verb :Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Biyani, Tsioutsiouliklis, Blackmer - 2016 - Detecting Clickbaits in News Streams Using Article Informality.pdf:pdf
      \endverb
      \keyw{Technical Papers: Artificial Intelligence and the}
    \endentry
  \enddatalist
\endrefsection
\endinput

