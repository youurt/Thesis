Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@book{Chollet2017,
author = {Chollet, Francois},
doi = {10.23919/ICIF.2018.8455530},
file = {:Users/ugurtigu/Downloads/Deep learning with Python by Chollet, Francois (z-lib.org).pdf:pdf},
isbn = {9780996452762},
title = {{Deep learning with Python}},
year = {2017}
}
@inproceedings{cho2019shop,
author = {Cho, Jaeyoung and Lee, Sangwon and Chung, Tai Myoung},
booktitle = {Proceedings of the Korean Society of Computer Information Conference},
organization = {Korean Society of Computer Information},
pages = {267--270},
title = {{A shop recommendation learning with Tensorflow. js}},
year = {2019}
}
@misc{Manning1999,
author = {Manning, Christopher and Sch{\"{u}}tze, Hinrich},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Manning, Sch{\"{u}}tze - 1999 - Manning {\&} Sch{\"{u}}tze 2003 - Foundations of Statistical NLP.pdf.pdf:pdf},
pages = {675},
title = {{Manning {\&} Sch{\"{u}}tze 2003 - Foundations of Statistical NLP.pdf}},
year = {1999}
}
@techreport{Lorent2018,
author = {Lorent, Simon and Itoo, Ashwin},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Lorent, Itoo - 2018 - Fake News Detection Using Machine Learning.pdf:pdf},
title = {{Fake News Detection Using Machine Learning}},
year = {2018}
}
@article{korde2012text,
author = {Korde, Vandana and Mahender, C Namrata},
journal = {International Journal of Artificial Intelligence {\&} Applications},
number = {2},
pages = {85},
publisher = {Academy {\&} Industry Research Collaboration Center (AIRCC)},
title = {{Text classification and classifiers: A survey}},
volume = {3},
year = {2012}
}
@inproceedings{Park2017,
abstract = {Phishers often exploit users' trust on the appearance of a site by using webpages that are visually similar to an authentic site. In the past, various research studies have tried to identify and classify the factors contributing towards the detection of phishing websites. The focus of this research is to establish a strong relationship between those identified heuristics (content-based) and the legitimacy of a website by analyzing training sets of websites (both phishing and legitimate websites) and in the process analyze new patterns and report findings. Many existing phishing detection tools are often not very accurate as they depend mostly on the old database of previously identified phishing websites. However, there are thousands of new phishing websites appearing every year targeting financial institutions, cloud storage/file hosting sites, government websites, and others. This paper presents a framework called Phishing-Detective that detects phishing websites based on existing and newly found heuristics. For this framework, a web crawler was developed to scrape the contents of phishing and legitimate websites. These contents were analyzed to rate the heuristics and their contribution scale factor towards the illegitimacy of a website. The data set collected from Web Scraper was then analyzed using a data mining tool to find patterns and report findings. A case study shows how this framework can be used to detect a phishing website. This research is still in progress but shows a new way of finding and using heuristics and the sum of their contributing weights to effectively and accurately detect phishing websites. Further development of this framework is discussed at the end of the paper.},
author = {Park, Andrew J. and Quadari, Ruhi Naaz and Tsang, Herbert H.},
booktitle = {2017 8th IEEE Annual Information Technology, Electronics and Mobile Communication Conference, IEMCON 2017},
doi = {10.1109/IEMCON.2017.8117212},
isbn = {9781538633717},
keywords = {Phishing detection,heuristic weights,web crawler},
month = {nov},
pages = {680--684},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Phishing website detection framework through web scraping and data mining}},
year = {2017}
}
@book{Kulkarni2019,
abstract = {Includes index. Implement natural language processing applications with Python using a problem-solution approach. This book has numerous coding exercises that will help you to quickly deploy natural language processing techniques, such as text classification, parts of speech identification, topic modeling, text summarization, and sentiment analysis. "Natural language processing recipes" starts by offering solutions for cleaning and preprocessing text data and ways to analyze it with advanced algorithms. You'll see practical applications of the semantic as well as syntactic analysis of text, as well as complex natural language processing approaches that involve text normalization, advanced preprocessing, POS tagging, parsing, text summarization, and sentiment analysis. You will also learn various applications of machine learning and deep learning in natural language processing. By using the recipes in this book, you will have a toolbox of solutions to apply to your own projects in the real world, making your development time quicker and more efficient. You will: Apply NLP techniques using Python libraries such as NLTK, TextBlob, soaCy, Stanford CoreNLP, and many more ; Implement the concepts of information retrieval, text summarization, sentiment analysis, and other advanced natural language processing techniques ; Identify machine learning and deep learning techniques for natural language processing and natural language generation problems. Introduction -- Extracting the data -- Exploring and processing text data -- Converting text to features -- Advanced natural language processing -- Implementing industry applications -- Deep learning for NLP.},
archivePrefix = {arXiv},
arxivId = {arXiv:1510.00726v1},
author = {Kulkarni, Akshay and Shivananda, Adarsha},
eprint = {arXiv:1510.00726v1},
file = {:Users/ugurtigu/MasterThesis/BOOKS/Natural Language Processing Recipes Unlocking Text Data with Machine Learning and Deep Learning using Python by Akshay Kulkarni, Adarsha Shivananda (z-lib.org).pdf:pdf},
isbn = {2006062298},
issn = {00010782},
pages = {234},
pmid = {21603045},
title = {{Natural Language Processing Recipes: Unlocking Text Data with Machine Learning and Deep Learning using Python}},
year = {2019}
}
@techreport{Barcaroli2014,
abstract = {The Istat sampling survey on "ICT in enterprises" aims at producing information on the use of ICT and in particular on the use of Internet by Italian enterprises for various purposes (e-commerce, e-recruitment, advertisement, e-tendering, e-procurement, e-government). To such a scope, data are collected by means of the traditional instrument of the questionnaire. Istat began to explore the possibility to use web scraping techniques, associated, in the estimation phase, to text and data mining algorithms, with the aim to substitute traditional instruments of data collection and estimation, or to combine them in an integrated approach. The 8,600 websites, indicated by the 19,000 enterprises responding to ICT survey of year 2013, have been "scraped" and the acquired texts have been processed in order to try to reproduce the same information collected via questionnaire. Preliminary results are encouraging, showing in some cases a satisfactory predictive capability of fitted models (mainly those obtained by using the Na{\"{i}}ve Bayes algorithm). Also the method known as Content Analysis has been applied, and its results compared to those obtained with classical learners. In order to improve the overall performance, an advanced system for scraping and mining is being adopted, based on the open source Apache suite Nutch-Solr-Lucene. On the basis of the final results of this test, an integrated system harnessing both survey data and data from the Internet to produce the required estimates will be implemented, based on systematic scraping of the near 100,000 websites related to the whole population of Italian enterprises with 10 persons employed and more, operating in industry and services. This new approach, based on "Internet as Data source (IaD)", is characterized by advantages and drawbacks that need to be carefully analysed.},
author = {Barcaroli, Giulio and Nurra, Alessandra and Scarn{\`{o}}, Marco and Summa, Donato},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Barcaroli et al. - 2014 - Use of web scraping and text mining techniques in the Istat survey on Information and Communication Technology.pdf:pdf},
keywords = {Big Data,Internet as Data source,data mining,text mining,web mining,web scraping},
title = {{Use of web scraping and text mining techniques in the Istat survey on "Information and Communication Technology in enterprises" Using Big Data for Offical Statistics View project ICT and digitalization View project Use of web scraping and text mining tech}},
url = {https://www.researchgate.net/publication/263921951},
year = {2014}
}
@article{gairola2017neural,
author = {Gairola, Siddhartha and Lal, Yash Kumar and Kumar, Vaibhav and Khattar, Dhruv},
journal = {arXiv preprint arXiv:1710.01507},
title = {{A neural clickbait detection engine}},
year = {2017}
}
@inproceedings{severyn2015unitn,
author = {Severyn, Aliaksei and Moschitti, Alessandro},
booktitle = {Proceedings of the 9th international workshop on semantic evaluation (SemEval 2015)},
pages = {464--469},
title = {{Unitn: Training deep convolutional neural network for twitter sentiment classification}},
year = {2015}
}
@article{Nielsen2014,
abstract = {BACKGROUND: Causes of mandibular condylar (condylar) head necrosis as a consequence of intracapsular mandibular fractures are still a subject of controversy.$\backslash$n$\backslash$nOBJECTIVES: To investigate why in some cases of intracapsular fractures condylar head necrosis occurs.$\backslash$n$\backslash$nMATERIAL: 58 human heads from the collection of Head and Neck Clinical Anatomy Laboratory, from the Institute of Physiology and Pathology of Hearing, Warsaw, Poland, constituted the material.$\backslash$n$\backslash$nSTUDY: Head arterial tree injections, anatomical preparation with the use of standard set of microsurgical equipment and an operating microscope.$\backslash$n$\backslash$nRESULTS: The main source of condylar head vascularization is the inferior alveolar artery, supplying bone marrow of the whole mandible as well as its cortical layer. Additional arterial blood supplying comes from a various number (2-7) of branches supplying the temporomandibular joint capsule. They originate directly from the maxillary artery or from its primary branches: masseteric artery, external pterygoid artery or superficial temporal artery. Two rare variants of accessory mandibular head vascularization were encountered. The first (2 cases) was an arterial branch from the maxillary artery and the second (1 case) was a branch from the external pterygoid artery. In these cases the arterial supply of lateral part of temporomandibular joint capsule from other sources was reduced.$\backslash$n$\backslash$nCONCLUSION: Fractures resulting in the lateral part of the condylar head in isolation could be potentially threatened by necrosis because of poor vascularization.},
author = {Nielsen, Michael},
journal = {Neural Networks and Deep Learning},
title = {{Improving the way neural networks learn The cross-entropy cost function - Chapter 3}},
year = {2014}
}
@article{Zuhroh2019,
abstract = {Online news portals are currently one of the fastest sources of information used by people. Its impact is due to the credibility of the news produced by actors from the media industry, which is sometimes questioned. However, one of the problems associated with this medium used to obtain information is clickbait. This technique aims to attract users to click hyperbolic headlines with content that often disappoints the reader. This study was, therefore, conducted to determine: 1) existing dataset available. 2) The method used in clickbait detection which consists of data preprocessing, analysis of features, and classification. 3) Difference steps from the method used.},
author = {Zuhroh, Nurrida Aini and Rakhmawati, Aini},
doi = {10.26594/register.v6i1.1561},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Zuhroh, Rakhmawati - 2019 - Clickbait detection A literature review of the methods used.pdf:pdf},
journal = {Scientific Journal of Information System Technology},
number = {1},
pages = {2020--2021},
title = {{Clickbait detection: A literature review of the methods used}},
url = {http://doi.org/10.26594/register.v6i1.1561},
volume = {6},
year = {2019}
}
@techreport{Hinton,
abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associa-tive memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
author = {Hinton, Geoffrey E and Osindero, Simon},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Hinton, Osindero - Unknown - A Fast Learning Algorithm for Deep Belief Nets Yee-Whye Teh.pdf:pdf},
title = {{A Fast Learning Algorithm for Deep Belief Nets Yee-Whye Teh}}
}
@incollection{aggarwal2012survey,
author = {Aggarwal, Charu C and Zhai, ChengXiang},
booktitle = {Mining text data},
pages = {163--222},
publisher = {Springer},
title = {{A survey of text classification algorithms}},
year = {2012}
}
@techreport{Chakraborty,
abstract = {Most of the online news media outlets rely heavily on the revenues generated from the clicks made by their readers, and due to the presence of numerous such outlets, they need to compete with each other for reader attention. To attract the readers to click on an article and subsequently visit the media site, the outlets often come up with catchy headlines accompanying the article links, which lure the readers to click on the link. Such headlines are known as Clickbaits. While these baits may trick the readers into clicking, in the long-run, clickbaits usually don't live up to the expectation of the readers, and leave them disappointed. In this work, we attempt to automatically detect clickbaits and then build a browser extension which warns the readers of different media sites about the possibility of being baited by such headlines. The extension also offers each reader an option to block clickbaits she doesn't want to see. Then, using such reader choices, the extension automatically blocks similar clickbaits during her future visits. We run extensive offline and online experiments across multiple media sites and find that the proposed clickbait detection and the personalized blocking approaches perform very well achieving 93{\%} accuracy in detecting and 89{\%} accuracy in blocking clickbaits.},
archivePrefix = {arXiv},
arxivId = {1610.09786v1},
author = {Chakraborty, Abhijnan and Paranjape, Bhargavi and Kakarla, Sourya and Ganguly, Niloy},
eprint = {1610.09786v1},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Chakraborty et al. - Unknown - Stop Clickbait Detecting and Preventing Clickbaits in Online News Media.pdf:pdf},
title = {{Stop Clickbait: Detecting and Preventing Clickbaits in Online News Media}}
}
@misc{Taylor2017a,
author = {Taylor, Michael},
title = {{The Math of Neural Networks}},
url = {https://dl.acm.org/doi/book/10.5555/3181144 http://himarsh.org/the-math-neural-networks/},
urldate = {2020-12-08},
year = {2017}
}
@book{IanGoodfellowYoshuaBengio2016,
author = {{Ian Goodfellow, Yoshua Bengio}, Aaron Courville},
booktitle = {Prmu},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Ian Goodfellow, Yoshua Bengio - 2016 - Deep Learning.pdf:pdf},
pages = {1--10},
title = {{Deep Learning}},
year = {2016}
}
@misc{Herculano-Houzel2009,
abstract = {The human brain has often been viewed as outstanding among mammalian brains: the most cognitively able, the largest-than-expected from body size, endowed with an overdeveloped cerebral cortex that represents over 80{\%} of brain mass, and purportedly containing 100 billion neurons and 10× more glial cells. Such uniqueness was seemingly necessary to justify the superior cognitive abilities of humans over larger-brained mammals such as elephants and whales. However, our recent studies using a novel method to determine the cellular composition of the brain of humans and other primates as well as of rodents and insectivores show that, since different cellular scaling rules apply to the brains within these orders, brain size can no longer be considered a proxy for the number of neurons in the brain. These studies also showed that the human brain is not exceptional in its cellular composition, as it was found to contain as many neuronal and non-neuronal cells as would be expected of a primate brain of its size. Additionally, the so-called overdeveloped human cerebral cortex holds only 19{\%} of all brain neurons, a fraction that is similar to that found in other mammals. In what regards absolute numbers of neurons, however, the human brain does have two advantages compared to other mammalian brains: compared to rodents, and probably to whales and elephants as well, it is built according to the very economical, space-saving scaling rules that apply to other primates; and, among economically built primate brains, it is the largest, hence containing the most neurons. These findings argue in favor of a view of cognitive abilities that is centered on absolute numbers of neurons, rather than on body size or encephalization, and call for a re-examination of several concepts related to the exceptionality of the human brain. {\textcopyright} 2009 Herculano-Houzel.},
author = {Herculano-Houzel, Suzana},
booktitle = {Frontiers in Human Neuroscience},
doi = {10.3389/neuro.09.031.2009},
issn = {16625161},
number = {NOV},
title = {{The human brain in numbers: A linearly scaled-up primate brain}},
volume = {3},
year = {2009}
}
@article{Kaur2020,
abstract = {Clickbait indicates the type of content with an intending goal to attract the attention of readers. It has grown to become a nuisance to social media users. The purpose of clickbait is to bring an appealing link in front of users. Clickbaits seen in the form of headlines influence people to get attracted and curious to read the inside content. The content seen in the form of text on clickbait posts is very short to identify its features as clickbait. In this paper, a novel approach (two-phase hybrid CNN-LSTM Biterm model) has been proposed for modeling short topic content. The hybrid CNN-LSTM model when implemented with pre-trained GloVe embedding yields the best results based on accuracy, recall, precision, and F1-score performance metrics. The proposed model achieves 91.24{\%}, 95.64{\%}, 95.87{\%} precision values for Dataset 1, Dataset 2 and Dataset 3, respectively. Eight types of clickbait such as Reasoning, Number, Reaction, Revealing, Shocking/Unbelievable, Hypothesis/Guess, Questionable, Forward referencing are classified in this work using the Biterm Topic Model (BTM). It has been shown that the clickbaits such as Shocking/Unbelievable, Hypothesis/Guess and Reaction are the highest in numbers among rest of the clickbait headlines published online. Also, a ground dataset of non-textual (image-based) data using multiple social media platforms has been created in this paper. The textual information has been retrieved from the images with the help of OCR tool. A comparative study is performed to show the effectiveness of our proposed model which helps to identify the various categories of clickbait headlines that are spread on social media platforms.},
author = {Kaur, Sawinder and Kumar, Parteek and Kumaraguru, Ponnurangam},
doi = {10.1016/j.eswa.2020.113350},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Classifier,Clickbait,Features,News,Social media},
month = {aug},
pages = {113350},
publisher = {Elsevier Ltd},
title = {{Detecting clickbaits using two-phase hybrid CNN-LSTM biterm model}},
volume = {151},
year = {2020}
}
@article{zhao2019speech,
author = {Zhao, Jianfeng and Mao, Xia and Chen, Lijiang},
journal = {Biomedical Signal Processing and Control},
pages = {312--323},
publisher = {Elsevier},
title = {{Speech emotion recognition using deep 1D {\&} 2D CNN LSTM networks}},
volume = {47},
year = {2019}
}
@techreport{Rohringer2019,
author = {Rohringer, Sophie},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Rohringer - 2019 - Mit diesem Ergebnis hat niemand gerechnet-Chancen und Grenzen von Clickbaiting im Online-Journalismus.pdf:pdf},
title = {{Mit diesem Ergebnis hat niemand gerechnet-Chancen und Grenzen von Clickbaiting im Online-Journalismus}},
year = {2019}
}
@article{Main2017,
abstract = {The use of tempting headlines (clickbait) to allure readers has become a growing practice nowadays. The widespread use of clickbait risks the reader's trust in media. According to a study performed by Facebook, 80{\%} users "preferred headlines that helped them decide if they wanted to read the full article before they had to click through". In this paper, we present a clickbait detection model which uses distributed subword embeddings and achieves an accuracy of 98.3{\%}. Powered with the model, we build BaitBuster, a solution framework (social bot+browser extension), which not only detects clickbaits floating on the web but also provides brief explanation behind its action. Moreover, we study 1.67 million contents created by 153 media organizations and discover the relation between clickbait usage and media reliability.},
author = {Main, Md and Rony, Uddin and Hassan, Naeemul and Yousuf, Mohammad},
doi = {10.1145/nnnnnnn.nnnnnnn},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Main et al. - 2017 - BaitBuster Destined to Save You Some Clicks.pdf:pdf},
title = {{BaitBuster: Destined to Save You Some Clicks}},
url = {https://doi.org/10.1145/nnnnnnn.nnnnnnn},
year = {2017}
}
@book{Moolayil2019a,
abstract = {Place of publication from publisher website. What's Next for DL Expertise? Learn, understand, and implement deep neural networks in a math- and programming-friendly approach using Keras and Python. The book focuses on an end-to-end approach to developing supervised learning algorithms in regression and classification with practical business-centric use-cases implemented in Keras. The overall book comprises three sections with two chapters in each section. The first section prepares you with all the necessary basics to get started in deep learning. Chapter 1 introduces you to the world of deep learning and its difference from machine learning, the choices of frameworks for deep learning, and the Keras ecosystem. You will cover a real-life business problem that can be solved by supervised learning algorithms with deep neural networks. You'll tackle one use case for regression and another for classification leveraging popular Kaggle datasets. Later, you will see an interesting and challenging part of deep learning: hyperparameter tuning; helping you further improve your models when building robust deep learning applications. Finally, you'll further hone your skills in deep learning and cover areas of active development and research in deep learning. At the end of Learn Keras for Deep Neural Networks, you will have a thorough understanding of deep learning principles and have practical hands-on experience in developing enterprise-grade deep learning solutions in Keras. You will: Master fast-paced practical deep learning concepts with math- and programming-friendly abstractions. Design, develop, train, validate, and deploy deep neural networks using the Keras framework Use best practices for debugging and validating deep learning models Deploy and integrate deep learning as a service into a larger software service or product Extend deep learning principles into other popular frameworks. Intro; Table of Contents; About the Author; About the Technical Reviewer; Acknowledgments; Introduction; Chapter 1: An Introduction to Deep Learning and Keras; Introduction to DL; Demystifying the Buzzwords; What Are Some Classic Problems Solved by DL in Today's Market?; Decomposing a DL Model; Exploring the Popular DL Frameworks; Low-Level DL Frameworks; Theano; Torch; PyTorch; MxNet; TensorFlow; High-Level DL Frameworks; A Sneak Peek into the Keras Framework; Getting the Data Ready; Defining the Model Structure; Training the Model and Making Predictions; Summary; Chapter 2: Keras in Action Setting Up the EnvironmentSelecting the Python Version; Installing Python for Windows, Linux, or macOS; Installing Keras and TensorFlow Back End; Getting Started with DL in Keras; Input Data; Neuron; Activation Function; Sigmoid Activation Function; ReLU Activation Function; Model; Layers; Core Layers; Dense Layer; Dropout Layer; Other Important Layers; The Loss Function; Optimizers; Stochastic Gradient Descent (SGD); Adam; Other Important Optimizers; Metrics; Model Configuration; Model Training; Model Evaluation; Putting All the Building Blocks Together; Summary Chapter 3: Deep Neural Networks for Supervised Learning: RegressionGetting Started; Problem Statement; Why Is Representing a Problem Statement with a Design Principle Important?; Designing an SCQ; Designing the Solution; Exploring the Data; Looking at the Data Dictionary; Finding Data Types; Working with Time; Predicting Sales; Exploring Numeric Columns; Understanding the Categorical Features; Data Engineering; Defining Model Baseline Performance; Designing the DNN; Testing the Model Performance; Improving the Model; Increasing the Number of Neurons; Plotting the Loss Metric Across Epochs Testing the Model ManuallySummary; Chapter 4: Deep Neural Networks for Supervised Learning: Classification; Getting Started; Problem Statement; Designing the SCQ; Designing the Solution; How Can We Identify a Potential Customer?; Exploring the Data; Data Engineering; Defining Model Baseline Accuracy; Designing the DNN for Classification; Revisiting the Data; Standardize, Normalize, or Scale the Data; Transforming the Input Data; DNNs for Classification with Improved Data; Summary; Chapter 5: Tuning and Deploying Deep Neural Networks; The Problem of Overfitting; So, What Is Regularization? L1 RegularizationL2 Regularization; Dropout Regularization; Hyperparameter Tuning; Hyperparameters in DL; Number of Neurons in a Layer; Number of Layers; Number of Epochs; Weight Initialization; Batch Size; Learning Rate; Activation Function; Optimization; Approaches for Hyperparameter Tuning; Manual Search; Grid Search; Random Search; Further Reading; Model Deployment; Tailoring the Test Data; Saving Models to Memory; Retraining the Models with New Data; Online Models; Delivering Your Model As an API; Putting All the Pieces of the Puzzle Together; Summary; Chapter 6: The Path Ahead},
author = {Moolayil, Jojo},
booktitle = {Learn Keras for Deep Neural Networks},
file = {:Users/ugurtigu/MasterThesis/BOOKS/Learn Keras for Deep Neural Networks A Fast-Track Approach to Modern Deep Learning with Python by Jojo John Moolayil (z-lib.org).pdf:pdf},
isbn = {978-1-4842-4239-1},
pages = {182},
title = {{Learn Keras for Deep Neural Networks: A Fast-Track Approach to Modern Deep Learning with Python}},
year = {2019}
}
@techreport{Ter-Akopyan1101,
author = {Ter-Akopyan, Bagrat},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Ter-Akopyan - 1101 - Korpuskonstruktion und Entwicklung einer Pipeline f{\"{u}}r Clickbait-Spoiling Bachelorarbeit.pdf:pdf},
title = {{Korpuskonstruktion und Entwicklung einer Pipeline f{\"{u}}r Clickbait-Spoiling Bachelorarbeit}},
year = {1101}
}
@article{Kaur2020a,
abstract = {Clickbait indicates the type of content with an intending goal to attract the attention of readers. It has grown to become a nuisance to social media users. The purpose of clickbait is to bring an appealing link in front of users. Clickbaits seen in the form of headlines influence people to get attracted and curious to read the inside content. The content seen in the form of text on clickbait posts is very short to identify its features as clickbait. In this paper, a novel approach (two-phase hybrid CNN-LSTM Biterm model) has been proposed for modeling short topic content. The hybrid CNN-LSTM model when implemented with pre-trained GloVe embedding yields the best results based on accuracy, recall, precision, and F1-score performance metrics. The proposed model achieves 91.24{\%}, 95.64{\%}, 95.87{\%} precision values for Dataset 1, Dataset 2 and Dataset 3, respectively. Eight types of clickbait such as Reasoning, Number, Reaction, Revealing, Shocking/Unbelievable, Hypothesis/Guess, Questionable, Forward referencing are classified in this work using the Biterm Topic Model (BTM). It has been shown that the clickbaits such as Shocking/Unbelievable, Hypothesis/Guess and Reaction are the highest in numbers among rest of the clickbait headlines published online. Also, a ground dataset of non-textual (image-based) data using multiple social media platforms has been created in this paper. The textual information has been retrieved from the images with the help of OCR tool. A comparative study is performed to show the effectiveness of our proposed model which helps to identify the various categories of clickbait headlines that are spread on social media platforms.},
author = {Kaur, Sawinder and Kumar, Parteek and Kumaraguru, Ponnurangam},
doi = {10.1016/j.eswa.2020.113350},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Classifier,Clickbait,Features,News,Social media},
month = {aug},
pages = {113350},
publisher = {Elsevier Ltd},
title = {{Detecting clickbaits using two-phase hybrid CNN-LSTM biterm model}},
volume = {151},
year = {2020}
}
@book{Taylor2017,
author = {Taylor, Michael},
file = {:Users/ugurtigu/Downloads/The Math of Neural Networks by Michael Taylor, Mark Koning (z-lib.org).epub.pdf:pdf},
title = {{The Math of Neural Networks}},
year = {2017}
}
@techreport{Kobler,
author = {Kobler, Florian and {Angestrebter Akademischer Grad}, B A},
title = {{Klick mich!}}
}
@techreport{Mertens,
author = {Mertens, Lukas},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Mertens - Unknown - Zusammenfassung K{\"{u}}nstliche neuronale Netze am Beispiel der Klassifizierung von Scandaten.pdf:pdf},
title = {{Zusammenfassung K{\"{u}}nstliche neuronale Netze am Beispiel der Klassifizierung von Scandaten}}
}
@book{Bruce2013,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bruce, 2011},
booktitle = {Journal of Chemical Information and Modeling},
eprint = {arXiv:1011.1669v3},
file = {:Users/ugurtigu/MasterThesis/BOOKS/Introduction to Deep Learning by Eugene Charniak (z-lib.org).pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{Intro to Deep Learning}},
volume = {53},
year = {2013}
}
@inproceedings{Vorakitphan2019,
abstract = {In recent years, social networking platform serves as a new media of news sharing and information diffusion. Social networking platform has become a part of our daily life. As such, social media advertising budgets have explosively expanded worldwide over the past few years. Due to the huge commercial interest, clickbait behaviors are commonly observed, which use attractive headlines and sensationalized textual description to bait users to visit websites. Clickbaits mainly exploit the users' curiosity's gap by interesting headlines to entice its readers to click an accompanying link to articles often with poor contents. Clickbaits are bothersome either to social media users or platform site owners. In this paper, we propose an approach called Ontology-based LSTM Model (OLSTM) to detect clickbaits. Compared with the existing solutions for clickbait detection, our approach is characterized by the following three components: word embedding model, Recurrent Neural Networks (RNN), and word ontology information. The observation is that preserving semantic relationships is significantly an important factor to be considered in detecting clickbaits. Therefore, we propose to capture semantic relationships between words by word embedding models. In addition, we adopted RNN as our classification models to consider word orders in a sentence. Furthermore, we consider the word ontology relation as another feature set for clickbait classification, as clickbaits often uses words with generalized concepts to induce curiosity. We conduct experiments with real data from Twitter and news websites to validate the effectiveness of the proposed approach, which demonstrates that the employment of the proposed method improves clickbait detection accuracy from 80{\%} to 90{\%} compared with the existing solutions.},
author = {Vorakitphan, Vorakit and Leu, Fang Yie and Fan, Yao Chung},
booktitle = {Advances in Intelligent Systems and Computing},
doi = {10.1007/978-3-319-93554-6_54},
isbn = {9783319935539},
issn = {21945357},
month = {jul},
pages = {557--564},
publisher = {Springer Verlag},
title = {{Clickbait detection based on word embedding models}},
url = {https://link.springer.com/chapter/10.1007/978-3-319-93554-6{\_}54},
volume = {773},
year = {2019}
}
@techreport{Rosenblatt,
author = {Rosenblatt, F},
booktitle = {Psychological Review},
number = {6},
pages = {19--27},
title = {{THE PERCEPTRON: A PROBABILISTIC MODEL FOR INFORMATION STORAGE AND ORGANIZATION IN THE BRAIN 1}},
volume = {65}
}
@techreport{Chavan2019,
abstract = {Stock also known as equity or shares is a type of security that signifies ownership in a corporation and represents a claim on part of the corporations assets and earnings whereas a stock market or share market is the aggregation of buyers and sellers of stocks. Stock Exchange is defined as a process where the stock brokers can buy as well as sell the shares, bonds or other securities. Many companies regardless of their domains or sectors make their stocks or shares available through Stock Market. Such type of an exchange deals with concentration and a focused mind as the entire process is dealing with valuable assets of a person. Thus to ease the process of stock exchange many stock brokers use the act of predicting the stock prices i.e. trying to determine the whether the financial instruments of a company will go up or down in their values. Prediction is done by fundamental analysis and technical analysis and now using Machine Learning Concepts. In this project we propose a Convolutional Neural Network for predicting the stock price in order to make profit.},
author = {Chavan, Sandeep and Doshi, Harshil and Godbole, Diksha and Parge, Pranav and Gore, Deipali},
booktitle = {International Journal of Innovative Science and Research Technology},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Chavan et al. - 2019 - D Convolutional Neural Network for Stock Market Prediction using Tensorflow.js.pdf:pdf},
keywords = {Convolutional Neural Network,Deep Learning,JavaScript,Ma-chine Learning,Neural Network,Prediction/Forecasting,Stock,Stock Exchange,Stock Market,Tensorflowjs,Time Series Prediction},
number = {6},
title = {{D Convolutional Neural Network for Stock Market Prediction using Tensorflow.js}},
url = {www.ijisrt.com},
volume = {4},
year = {2019}
}
@techreport{Kinne,
author = {Kinne, Jan and Axenbeck, Janna},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Kinne, Axenbeck - Unknown - Web Mining of Firm Websites A Framework for Web Scraping and a Pilot Study for Germany.pdf:pdf},
keywords = {indicators,innovation,r{\&}d,r{\&}i,sti,text mining,web mining,web scraping},
title = {{Web Mining of Firm Websites: A Framework for Web Scraping and a Pilot Study for Germany}},
url = {http://ftp.zew.de/pub/zew-docs/dp/dp18033.pdf}
}
@inproceedings{severyn2015twitter,
author = {Severyn, Aliaksei and Moschitti, Alessandro},
booktitle = {Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {959--962},
title = {{Twitter sentiment analysis with deep convolutional neural networks}},
year = {2015}
}
@inproceedings{Ertam2019,
abstract = {It is a huge repository of information used to meet the many needs of Internet users. The collection and processing of data in some internet sites in this information repository has become very important nowadays. In this study, categorical news headlines and summaries in a Turkish news agency site were collected by using web scraping methods and test data were classified by using 'one hot encoding' method with vector learning methods and depth learning methods. A classification accuracy of 90{\%} has been achieved.},
author = {Ertam, Fatih},
booktitle = {2018 International Conference on Artificial Intelligence and Data Processing, IDAP 2018},
doi = {10.1109/IDAP.2018.8620790},
isbn = {9781538668788},
keywords = {Web scraping,deep learning,information extraction,machine learning,text classification},
month = {jan},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Deep learning based text classification with Web Scraping methods}},
year = {2019}
}
@article{kiranyaz20191d,
author = {Kiranyaz, Serkan and Avci, Onur and Abdeljaber, Osama and Ince, Turker and Gabbouj, Moncef and Inman, Daniel J},
journal = {arXiv preprint arXiv:1905.03554},
title = {{1D convolutional neural networks and applications: A survey}},
year = {2019}
}
@inproceedings{chawda2019novel,
author = {Chawda, Sarjak and Patil, Aditi and Singh, Abhishek and Save, Ashwini},
booktitle = {2019 3rd International Conference on Trends in Electronics and Informatics (ICOEI)},
organization = {IEEE},
pages = {1318--1321},
title = {{A Novel Approach for Clickbait Detection}},
year = {2019}
}
@article{seopredicting,
author = {Seo, Jae Duk},
title = {{Predicting Keyword Popularity using Tensorflow JS}}
}
@inproceedings{Anand2017a,
abstract = {Online content publishers often use catchy headlines for their articles in order to attract users to their websites. These headlines, popularly known as clickbaits, exploit a user's curiosity gap and lure them to click on links that often disappoint them. Existing methods for automatically detecting clickbaits rely on heavy feature engineering and domain knowledge. Here, we introduce a neural network architecture based on Recurrent Neural Networks for detecting clickbaits. Our model relies on distributed word representations learned from a large unannotated corpora, and character embeddings learned via Convolutional Neural Networks. Experimental results on a dataset of news headlines show that our model outperforms existing techniques for clickbait detection with an accuracy of 0.98 with F1-score of 0.98 and ROC-AUC of 0.99.},
archivePrefix = {arXiv},
arxivId = {1612.01340},
author = {Anand, Ankesh and Chakraborty, Tanmoy and Park, Noseong},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-56608-5_46},
eprint = {1612.01340},
isbn = {9783319566078},
issn = {16113349},
keywords = {Clickbait detection,Deep learning,Neural networks},
pages = {541--547},
publisher = {Springer Verlag},
title = {{We used neural networks to detect clickbaits: You won't believe what happened next!}},
url = {https://link.springer.com/chapter/10.1007/978-3-319-56608-5{\_}46},
volume = {10193 LNCS},
year = {2017}
}
@inproceedings{Zeiler2014,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets. {\textcopyright} 2014 Springer International Publishing.},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Zeiler, Matthew D. and Fergus, Rob},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {1311.2901},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Zeiler, Fergus - 2014 - Visualizing and understanding convolutional networks.pdf:pdf},
isbn = {9783319105895},
issn = {16113349},
number = {PART 1},
pages = {818--833},
publisher = {Springer Verlag},
title = {{Visualizing and understanding convolutional networks}},
url = {https://link.springer.com/chapter/10.1007/978-3-319-10590-1{\_}53},
volume = {8689 LNCS},
year = {2014}
}
@article{Bunde2019,
author = {Bunde, Enrico},
file = {:Users/ugurtigu/Downloads/2019-03-29 ma{\_}enrico{\_}bunde.pdf:pdf},
pages = {130},
title = {{Sentimentanalyse deutscher Twitter-Korpora mittels Deep Learning}},
year = {2019}
}
@techreport{Guderlei,
author = {Guderlei, Maike},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Guderlei - Unknown - Fake News Detection Evaluating Unsupervised Representation Learning for Detecting Stances of Fake News.pdf:pdf},
title = {{Fake News Detection Evaluating Unsupervised Representation Learning for Detecting Stances of Fake News}}
}
@misc{Gao2017,
author = {Gao, Liangcai and Yi, Xiaohan and Hao, Leipeng and Jiang, Zhuoren and Tang, Zhi},
title = {{ICDAR 2017 POD Competition: Evaluation}},
url = {http://www.icst.pku.edu.cn/cpdp/ICDAR2017{\_}PODCompetition/evaluation.html},
year = {2017}
}
@inproceedings{kumar2018identifying,
author = {Kumar, Vaibhav and Khattar, Dhruv and Gairola, Siddhartha and {Kumar Lal}, Yash and Varma, Vasudeva},
booktitle = {The 41st International ACM SIGIR Conference on Research {\&} Development in Information Retrieval},
pages = {1225--1228},
title = {{Identifying clickbait: A multi-strategy approach using neural networks}},
year = {2018}
}
@techreport{Obrien2018,
abstract = {Recent political events have lead to an increase in the popularity and spread of fake news. As demonstrated by the widespread effects of the large onset of fake news, humans are inconsistent if not outright poor detectors of fake news. With this, efforts have been made to automate the process of fake news detection. The most popular of such attempts include "blacklists" of sources and authors that are unreliable. While these tools are useful, in order to create a more complete end to end solution, we need to account for more difficult cases where reliable sources and authors release fake news. As such, the goal of this project was to create a tool for detecting the language patterns that characterize fake and real news through the use of machine learning and natural language processing techniques. The results of this project demonstrate the ability for machine learning to be useful in this task. We have built a model that catches many intuitive indications of real and fake news as well as an application that aids in the visualization of the classification decision. 2},
author = {O'brien, Nicole},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/O'brien - 2018 - Machine Learning for Detection of Fake News.pdf:pdf},
title = {{Machine Learning for Detection of Fake News}},
year = {2018}
}
@article{smilkov2019tensorflow,
author = {Smilkov, Daniel and Thorat, Nikhil and Assogba, Yannick and Yuan, Ann and Kreeger, Nick and Yu, Ping and Zhang, Kangyi and Cai, Shanqing and Nielsen, Eric and Soergel, David and Others},
journal = {arXiv preprint arXiv:1901.05350},
title = {{Tensorflow. js: Machine learning for the web and beyond}},
year = {2019}
}
@article{FrancoisChollet2017,
abstract = {Summary Deep Learning with Python introduces the field of deep learning using the Python language and the powerful Keras library. Written by Keras creator and Google AI researcher Fran{\c{c}}ois Chollet, this book builds your understanding through intuitive explanations and practical examples. Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications. About the Technology Machine learning has made remarkable progress in recent years. We went from near-unusable speech and image recognition, to near-human accuracy. We went from machines that couldn't beat a serious Go player, to defeating a world champion. Behind this progress is deep learning--a combination of engineering advances, best practices, and theory that enables a wealth of previously impossible smart applications. About the Book Deep Learning with Python introduces the field of deep learning using the Python language and the powerful Keras library. Written by Keras creator and Google AI researcher Fran{\c{c}}ois Chollet, this book builds your understanding through intuitive explanations and practical examples. You'll explore challenging concepts and practice with applications in computer vision, natural-language processing, and generative models. By the time you finish, you'll have the knowledge and hands-on skills to apply deep learning in your own projects. What's Inside Deep learning from first principles Setting up your own deep-learning environment Image-classification models Deep learning for text and sequences Neural style transfer, text generation, and image generation About the Reader Readers need intermediate Python skills. No previous experience with Keras, TensorFlow, or machine learning is required. About the Author Fran{\c{c}}ois Chollet works on deep learning at Google in Mountain View, CA. He is the creator of the Keras deep-learning library, as well as a contributor to the TensorFlow machine-learning framework. He also does deep-learning research, with a focus on computer vision and the application of machine learning to formal reasoning. His papers have been published at major conferences in the field, including the Conference on Computer Vision and Pattern Recognition (CVPR), the Conference and Workshop on Neural Information Processing Systems (NIPS), the International Conference on Learning Representations (ICLR), and others. Table of Contents PART 1 - FUNDAMENTALS OF DEEP LEARNING What is deep learning? Before we begin: the mathematical building blocks of neural networks Getting started with neural networks Fundamentals of machine learning PART 2 - DEEP LEARNING IN PRACTICE Deep learning for computer vision Deep learning for text and sequences Advanced deep-learning best practices Generative deep learning Conclusions appendix A - Installing Keras and its dependencies on Ubuntu appendix B - Running Jupyter notebooks on an EC2 GPU instance},
author = {{Fran{\c{c}}ois Chollet}},
journal = {Deep Learning with Python},
title = {{Deep Learning with Python}},
year = {2017}
}
@article{raamkumar2020use,
author = {Raamkumar, Aravind Sesagiri and Tan, Soon Guan and Wee, Hwee Lin},
journal = {JMIR public health and surveillance},
number = {3},
pages = {e20493},
publisher = {JMIR Publications Inc., Toronto, Canada},
title = {{Use of health belief model--based deep learning classifiers for covid-19 social media content to examine public perceptions of physical distancing: Model development and case study}},
volume = {6},
year = {2020}
}
@article{roberts2018magenta,
author = {Roberts, Adam and Hawthorne, Curtis and Simon, Ian},
title = {{Magenta. js: A javascript api for augmenting creativity with deep learning}},
year = {2018}
}
@book{AntonioGuili;AmitaKapoor;SujitPal2019,
abstract = {2nd ed. Predicting house price using linear regression Deep Learning with TensorFlow 2 and Keras, Second Edition teaches deep learning techniques alongside TensorFlow (TF) and Keras. The book introduces neural networks with TensorFlow, runs through the main applications, covers two working example apps, and then dives into TF and cloudin production, TF mobile, and using TensorFlow with AutoML. Cover -- Copyright -- Packt Page -- Contributors -- Table of Contents -- Preface -- Chapter 1: Neural Network Foundations with TensorFlow 2.0 -- What is TensorFlow (TF)? -- What is Keras? -- What are the most important changes in TensorFlow 2.0? -- Introduction to neural networks -- Perceptron -- A first example of TensorFlow 2.0 code -- Multi-layer perceptron -- our first example of a network -- Problems in training the perceptron and their solutions -- Activation function -- sigmoid -- Activation function -- tanh -- Activation function -- ReLU Two additional activation functions -- ELU and LeakyReLU -- Activation functions -- In short -- what are neural networks after all? -- A real example -- recognizing handwritten digits -- One-hot encoding (OHE) -- Defining a simple neural network in TensorFlow 2.0 -- Running a simple TensorFlow 2.0 net and establishing a baseline -- Improving the simple net in TensorFlow 2.0 with hidden layers -- Further improving the simple net in TensorFlow with Dropout -- Testing different optimizers in TensorFlow 2.0 -- Increasing the number of epochs -- Controlling the optimizer learning rate Increasing the number of internal hidden neurons -- Increasing the size of batch computation -- Summarizing experiments run for recognizing handwritten charts -- Regularization -- Adopting regularization to avoid overfitting -- Understanding BatchNormalization -- Playing with Google Colab -- CPUs, GPUs, and TPUs -- Sentiment analysis -- Hyperparameter tuning and AutoML -- Predicting output -- A practical overview of backpropagation -- What have we learned so far? -- Towards a deep learning approach -- References -- Chapter 2: TensorFlow 1.x and 2.x -- Understanding TensorFlow 1.x TensorFlow 1.x computational graph program structure -- Computational graphs -- Working with constants, variables, and placeholders -- Examples of operations -- Constants -- Sequences -- Random tensors -- Variables -- An example of TensorFlow 1.x in TensorFlow 2.x -- Understanding TensorFlow 2.x -- Eager execution -- AutoGraph -- Keras APIs -- three programming models -- Sequential API -- Functional API -- Model subclassing -- Callbacks -- Saving a model and weights -- Training from tf.data.datasets -- tf.keras or Estimators? -- Ragged tensors -- Custom training Distributed training in TensorFlow 2.x -- Multiple GPUs -- MultiWorkerMirroredStrategy -- TPUStrategy -- ParameterServerStrategy -- Changes in namespaces -- Converting from 1.x to 2.x -- Using TensorFlow 2.x effectively -- The TensorFlow 2.x ecosystem -- Language bindings -- Keras or tf.keras? -- Summary -- Chapter 3: Regression -- What is regression? -- Prediction using linear regression -- Simple linear regression -- Multiple linear regression -- Multivariate linear regression -- TensorFlow Estimators -- Feature columns -- Input functions -- MNIST using TensorFlow Estimator API},
author = {{Antonio Guili; Amita Kapoor; Sujit Pal}},
file = {:Users/ugurtigu/Downloads/Deep Learning with TensorFlow 2.0 and Keras Regression, ConvNets, GANs, RNNs, NLP  more with TF 2.0 and the Keras API by Antonio Gulli, Amita Kapoor, Sujit Pal (z-lib.org).pdf:pdf},
isbn = {9781838823412},
title = {{Deep Learning with TensorFlow 2 and Keras: Regression, ConvNets, GANs, RNNs, NLP, and More with TensorFlow 2 and the Keras API}},
year = {2019}
}
@article{glenski2017fishing,
author = {Glenski, Maria and Ayton, Ellyn and Arendt, Dustin and Volkova, Svitlana},
journal = {arXiv preprint arXiv:1710.06390},
title = {{Fishing for clickbaits in social images and texts with linguistically-infused neural network models}},
year = {2017}
}
@inproceedings{vorakitphan2018clickbait,
author = {Vorakitphan, Vorakit and Leu, Fang-Yie and Fan, Yao-Chung},
booktitle = {International Conference on Innovative Mobile and Internet Services in Ubiquitous Computing},
organization = {Springer},
pages = {557--564},
title = {{Clickbait detection based on word embedding models}},
year = {2018}
}
@book{Chollet2018,
abstract = {Neural Networks and Deep Learning is a free online book. The book will teach you about: Neural networks, a beautiful biologically-inspired programming paradigm which enables a computer to learn from observational data Deep learning, a powerful set of techniques for learning in neural networks Neural networks and deep learning currently provide the best solutions to many problems in image recognition, speech recognition, and natural language processing. This book will teach you many of the core concepts behind neural networks and deep learning.},
author = {Neapolitan, Richard E. and Neapolitan, Richard E.},
booktitle = {Artificial Intelligence},
doi = {10.1201/b22400-15},
file = {:Users/ugurtigu/MasterThesis/BOOKS/Neural Networks and Deep Learning A Textbook by Charu C. Aggarwal (z-lib.org).pdf:pdf},
isbn = {9783319944623},
pages = {389--411},
title = {{Neural Networks and Deep Learning}},
year = {2018}
}
@techreport{Schmidhuber2014,
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks. LATEX source: Preface This is the preprint of an invited Deep Learning (DL) overview. One of its goals is to assign credit to those who contributed to the present state of the art. I acknowledge the limitations of attempting to achieve this goal. The DL research community itself may be viewed as a continually evolving, deep network of scientists who have influenced each other in complex ways. Starting from recent DL results, I tried to trace back the origins of relevant ideas through the past half century and beyond, sometimes using "local search" to follow citations of citations backwards in time. Since not all DL publications properly acknowledge earlier relevant work, additional global search strategies were employed , aided by consulting numerous neural network experts. As a result, the present preprint mostly consists of references. Nevertheless, through an expert selection bias I may have missed important work. A related bias was surely introduced by my special familiarity with the work of my own DL research group in the past quarter-century. For these reasons, this work should be viewed as merely a snapshot of an ongoing credit assignment process. To help improve it, please do not hesitate to send corrections and suggestions to juergen@idsia.ch.},
archivePrefix = {arXiv},
arxivId = {1404.7828v4},
author = {Schmidhuber, J{\"{u}}rgen},
eprint = {1404.7828v4},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Schmidhuber - 2014 - Deep Learning in Neural Networks An Overview.pdf:pdf},
title = {{Deep Learning in Neural Networks: An Overview}},
url = {http://www.idsia.ch/˜juergen/DeepLearning8Oct2014.texCompleteBIBTEXfile},
year = {2014}
}
@inproceedings{Anand2017,
abstract = {Online content publishers often use catchy headlines for their articles in order to attract users to their websites. These headlines, popularly known as clickbaits, exploit a user's curiosity gap and lure them to click on links that often disappoint them. Existing methods for automatically detecting clickbaits rely on heavy feature engineering and domain knowledge. Here, we introduce a neural network architecture based on Recurrent Neural Networks for detecting clickbaits. Our model relies on distributed word representations learned from a large unannotated corpora, and character embeddings learned via Convolutional Neural Networks. Experimental results on a dataset of news headlines show that our model outperforms existing techniques for clickbait detection with an accuracy of 0.98 with F1-score of 0.98 and ROC-AUC of 0.99.},
archivePrefix = {arXiv},
arxivId = {1612.01340},
author = {Anand, Ankesh and Chakraborty, Tanmoy and Park, Noseong},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-56608-5_46},
eprint = {1612.01340},
isbn = {9783319566078},
issn = {16113349},
keywords = {Clickbait detection,Deep learning,Neural networks},
pages = {541--547},
publisher = {Springer Verlag},
title = {{We used neural networks to detect clickbaits: You won't believe what happened next!}},
url = {https://link.springer.com/chapter/10.1007/978-3-319-56608-5{\_}46},
volume = {10193 LNCS},
year = {2017}
}
@article{nguyen2020real,
author = {Nguyen, Kha},
title = {{Real-time fashion items classification using TensorflowJS and ZalandoMNIST dataset}},
year = {2020}
}
@article{kowsari2019text,
author = {Kowsari, Kamran and {Jafari Meimandi}, Kiana and Heidarysafa, Mojtaba and Mendu, Sanjana and Barnes, Laura and Brown, Donald},
journal = {Information},
number = {4},
pages = {150},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Text classification algorithms: A survey}},
volume = {10},
year = {2019}
}
@book{And,
author = {Trask, Andrew. W.},
file = {:Users/ugurtigu/MasterThesis/BOOKS/Grokking Deep Learning by Andrew W. Trask (z-lib.org).pdf:pdf},
isbn = {9781617293702},
title = {{Grokking Deep Learning}}
}
@article{Rumelhart1986,
abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure 1. {\textcopyright} 1986 Nature Publishing Group.},
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
doi = {10.1038/323533a0},
issn = {00280836},
journal = {Nature},
number = {6088},
title = {{Learning representations by back-propagating errors}},
volume = {323},
year = {1986}
}
@article{ChehrehChelgani2018,
abstract = {Support vector regression (SVR) modeling was used to predict the coal flotation responses (recovery (R∗) and flotation rate constant (k)) as a function of measured particle properties and hydrodynamic flotation variables. Coal flotation is a complicated multifaceted separation process and many measurable and unmeasurable variables can be considered for its modeling. Therefore, feature selection can be used to save time and cost of measuring irrelevant parameters. Mutual information (MI) as a powerful variable selection tool was used through laboratory measured variables to assess interactions and choose the most effective ones for predictions of R∗ and k. Feature selection by MI through variables indicated that the best arrangements for the R∗ and k predictions are the sets of particle Reynolds number-energy dissipation and particle size-bubble Reynolds number, respectively. Correlation of determination (R2) and difference between laboratory measured and SVR predicted values based on MI selected variables indicated that the SVR can model R∗ and k quite accurately with R2 = 0.93 and R2 = 0.72, respectively. These results demonstrated that the MI-SVR combination can quite satisfactorily measure the importance of variables, increase interpretability, reduce the risk of overfitting, decrease complexity and generate predictive models for high dimension of variables based on selected features for complicated processing systems.},
author = {{Chehreh Chelgani}, S. and Shahbazi, B. and Hadavandi, E.},
doi = {10.1016/j.measurement.2017.09.025},
file = {:Users/ugurtigu/Downloads/323533a0.pdf:pdf},
issn = {02632241},
journal = {Measurement: Journal of the International Measurement Confederation},
keywords = {Flotation rate constant,Mutual information,Recovery,Support vector regression,Variable importance measurement},
number = {2},
pages = {102--108},
title = {{Support vector regression modeling of coal flotation based on variable importance measurements by mutual information method}},
volume = {114},
year = {2018}
}
@article{Suah2017,
abstract = {A greener analytical procedure based on automated flow through system with an optical sensor is proposed for determination of Co(II). The flow through system consisted of polymer inclusion membrane (PIM) containing potassium thiocyanate (KSCN) that was placed between the measuring cell and fixed with optical sensor probe as an optical sensor for monitoring of Co(II) at 625 nm. In the presence of Co(II) ions, the colourless membrane changes to blue. The sensing membrane was prepared by incorporating SCN into a non plasticized PIM. The prepared PIM were found to be homogenous, transparent and mechanically stable. The optode shows reversible optical response in the range of 1.00 × 10−6 – 1.00 × 10−3 mol L−1 with detection limit of 6.10 × 10−7 mol L−1. The optode can be regenerated by using 0.1 mol L−1 of ethylenediaminetetraacetic acid (EDTA). The main parameters of the computer controlled flow system incorporating the flow-through optode, a multi-port selection valve and peristaltic pump were optimized too. The calculated Relative Standard Deviation (R.S.D) of the repeatability and reproducibility of the method are 0.76{\%} and 4.73{\%}, respectively. This green system has been applied to the determination of Co(II) in wastewater samples with reduced reagents and samples consumption and minimum waste generation.},
author = {Suah, Faiz Bukhari Mohd},
doi = {10.1016/j.ancr.2017.02.001},
file = {:Users/ugurtigu/Downloads/Zeiler-Fergus2014{\_}Chapter{\_}VisualizingAndUnderstandingCon.pdf:pdf},
issn = {22141812},
journal = {Analytical Chemistry Research},
keywords = {Aliquat 336,Cobalt(II),Flow through system,Green analytical chemistry,Optode,Polymer inclusion membrane},
pages = {40--46},
title = {{Preparation and characterization of a novel Co(II) optode based on polymer inclusion membrane}},
volume = {12},
year = {2017}
}
@techreport{Nielsen,
author = {Nielsen, Michael},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Nielsen - Unknown - Neural Networks and Deep Learning.pdf:pdf},
title = {{Neural Networks and Deep Learning}},
url = {http://neuralnetworksanddeeplearning.com}
}
@misc{nordberg2020crucial,
author = {Nordberg, Gustav and Grandien, Jesper},
title = {{The crucial parts of text classification with TensorFlow. js and categorisation of news articles}},
year = {2020}
}
@article{Werbos1990,
abstract = {Backpropagation is now the most widely used tool in the field of artificial neural networks. At the core of backpropagation is a method for calculating derivatives exactly and efficiently in any large system made up of elementary subsystems or calculations which are represented by known, differentiable functions; thus, backpropagation has many applications which do not involve neural networks as such. This paper first reviews basic backpropagation, a simple method which is now being widely used in areas like pattern recognition and fault diagnosis. Next, it presents the basic equations for back-propagation through time, and discusses applications to areas like pattern recognition involving dynamic systems, systems identification, and control. Finally, it describes further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations or true recurrent networks, and other practical issues which arise with this method. Pseudocode is provided to clarify the algorithms. The chain rule forordered derivatives—the theorem which underlies backpropagation—is briefly discussed. {\textcopyright} 1990, IEEE},
author = {Werbos, Paul J.},
doi = {10.1109/5.58337},
issn = {15582256},
journal = {Proceedings of the IEEE},
number = {10},
pages = {1550--1560},
title = {{Backpropagation Through Time: What It Does and How to Do It}},
volume = {78},
year = {1990}
}
@inproceedings{zhang2015character,
author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
booktitle = {Advances in neural information processing systems},
pages = {649--657},
title = {{Character-level convolutional networks for text classification}},
year = {2015}
}
@incollection{rivera2020identifying,
author = {Rivera, Juan De Dios Santos},
booktitle = {Practical TensorFlow. js},
pages = {151--162},
publisher = {Springer},
title = {{Identifying toxic text from a Google Chrome Extension}},
year = {2020}
}
@article{altinel2018semantic,
author = {Altinel, Berna and Ganiz, Murat Can},
journal = {Information Processing {\&} Management},
number = {6},
pages = {1129--1153},
publisher = {Elsevier},
title = {{Semantic text classification: A survey of past and recent advances}},
volume = {54},
year = {2018}
}
@article{eren2019generic,
author = {Eren, Levent and Ince, Turker and Kiranyaz, Serkan},
journal = {Journal of Signal Processing Systems},
number = {2},
pages = {179--189},
publisher = {Springer},
title = {{A generic intelligent bearing fault diagnosis system using compact adaptive 1D CNN classifier}},
volume = {91},
year = {2019}
}
