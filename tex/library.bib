Automatically generated by Mendeley Desktop 1.19.4
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@article{Biyani2016,
abstract = {Clickbaits are articles with misleading titles, exaggerating the content on the landing page. Their goal is to entice users to click on the title in order to monetize the landing page. The content on the landing page is usually of low quality. Their presence in user homepage stream of news aggregator sites (e.g., Yahoo news, Google news) may adversely impact user experience. Hence, it is important to identify and demote or block them on homepages. In this paper, we present a machine-learning model to detect clickbaits. We use a variety of features and show that the degree of informality of a web-page (as measured by different metrics) is a strong indicator of it being a clickbait. We conduct extensive experiments to evaluate our approach and analyze properties of clickbait and non-clickbait articles. Our model achieves high performance (74.9{\%} F-1 score) in predicting clickbaits.},
author = {Biyani, Prakhar and Tsioutsiouliklis, Kostas and Blackmer, John},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Biyani, Tsioutsiouliklis, Blackmer - 2016 - Detecting Clickbaits in News Streams Using Article Informality.pdf:pdf},
isbn = {9781577357605},
journal = {Thirtieth AAAI Conference on Artificial Intelligence},
keywords = {Technical Papers: Artificial Intelligence and the},
pages = {94--100},
title = {{Detecting Clickbaits in News Streams Using Article Informality}},
year = {2016}
}
@article{Bunde2019,
author = {Bunde, Enrico},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Bunde - 2019 - Sentimentanalyse deutscher Twitter-Korpora mittels Deep Learning.pdf:pdf},
pages = {130},
title = {{Sentimentanalyse deutscher Twitter-Korpora mittels Deep Learning}},
year = {2019}
}
@article{Main,
abstract = {The use of alluring headlines (clickbait) to tempt the readers has become a growing practice nowadays. For the sake of existence in the highly competitive media industry, most of the on-line media including the mainstream ones, have started following this practice. Although the widespread practice of clickbait makes the reader's reliability on media vulnerable, a large scale analysis to reveal this fact is still absent. In this paper, we analyze 1.67 million Facebook posts created by 153 media organizations to understand the extent of clickbait practice, its impact and user engagement by using our own developed clickbait detection model. The model uses distributed sub-word embeddings learned from a large corpus. The accuracy of the model is 98.3{\%}. Powered with this model, we further study the distribution of topics in clickbait and non-clickbait contents.},
author = {Main, Md and Rony, Uddin and Hassan, Naeemul and Yousuf, Mohammad},
doi = {10.1145/3110025.3110054},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Main et al. - Unknown - Diving Deep into Clickbaits Who Use Them to What Extents in Which Topics with What Effects(2).pdf:pdf},
isbn = {9781450349932},
title = {{Diving Deep into Clickbaits: Who Use Them to What Extents in Which Topics with What Effects?}},
url = {http://dx.doi.org/10.1145/3110025.3110054}
}
@article{Zannettou2018,
abstract = {The use of deceptive techniques in user-generated video portals is ubiquitous. Unscrupulous uploaders deliberately mislabel video descriptors aiming at increasing their views and subsequently their ad revenue. This problem, usually referred to as 'clickbait,' may severely undermine user experience. In this work, we study the clickbait problem on YouTube by collecting metadata for 206k videos. To address it, we devise a deep learning model based on variational autoencoders that supports the diverse modalities of data that videos include. The proposed model relies on a limited amount of manually labeled data to classify a large corpus of unlabeled data. Our evaluation indicates that the proposed model offers improved performance when compared to other conventional models. Our analysis of the collected data indicates that YouTube recommendation engine does not take into account clickbait. Thus, it is susceptible to recommending misleading videos to users.},
author = {Zannettou, Savvas and Chatzis, Sotirios and Papadamou, Kostantinos and Sirivianos, Michael},
doi = {10.1109/SPW.2018.00018},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Zannettou et al. - 2018 - The good, the bad and the bait Detecting and characterizing clickbait on youtube.pdf:pdf},
isbn = {9780769563497},
journal = {Proceedings - 2018 IEEE Symposium on Security and Privacy Workshops, SPW 2018},
keywords = {Clickbait,Deep learning,YouTube},
pages = {63--69},
publisher = {IEEE},
title = {{The good, the bad and the bait: Detecting and characterizing clickbait on youtube}},
year = {2018}
}
@techreport{VanDerMaaten2008,
abstract = {We present a new technique called "t-SNE" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualiza-tions produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.},
author = {{Van Der Maaten}, Laurens and Hinton, Geoffrey},
booktitle = {Journal of Machine Learning Research},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Van Der Maaten, Hinton - 2008 - Visualizing Data using t-SNE.pdf:pdf},
keywords = {dimensionality reduction,embedding algorithms,manifold learning,multidimensional scaling,visualization},
pages = {2579--2605},
title = {{Visualizing Data using t-SNE}},
volume = {9},
year = {2008}
}
@misc{Manning1999,
author = {Manning, Christopher and Sch{\"{u}}tze, Hinrich},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Manning, Sch{\"{u}}tze - 1999 - Manning {\&} Sch{\"{u}}tze 2003 - Foundations of Statistical NLP.pdf.pdf:pdf},
pages = {675},
title = {{Manning {\&} Sch{\"{u}}tze 2003 - Foundations of Statistical NLP.pdf}},
year = {1999}
}
@article{Kaur2020,
abstract = {Clickbait indicates the type of content with an intending goal to attract the attention of readers. It has grown to become a nuisance to social media users. The purpose of clickbait is to bring an appealing link in front of users. Clickbaits seen in the form of headlines influence people to get attracted and curious to read the inside content. The content seen in the form of text on clickbait posts is very short to identify its features as clickbait. In this paper, a novel approach (two-phase hybrid CNN-LSTM Biterm model) has been proposed for modeling short topic content. The hybrid CNN-LSTM model when implemented with pre-trained GloVe embedding yields the best results based on accuracy, recall, precision, and F1-score performance metrics. The proposed model achieves 91.24{\%}, 95.64{\%}, 95.87{\%} precision values for Dataset 1, Dataset 2 and Dataset 3, respectively. Eight types of clickbait such as Reasoning, Number, Reaction, Revealing, Shocking/Unbelievable, Hypothesis/Guess, Questionable, Forward referencing are classified in this work using the Biterm Topic Model (BTM). It has been shown that the clickbaits such as Shocking/Unbelievable, Hypothesis/Guess and Reaction are the highest in numbers among rest of the clickbait headlines published online. Also, a ground dataset of non-textual (image-based) data using multiple social media platforms has been created in this paper. The textual information has been retrieved from the images with the help of OCR tool. A comparative study is performed to show the effectiveness of our proposed model which helps to identify the various categories of clickbait headlines that are spread on social media platforms.},
author = {Kaur, Sawinder and Kumar, Parteek and Kumaraguru, Ponnurangam},
doi = {10.1016/j.eswa.2020.113350},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Classifier,Clickbait,Features,News,Social media},
month = {aug},
pages = {113350},
publisher = {Elsevier Ltd},
title = {{Detecting clickbaits using two-phase hybrid CNN-LSTM biterm model}},
volume = {151},
year = {2020}
}
@article{StanfordUniversityCoursecs231n2018a,
abstract = {Convolutional Neural Networks are very similar to ordinary Neural Networks from the previous chapter: they are made up of neurons that have learnable weights and biases. Each neuron receives some inputs, performs a dot product and optionally follows it with a non-linearity. The whole network still expresses a single differentiable score function: from the raw image pixels on one end to class scores at the other. And they still have a loss function (e.g. SVM/Softmax) on the last (fully-connected) layer and all the tips/tricks we developed for learning regular Neural Networks still apply. So what does change? ConvNet architectures make the explicit assumption that the inputs are images, which allows us to encode certain properties into the architecture. These then make the forward function more efficient to implement and vastly reduce the amount of parameters in the network.},
author = {{Stanford University Course cs231n}},
journal = {Stanford University Course cs231n},
pages = {30},
title = {{CS231n Convolutional Neural Networks for Visual Recognition}},
url = {https://cs231n.github.io/convolutional-networks/},
year = {2018}
}
@article{altinel2018semantic,
author = {Altinel, Berna and Ganiz, Murat Can},
journal = {Information Processing {\&} Management},
number = {6},
pages = {1129--1153},
publisher = {Elsevier},
title = {{Semantic text classification: A survey of past and recent advances}},
volume = {54},
year = {2018}
}
@book{Kulkarni2019,
abstract = {Includes index. Implement natural language processing applications with Python using a problem-solution approach. This book has numerous coding exercises that will help you to quickly deploy natural language processing techniques, such as text classification, parts of speech identification, topic modeling, text summarization, and sentiment analysis. "Natural language processing recipes" starts by offering solutions for cleaning and preprocessing text data and ways to analyze it with advanced algorithms. You'll see practical applications of the semantic as well as syntactic analysis of text, as well as complex natural language processing approaches that involve text normalization, advanced preprocessing, POS tagging, parsing, text summarization, and sentiment analysis. You will also learn various applications of machine learning and deep learning in natural language processing. By using the recipes in this book, you will have a toolbox of solutions to apply to your own projects in the real world, making your development time quicker and more efficient. You will: Apply NLP techniques using Python libraries such as NLTK, TextBlob, soaCy, Stanford CoreNLP, and many more ; Implement the concepts of information retrieval, text summarization, sentiment analysis, and other advanced natural language processing techniques ; Identify machine learning and deep learning techniques for natural language processing and natural language generation problems. Introduction -- Extracting the data -- Exploring and processing text data -- Converting text to features -- Advanced natural language processing -- Implementing industry applications -- Deep learning for NLP.},
archivePrefix = {arXiv},
arxivId = {arXiv:1510.00726v1},
author = {Kulkarni, Akshay and Shivananda, Adarsha},
eprint = {arXiv:1510.00726v1},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Kulkarni, Shivananda - 2019 - Natural Language Processing Recipes Unlocking Text Data with Machine Learning and Deep Learning using Pyth.pdf:pdf},
isbn = {2006062298},
issn = {00010782},
pages = {234},
pmid = {21603045},
title = {{Natural Language Processing Recipes: Unlocking Text Data with Machine Learning and Deep Learning using Python}},
year = {2019}
}
@article{Kumar,
abstract = {Online media outlets, in a bid to expand their reach and subsequently increase revenue through ad monetisation, have begun adopting clickbait techniques to lure readers to click on articles. The article fails to fulfill the promise made by the headline. Traditional methods for clickbait detection have relied heavily on feature engineering which, in turn, is dependent on the dataset it is built for. The application of neural networks for this task has only been explored partially. We propose a novel approach considering all information found in a social media post. We train a bidirectional LSTM with an attention mechanism to learn the extent to which a word contributes to the post's clickbait score in a differential manner. We also employ a Siamese net to capture the similarity between source and target information. Information gleaned from images has not been considered in previous approaches. We learn image embeddings from large amounts of data using Convolutional Neural Networks to add another layer of complexity to our model. Finally, we concatenate the outputs from the three separate components , serving it as input to a fully connected layer. We conduct experiments over a test corpus of 19538 social media posts, attaining an F1 score of 65.37{\%} on the dataset bettering the previous state-of-the-art, as well as other proposed approaches, feature engineering or otherwise.},
author = {Kumar, Vaibhav and Khattar, Dhruv and Gairola, Siddhartha and {Kumar Lal}, Yash and Varma, Vasudeva},
doi = {10.1145/3209978.3210144},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Kumar et al. - Unknown - Identifying Clickbait A Multi-Strategy Approach Using Neural Networks Identifying Clickbait A Multi-Strategy A.pdf:pdf},
isbn = {9781450356572},
keywords = {Attention-Mechanism,Clickbait,Image Embeddings,Neural Network,Siamese Network,Text Embeddings},
title = {{Identifying Clickbait: A Multi-Strategy Approach Using Neural Networks * Identifying Clickbait: A Multi-Strategy Approach Using}},
url = {https://doi.org/10.1145/3209978.3210144}
}
@techreport{Zhang,
abstract = {Convolutional Neural Networks (CNNs) have recently achieved remarkably strong performance on the practically important task of sentence classification (Kim, 2014; Kalchbrenner et al., 2014; Johnson and Zhang, 2014). However, these models require practitioners to specify an exact model architecture and set accompanying hyperparameters, including the filter region size, regularization parameters, and so on. It is currently unknown how sensitive model performance is to changes in these configurations for the task of sentence classification. We thus conduct a sensitivity analysis of one-layer CNNs to explore the effect of architecture components on model performance; our aim is to distinguish between important and comparatively inconsequential design decisions for sentence classification. We focus on one-layer CNNs (to the exclusion of more complex models) due to their comparative simplicity and strong empirical performance, which makes it a modern standard baseline method akin to Support Vector Machine (SVMs) and logistic regression. We derive practical advice from our extensive empirical results for those interested in getting the most out of CNNs for sentence classification in real world settings.},
archivePrefix = {arXiv},
arxivId = {1510.03820v4},
author = {Zhang, Ye and Wallace, Byron C},
eprint = {1510.03820v4},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Zhang, Wallace - Unknown - A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classifica.pdf:pdf},
title = {{A Sensitivity Analysis of (and Practitioners' Guide to) Convolutional Neural Networks for Sentence Classification}},
url = {http://nlp.stanford.edu/projects/}
}
@book{IanGoodfellowYoshuaBengio2016,
author = {{Ian Goodfellow, Yoshua Bengio}, Aaron Courville},
booktitle = {Prmu},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Ian Goodfellow, Yoshua Bengio - 2016 - Deep Learning.pdf:pdf},
title = {{Deep Learning}},
year = {2016}
}
@incollection{aggarwal2012survey,
author = {Aggarwal, Charu C and Zhai, ChengXiang},
booktitle = {Mining text data},
pages = {163--222},
publisher = {Springer},
title = {{A survey of text classification algorithms}},
year = {2012}
}
@inproceedings{Zeiler2014,
abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark Krizhevsky et al. [18]. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we explore both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. Used in a diagnostic role, these visualizations allow us to find model architectures that outperform Krizhevsky et al on the ImageNet classification benchmark. We also perform an ablation study to discover the performance contribution from different model layers. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets. {\textcopyright} 2014 Springer International Publishing.},
archivePrefix = {arXiv},
arxivId = {1311.2901},
author = {Zeiler, Matthew D. and Fergus, Rob},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-10590-1_53},
eprint = {1311.2901},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Zeiler, Fergus - 2014 - Visualizing and understanding convolutional networks.pdf:pdf},
isbn = {9783319105895},
issn = {16113349},
number = {PART 1},
pages = {818--833},
publisher = {Springer Verlag},
title = {{Visualizing and understanding convolutional networks}},
url = {https://link.springer.com/chapter/10.1007/978-3-319-10590-1{\_}53},
volume = {8689 LNCS},
year = {2014}
}
@techreport{Kobler,
author = {Kobler, Florian and {Angestrebter Akademischer Grad}, B A},
title = {{Klick mich!}}
}
@book{Chollet2018,
abstract = {Neural Networks and Deep Learning is a free online book. The book will teach you about: Neural networks, a beautiful biologically-inspired programming paradigm which enables a computer to learn from observational data Deep learning, a powerful set of techniques for learning in neural networks Neural networks and deep learning currently provide the best solutions to many problems in image recognition, speech recognition, and natural language processing. This book will teach you many of the core concepts behind neural networks and deep learning.},
author = {Neapolitan, Richard E. and Neapolitan, Richard E.},
booktitle = {Artificial Intelligence},
doi = {10.1201/b22400-15},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Neapolitan, Neapolitan - 2018 - Neural Networks and Deep Learning.pdf:pdf},
isbn = {9783319944623},
pages = {389--411},
title = {{Neural Networks and Deep Learning}},
year = {2018}
}
@techreport{Guderlei,
author = {Guderlei, Maike},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Guderlei - Unknown - Fake News Detection Evaluating Unsupervised Representation Learning for Detecting Stances of Fake News.pdf:pdf},
title = {{Fake News Detection Evaluating Unsupervised Representation Learning for Detecting Stances of Fake News}}
}
@article{Nielsen2014,
abstract = {BACKGROUND: Causes of mandibular condylar (condylar) head necrosis as a consequence of intracapsular mandibular fractures are still a subject of controversy.$\backslash$n$\backslash$nOBJECTIVES: To investigate why in some cases of intracapsular fractures condylar head necrosis occurs.$\backslash$n$\backslash$nMATERIAL: 58 human heads from the collection of Head and Neck Clinical Anatomy Laboratory, from the Institute of Physiology and Pathology of Hearing, Warsaw, Poland, constituted the material.$\backslash$n$\backslash$nSTUDY: Head arterial tree injections, anatomical preparation with the use of standard set of microsurgical equipment and an operating microscope.$\backslash$n$\backslash$nRESULTS: The main source of condylar head vascularization is the inferior alveolar artery, supplying bone marrow of the whole mandible as well as its cortical layer. Additional arterial blood supplying comes from a various number (2-7) of branches supplying the temporomandibular joint capsule. They originate directly from the maxillary artery or from its primary branches: masseteric artery, external pterygoid artery or superficial temporal artery. Two rare variants of accessory mandibular head vascularization were encountered. The first (2 cases) was an arterial branch from the maxillary artery and the second (1 case) was a branch from the external pterygoid artery. In these cases the arterial supply of lateral part of temporomandibular joint capsule from other sources was reduced.$\backslash$n$\backslash$nCONCLUSION: Fractures resulting in the lateral part of the condylar head in isolation could be potentially threatened by necrosis because of poor vascularization.},
author = {Nielsen, Michael},
journal = {Neural Networks and Deep Learning},
title = {{Improving the way neural networks learn The cross-entropy cost function - Chapter 3}},
year = {2014}
}
@article{Nielsen2015,
author = {Nielsen, MA},
title = {{Neural networks and deep learning}},
year = {2015}
}
@misc{widlml,
author = {{Francois Chaubard}, Rohit and Mundra, Richard Socher},
title = {{Understanding Convolutional Neural Networks for NLP – WildML}},
url = {http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/},
urldate = {2020-12-15}
}
@misc{Gao2017,
author = {Gao, Liangcai and Yi, Xiaohan and Hao, Leipeng and Jiang, Zhuoren and Tang, Zhi},
title = {{ICDAR 2017 POD Competition: Evaluation}},
url = {http://www.icst.pku.edu.cn/cpdp/ICDAR2017{\_}PODCompetition/evaluation.html},
year = {2017}
}
@article{Rumelhart1986,
abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure 1. {\textcopyright} 1986 Nature Publishing Group.},
author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
doi = {10.1038/323533a0},
issn = {00280836},
journal = {Nature},
number = {6088},
title = {{Learning representations by back-propagating errors}},
volume = {323},
year = {1986}
}
@article{kiranyaz20191d,
author = {Kiranyaz, Serkan and Avci, Onur and Abdeljaber, Osama and Ince, Turker and Gabbouj, Moncef and Inman, Daniel J},
journal = {arXiv preprint arXiv:1905.03554},
title = {{1D convolutional neural networks and applications: A survey}},
year = {2019}
}
@article{Lecun2015,
abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
author = {Lecun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
doi = {10.1038/nature14539},
file = {:Users/ugurtigu/Downloads/nature14539.pdf:pdf},
issn = {14764687},
journal = {Nature},
number = {7553},
pages = {436--444},
pmid = {26017442},
title = {{Deep learning}},
volume = {521},
year = {2015}
}
@book{AntonioGuili;AmitaKapoor;SujitPal2019,
abstract = {2nd ed. Predicting house price using linear regression Deep Learning with TensorFlow 2 and Keras, Second Edition teaches deep learning techniques alongside TensorFlow (TF) and Keras. The book introduces neural networks with TensorFlow, runs through the main applications, covers two working example apps, and then dives into TF and cloudin production, TF mobile, and using TensorFlow with AutoML. Cover -- Copyright -- Packt Page -- Contributors -- Table of Contents -- Preface -- Chapter 1: Neural Network Foundations with TensorFlow 2.0 -- What is TensorFlow (TF)? -- What is Keras? -- What are the most important changes in TensorFlow 2.0? -- Introduction to neural networks -- Perceptron -- A first example of TensorFlow 2.0 code -- Multi-layer perceptron -- our first example of a network -- Problems in training the perceptron and their solutions -- Activation function -- sigmoid -- Activation function -- tanh -- Activation function -- ReLU Two additional activation functions -- ELU and LeakyReLU -- Activation functions -- In short -- what are neural networks after all? -- A real example -- recognizing handwritten digits -- One-hot encoding (OHE) -- Defining a simple neural network in TensorFlow 2.0 -- Running a simple TensorFlow 2.0 net and establishing a baseline -- Improving the simple net in TensorFlow 2.0 with hidden layers -- Further improving the simple net in TensorFlow with Dropout -- Testing different optimizers in TensorFlow 2.0 -- Increasing the number of epochs -- Controlling the optimizer learning rate Increasing the number of internal hidden neurons -- Increasing the size of batch computation -- Summarizing experiments run for recognizing handwritten charts -- Regularization -- Adopting regularization to avoid overfitting -- Understanding BatchNormalization -- Playing with Google Colab -- CPUs, GPUs, and TPUs -- Sentiment analysis -- Hyperparameter tuning and AutoML -- Predicting output -- A practical overview of backpropagation -- What have we learned so far? -- Towards a deep learning approach -- References -- Chapter 2: TensorFlow 1.x and 2.x -- Understanding TensorFlow 1.x TensorFlow 1.x computational graph program structure -- Computational graphs -- Working with constants, variables, and placeholders -- Examples of operations -- Constants -- Sequences -- Random tensors -- Variables -- An example of TensorFlow 1.x in TensorFlow 2.x -- Understanding TensorFlow 2.x -- Eager execution -- AutoGraph -- Keras APIs -- three programming models -- Sequential API -- Functional API -- Model subclassing -- Callbacks -- Saving a model and weights -- Training from tf.data.datasets -- tf.keras or Estimators? -- Ragged tensors -- Custom training Distributed training in TensorFlow 2.x -- Multiple GPUs -- MultiWorkerMirroredStrategy -- TPUStrategy -- ParameterServerStrategy -- Changes in namespaces -- Converting from 1.x to 2.x -- Using TensorFlow 2.x effectively -- The TensorFlow 2.x ecosystem -- Language bindings -- Keras or tf.keras? -- Summary -- Chapter 3: Regression -- What is regression? -- Prediction using linear regression -- Simple linear regression -- Multiple linear regression -- Multivariate linear regression -- TensorFlow Estimators -- Feature columns -- Input functions -- MNIST using TensorFlow Estimator API},
author = {{Antonio Guili; Amita Kapoor; Sujit Pal}},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Antonio Guili Amita Kapoor Sujit Pal - 2019 - Deep Learning with TensorFlow 2 and Keras Regression, ConvNets, GANs, RNNs, NLP, and More.pdf:pdf},
isbn = {9781838823412},
title = {{Deep Learning with TensorFlow 2 and Keras: Regression, ConvNets, GANs, RNNs, NLP, and More with TensorFlow 2 and the Keras API}},
year = {2019}
}
@techreport{Nielsen,
author = {Nielsen, Michael},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Nielsen - Unknown - Neural Networks and Deep Learning.pdf:pdf},
title = {{Neural Networks and Deep Learning}},
url = {http://neuralnetworksanddeeplearning.com}
}
@techreport{Potthastb,
abstract = {This paper proposes a new model for the detection of clickbait, i.e., short messages that lure readers to click a link. Clickbait is primarily used by online content publishers to increase their readership, whereas its automatic detection will give readers a way of filtering their news stream. We contribute by compiling the first clickbait corpus of 2992 Twitter tweets, 767 of which are clickbait, and, by developing a clickbait model based on 215 features that enables a random forest classifier to achieve 0.79 ROC-AUC at 0.76 precision and 0.76 recall.},
author = {Potthast, Martin and K{\"{o}}psel, Sebastian and Stein, Benno and Hagen, Matthias},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Potthast et al. - Unknown - Clickbait Detection.pdf:pdf},
isbn = {6941502517},
title = {{Clickbait Detection}}
}
@techreport{Jurafskya,
author = {Jurafsky, Daniel and Martin, James H},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Jurafsky, Martin - Unknown - Speech and Language Processing An Introduction to Natural Language Processing, Computational Linguistics, a.pdf:pdf},
title = {{Speech and Language Processing An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition Third Edition draft}}
}
@article{raamkumar2020use,
author = {Raamkumar, Aravind Sesagiri and Tan, Soon Guan and Wee, Hwee Lin},
journal = {JMIR public health and surveillance},
number = {3},
pages = {e20493},
publisher = {JMIR Publications Inc., Toronto, Canada},
title = {{Use of health belief model--based deep learning classifiers for covid-19 social media content to examine public perceptions of physical distancing: Model development and case study}},
volume = {6},
year = {2020}
}
@inproceedings{Anand2017a,
abstract = {Online content publishers often use catchy headlines for their articles in order to attract users to their websites. These headlines, popularly known as clickbaits, exploit a user's curiosity gap and lure them to click on links that often disappoint them. Existing methods for automatically detecting clickbaits rely on heavy feature engineering and domain knowledge. Here, we introduce a neural network architecture based on Recurrent Neural Networks for detecting clickbaits. Our model relies on distributed word representations learned from a large unannotated corpora, and character embeddings learned via Convolutional Neural Networks. Experimental results on a dataset of news headlines show that our model outperforms existing techniques for clickbait detection with an accuracy of 0.98 with F1-score of 0.98 and ROC-AUC of 0.99.},
archivePrefix = {arXiv},
arxivId = {1612.01340},
author = {Anand, Ankesh and Chakraborty, Tanmoy and Park, Noseong},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-56608-5_46},
eprint = {1612.01340},
isbn = {9783319566078},
issn = {16113349},
keywords = {Clickbait detection,Deep learning,Neural networks},
pages = {541--547},
publisher = {Springer Verlag},
title = {{We used neural networks to detect clickbaits: You won't believe what happened next!}},
url = {https://link.springer.com/chapter/10.1007/978-3-319-56608-5{\_}46},
volume = {10193 LNCS},
year = {2017}
}
@misc{Herculano-Houzel2009,
abstract = {The human brain has often been viewed as outstanding among mammalian brains: the most cognitively able, the largest-than-expected from body size, endowed with an overdeveloped cerebral cortex that represents over 80{\%} of brain mass, and purportedly containing 100 billion neurons and 10× more glial cells. Such uniqueness was seemingly necessary to justify the superior cognitive abilities of humans over larger-brained mammals such as elephants and whales. However, our recent studies using a novel method to determine the cellular composition of the brain of humans and other primates as well as of rodents and insectivores show that, since different cellular scaling rules apply to the brains within these orders, brain size can no longer be considered a proxy for the number of neurons in the brain. These studies also showed that the human brain is not exceptional in its cellular composition, as it was found to contain as many neuronal and non-neuronal cells as would be expected of a primate brain of its size. Additionally, the so-called overdeveloped human cerebral cortex holds only 19{\%} of all brain neurons, a fraction that is similar to that found in other mammals. In what regards absolute numbers of neurons, however, the human brain does have two advantages compared to other mammalian brains: compared to rodents, and probably to whales and elephants as well, it is built according to the very economical, space-saving scaling rules that apply to other primates; and, among economically built primate brains, it is the largest, hence containing the most neurons. These findings argue in favor of a view of cognitive abilities that is centered on absolute numbers of neurons, rather than on body size or encephalization, and call for a re-examination of several concepts related to the exceptionality of the human brain. {\textcopyright} 2009 Herculano-Houzel.},
author = {Herculano-Houzel, Suzana},
booktitle = {Frontiers in Human Neuroscience},
doi = {10.3389/neuro.09.031.2009},
issn = {16625161},
number = {NOV},
title = {{The human brain in numbers: A linearly scaled-up primate brain}},
volume = {3},
year = {2009}
}
@book{Chollet2017,
author = {Chollet, Francois},
doi = {10.23919/ICIF.2018.8455530},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Chollet - 2017 - Deep learning with Python.pdf:pdf},
isbn = {9780996452762},
title = {{Deep learning with Python}},
year = {2017}
}
@techreport{Lorent2018,
author = {Lorent, Simon and Itoo, Ashwin},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Lorent, Itoo - 2018 - Fake News Detection Using Machine Learning.pdf:pdf},
title = {{Fake News Detection Using Machine Learning}},
year = {2018}
}
@techreport{Mertens,
author = {Mertens, Lukas},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Mertens - Unknown - Zusammenfassung K{\"{u}}nstliche neuronale Netze am Beispiel der Klassifizierung von Scandaten.pdf:pdf},
title = {{Zusammenfassung K{\"{u}}nstliche neuronale Netze am Beispiel der Klassifizierung von Scandaten}}
}
@techreport{EnyinnaNwankpa,
abstract = {Deep neural networks have been successfully used in diverse emerging domains to solve real world complex problems with may more deep learning(DL) architectures, being developed to date. To achieve these state-of-the-art performances, the DL architectures use activation functions (AFs), to perform diverse computations between the hidden layers and the output layers of any given DL architecture. This paper presents a survey on the existing AFs used in deep learning applications and highlights the recent trends in the use of the activation functions for deep learning applications. The novelty of this paper is that it compiles majority of the AFs used in DL and outlines the current trends in the applications and usage of these functions in practical deep learning deployments against the state-of-the-art research results. This compilation will aid in making effective decisions in the choice of the most suitable and appropriate activation function for any given application, ready for deployment. This paper is timely because most research papers on AF highlights similar works and results while this paper will be the first, to compile the trends in AF applications in practice against the research results from literature, found in deep learning research to date.},
archivePrefix = {arXiv},
arxivId = {1811.03378v1},
author = {{Enyinna Nwankpa}, Chigozie and Ijomah, Winifred and Gachagan, Anthony and Marshall, Stephen},
eprint = {1811.03378v1},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Enyinna Nwankpa et al. - Unknown - Activation Functions Comparison of Trends in Practice and Research for Deep Learning.pdf:pdf},
keywords = {Index Terms activation function,activation function choices,activation function types,deep learning,learning algorithms,neural networks},
title = {{Activation Functions: Comparison of Trends in Practice and Research for Deep Learning}}
}
@inproceedings{kumar2018identifying,
author = {Kumar, Vaibhav and Khattar, Dhruv and Gairola, Siddhartha and {Kumar Lal}, Yash and Varma, Vasudeva},
booktitle = {The 41st International ACM SIGIR Conference on Research {\&} Development in Information Retrieval},
pages = {1225--1228},
title = {{Identifying clickbait: A multi-strategy approach using neural networks}},
year = {2018}
}
@inproceedings{severyn2015twitter,
author = {Severyn, Aliaksei and Moschitti, Alessandro},
booktitle = {Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {959--962},
title = {{Twitter sentiment analysis with deep convolutional neural networks}},
year = {2015}
}
@article{Gardner1998,
abstract = {Artificial neural networks are appearing as useful alternatives to traditional statistical modelling techniques in many scientific disciplines. This paper presents a general introduction and discussion of recent applications of the multilayer perceptron, one type of artificial neural network, in the atmospheric sciences.},
author = {Gardner, M. W. and Dorling, S. R.},
doi = {10.1016/S1352-2310(97)00447-0},
file = {:Users/ugurtigu/Downloads/s1352-2310(97)00447-0.pdf:pdf},
issn = {13522310},
journal = {Atmospheric Environment},
keywords = {Artificial intelligence,Backpropagation,Neural network,Statistical modelling},
number = {14-15},
pages = {2627--2636},
title = {{Artificial neural networks (the multilayer perceptron) - a review of applications in the atmospheric sciences}},
volume = {32},
year = {1998}
}
@techreport{Rosenblatt,
author = {Rosenblatt, F},
booktitle = {Psychological Review},
number = {6},
pages = {19--27},
title = {{THE PERCEPTRON: A PROBABILISTIC MODEL FOR INFORMATION STORAGE AND ORGANIZATION IN THE BRAIN 1}},
volume = {65}
}
@article{Vijgen2014,
author = {Vijgen, Bram},
issn = {2065-9555},
journal = {Studia Universitatis Babes-Bolyai - Ephemerides},
keywords = {BuzzFeed,exploring,listicle,shareability,success.},
number = {1},
pages = {103--122},
publisher = {Studia Universitatis Babes-Bolyai},
title = {{THE LISTICLE: AN EXPLORING RESEARCH ON AN INTERESTING SHAREABLE NEW MEDIA PHENOMENON}},
volume = {59},
year = {2014}
}
@techreport{Ter-Akopyan1101,
author = {Ter-Akopyan, Bagrat},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Ter-Akopyan - 1101 - Korpuskonstruktion und Entwicklung einer Pipeline f{\"{u}}r Clickbait-Spoiling Bachelorarbeit.pdf:pdf},
title = {{Korpuskonstruktion und Entwicklung einer Pipeline f{\"{u}}r Clickbait-Spoiling Bachelorarbeit}},
year = {1101}
}
@article{Agrawal2017,
abstract = {Clickbaits, in social media, are exaggerated headlines whose main motive is to mislead the reader to 'click' on them. They create a nuisance in the online experience by creating a lure towards poor content. Online content creators are utilizing more of them to get increased page views and thereby more ad revenue without providing the backing content. This paper proposes a model for detection of clickbait by utilizing convolutional neural networks and presents a compiled clickbait corpus. We create a corpus using multiple social media platforms and utilize deep learning for learning features rather than undergoing the long and complex process of feature engineering. Our model achieves high performance in identification of clickbaits.},
author = {Agrawal, Amol},
doi = {10.1109/NGCT.2016.7877426},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Agrawal - 2017 - Clickbait detection using deep learning.pdf:pdf},
isbn = {9781509032570},
journal = {Proceedings on 2016 2nd International Conference on Next Generation Computing Technologies, NGCT 2016},
keywords = {Clickbait,Convolutional neural networks,Deep learning},
number = {October},
pages = {268--272},
publisher = {IEEE},
title = {{Clickbait detection using deep learning}},
year = {2017}
}
@techreport{Mikolov,
abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
archivePrefix = {arXiv},
arxivId = {1301.3781v3},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
eprint = {1301.3781v3},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Mikolov et al. - Unknown - Efficient Estimation of Word Representations in Vector Space.pdf:pdf},
title = {{Efficient Estimation of Word Representations in Vector Space}},
url = {http://ronan.collobert.com/senna/}
}
@article{Zuhroh2019,
abstract = {Online news portals are currently one of the fastest sources of information used by people. Its impact is due to the credibility of the news produced by actors from the media industry, which is sometimes questioned. However, one of the problems associated with this medium used to obtain information is clickbait. This technique aims to attract users to click hyperbolic headlines with content that often disappoints the reader. This study was, therefore, conducted to determine: 1) existing dataset available. 2) The method used in clickbait detection which consists of data preprocessing, analysis of features, and classification. 3) Difference steps from the method used.},
author = {Zuhroh, Nurrida Aini and Rakhmawati, Aini},
doi = {10.26594/register.v6i1.1561},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Zuhroh, Rakhmawati - 2019 - Clickbait detection A literature review of the methods used.pdf:pdf},
journal = {Scientific Journal of Information System Technology},
number = {1},
pages = {2020--2021},
title = {{Clickbait detection: A literature review of the methods used}},
url = {http://doi.org/10.26594/register.v6i1.1561},
volume = {6},
year = {2019}
}
@techreport{Chakraborty,
abstract = {Most of the online news media outlets rely heavily on the revenues generated from the clicks made by their readers, and due to the presence of numerous such outlets, they need to compete with each other for reader attention. To attract the readers to click on an article and subsequently visit the media site, the outlets often come up with catchy headlines accompanying the article links, which lure the readers to click on the link. Such headlines are known as Clickbaits. While these baits may trick the readers into clicking, in the long-run, clickbaits usually don't live up to the expectation of the readers, and leave them disappointed. In this work, we attempt to automatically detect clickbaits and then build a browser extension which warns the readers of different media sites about the possibility of being baited by such headlines. The extension also offers each reader an option to block clickbaits she doesn't want to see. Then, using such reader choices, the extension automatically blocks similar clickbaits during her future visits. We run extensive offline and online experiments across multiple media sites and find that the proposed clickbait detection and the personalized blocking approaches perform very well achieving 93{\%} accuracy in detecting and 89{\%} accuracy in blocking clickbaits.},
archivePrefix = {arXiv},
arxivId = {1610.09786v1},
author = {Chakraborty, Abhijnan and Paranjape, Bhargavi and Kakarla, Sourya and Ganguly, Niloy},
eprint = {1610.09786v1},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Chakraborty et al. - Unknown - Stop Clickbait Detecting and Preventing Clickbaits in Online News Media.pdf:pdf},
title = {{Stop Clickbait: Detecting and Preventing Clickbaits in Online News Media}}
}
@techreport{Ruder,
abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
archivePrefix = {arXiv},
arxivId = {1609.04747v2},
author = {Ruder, Sebastian},
eprint = {1609.04747v2},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Ruder - Unknown - An overview of gradient descent optimization algorithms.pdf:pdf},
title = {{An overview of gradient descent optimization algorithms *}},
url = {http://caffe.berkeleyvision.org/tutorial/solver.html}
}
@techreport{Kinne,
author = {Kinne, Jan and Axenbeck, Janna},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Kinne, Axenbeck - Unknown - Web Mining of Firm Websites A Framework for Web Scraping and a Pilot Study for Germany.pdf:pdf},
keywords = {indicators,innovation,r{\&}d,r{\&}i,sti,text mining,web mining,web scraping},
title = {{Web Mining of Firm Websites: A Framework for Web Scraping and a Pilot Study for Germany}},
url = {http://ftp.zew.de/pub/zew-docs/dp/dp18033.pdf}
}
@misc{wordemdgood,
title = {{Embeddings: Translating to a Lower-Dimensional Space}},
url = {https://developers.google.com/machine-learning/crash-course/embeddings/translating-to-a-lower-dimensional-space},
urldate = {2020-12-15}
}
@techreport{Danforth2019,
author = {Danforth, Christopher and {Peter Sheridan Dodds}, Advisor and Hebert-Dufresne, Laurent and Rohan, Kelly and {Cynthia Forehand}, Chairperson J},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Danforth et al. - 2019 - Using Word Embeddings to Explore the Language of Depression on Twitter.pdf:pdf},
title = {{Using Word Embeddings to Explore the Language of Depression on Twitter}},
year = {2019}
}
@techreport{Obrien2018,
abstract = {Recent political events have lead to an increase in the popularity and spread of fake news. As demonstrated by the widespread effects of the large onset of fake news, humans are inconsistent if not outright poor detectors of fake news. With this, efforts have been made to automate the process of fake news detection. The most popular of such attempts include "blacklists" of sources and authors that are unreliable. While these tools are useful, in order to create a more complete end to end solution, we need to account for more difficult cases where reliable sources and authors release fake news. As such, the goal of this project was to create a tool for detecting the language patterns that characterize fake and real news through the use of machine learning and natural language processing techniques. The results of this project demonstrate the ability for machine learning to be useful in this task. We have built a model that catches many intuitive indications of real and fake news as well as an application that aids in the visualization of the classification decision. 2},
author = {O'brien, Nicole},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/O'brien - 2018 - Machine Learning for Detection of Fake News.pdf:pdf},
title = {{Machine Learning for Detection of Fake News}},
year = {2018}
}
@techreport{Chakrabortya,
abstract = {Most of the online news media outlets rely heavily on the revenues generated from the clicks made by their readers, and due to the presence of numerous such outlets, they need to compete with each other for reader attention. To attract the readers to click on an article and subsequently visit the media site, the outlets often come up with catchy headlines accompanying the article links, which lure the readers to click on the link. Such headlines are known as Clickbaits. While these baits may trick the readers into clicking, in the long-run, clickbaits usually don't live up to the expectation of the readers, and leave them disappointed. In this work, we attempt to automatically detect clickbaits and then build a browser extension which warns the readers of different media sites about the possibility of being baited by such headlines. The extension also offers each reader an option to block clickbaits she doesn't want to see. Then, using such reader choices, the extension automatically blocks similar clickbaits during her future visits. We run extensive offline and online experiments across multiple media sites and find that the proposed clickbait detection and the personalized blocking approaches perform very well achieving 93{\%} accuracy in detecting and 89{\%} accuracy in blocking clickbaits.},
archivePrefix = {arXiv},
arxivId = {1610.09786v1},
author = {Chakraborty, Abhijnan and Paranjape, Bhargavi and Kakarla, Sourya and Ganguly, Niloy},
eprint = {1610.09786v1},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Chakraborty et al. - Unknown - Stop Clickbait Detecting and Preventing Clickbaits in Online News Media(2).pdf:pdf},
title = {{Stop Clickbait: Detecting and Preventing Clickbaits in Online News Media}}
}
@article{nguyen2020real,
author = {Nguyen, Kha},
title = {{Real-time fashion items classification using TensorflowJS and ZalandoMNIST dataset}},
year = {2020}
}
@techreport{Elsafty,
author = {Elsafty, Ahmed and Abel, Fabian},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Elsafty, Abel - Unknown - Document Similarity using Dense Vector Representation In cooperation with XING AG Data Science.pdf:pdf},
title = {{Document Similarity using Dense Vector Representation In cooperation with: XING AG Data Science}}
}
@misc{nordberg2020crucial,
author = {Nordberg, Gustav and Grandien, Jesper},
title = {{The crucial parts of text classification with TensorFlow. js and categorisation of news articles}},
year = {2020}
}
@techreport{Barcaroli2014,
abstract = {The Istat sampling survey on "ICT in enterprises" aims at producing information on the use of ICT and in particular on the use of Internet by Italian enterprises for various purposes (e-commerce, e-recruitment, advertisement, e-tendering, e-procurement, e-government). To such a scope, data are collected by means of the traditional instrument of the questionnaire. Istat began to explore the possibility to use web scraping techniques, associated, in the estimation phase, to text and data mining algorithms, with the aim to substitute traditional instruments of data collection and estimation, or to combine them in an integrated approach. The 8,600 websites, indicated by the 19,000 enterprises responding to ICT survey of year 2013, have been "scraped" and the acquired texts have been processed in order to try to reproduce the same information collected via questionnaire. Preliminary results are encouraging, showing in some cases a satisfactory predictive capability of fitted models (mainly those obtained by using the Na{\"{i}}ve Bayes algorithm). Also the method known as Content Analysis has been applied, and its results compared to those obtained with classical learners. In order to improve the overall performance, an advanced system for scraping and mining is being adopted, based on the open source Apache suite Nutch-Solr-Lucene. On the basis of the final results of this test, an integrated system harnessing both survey data and data from the Internet to produce the required estimates will be implemented, based on systematic scraping of the near 100,000 websites related to the whole population of Italian enterprises with 10 persons employed and more, operating in industry and services. This new approach, based on "Internet as Data source (IaD)", is characterized by advantages and drawbacks that need to be carefully analysed.},
author = {Barcaroli, Giulio and Nurra, Alessandra and Scarn{\`{o}}, Marco and Summa, Donato},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Barcaroli et al. - 2014 - Use of web scraping and text mining techniques in the Istat survey on Information and Communication Technology.pdf:pdf},
keywords = {Big Data,Internet as Data source,data mining,text mining,web mining,web scraping},
title = {{Use of web scraping and text mining techniques in the Istat survey on "Information and Communication Technology in enterprises" Using Big Data for Offical Statistics View project ICT and digitalization View project Use of web scraping and text mining tech}},
url = {https://www.researchgate.net/publication/263921951},
year = {2014}
}
@article{glenski2017fishing,
author = {Glenski, Maria and Ayton, Ellyn and Arendt, Dustin and Volkova, Svitlana},
journal = {arXiv preprint arXiv:1710.06390},
title = {{Fishing for clickbaits in social images and texts with linguistically-infused neural network models}},
year = {2017}
}
@article{Kaur2020a,
abstract = {Clickbait indicates the type of content with an intending goal to attract the attention of readers. It has grown to become a nuisance to social media users. The purpose of clickbait is to bring an appealing link in front of users. Clickbaits seen in the form of headlines influence people to get attracted and curious to read the inside content. The content seen in the form of text on clickbait posts is very short to identify its features as clickbait. In this paper, a novel approach (two-phase hybrid CNN-LSTM Biterm model) has been proposed for modeling short topic content. The hybrid CNN-LSTM model when implemented with pre-trained GloVe embedding yields the best results based on accuracy, recall, precision, and F1-score performance metrics. The proposed model achieves 91.24{\%}, 95.64{\%}, 95.87{\%} precision values for Dataset 1, Dataset 2 and Dataset 3, respectively. Eight types of clickbait such as Reasoning, Number, Reaction, Revealing, Shocking/Unbelievable, Hypothesis/Guess, Questionable, Forward referencing are classified in this work using the Biterm Topic Model (BTM). It has been shown that the clickbaits such as Shocking/Unbelievable, Hypothesis/Guess and Reaction are the highest in numbers among rest of the clickbait headlines published online. Also, a ground dataset of non-textual (image-based) data using multiple social media platforms has been created in this paper. The textual information has been retrieved from the images with the help of OCR tool. A comparative study is performed to show the effectiveness of our proposed model which helps to identify the various categories of clickbait headlines that are spread on social media platforms.},
author = {Kaur, Sawinder and Kumar, Parteek and Kumaraguru, Ponnurangam},
doi = {10.1016/j.eswa.2020.113350},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Classifier,Clickbait,Features,News,Social media},
month = {aug},
pages = {113350},
publisher = {Elsevier Ltd},
title = {{Detecting clickbaits using two-phase hybrid CNN-LSTM biterm model}},
volume = {151},
year = {2020}
}
@article{Lai2014,
author = {Lai, Linda and Farbrot, Audun},
doi = {10.1080/15534510.2013.847859},
issn = {1553-4529},
journal = {Social Influence},
keywords = {Advertising,Computer-mediated communication,Internet,Question headlines,Readership},
title = {{Social Influence What makes you click? The effect of question headlines on readership in computer-mediated communication}},
url = {https://www.tandfonline.com/action/journalInformation?journalCode=psif20},
volume = {9},
year = {2014}
}
@techreport{Otter2019,
abstract = {Over the last several years, the field of natural language processing has been propelled forward by an explosion in the use of deep learning models. This survey provides a brief introduction to the field and a quick overview of deep learning architectures and methods. It then sifts through the plethora of recent studies and summarizes a large assortment of relevant contributions. Analyzed research areas include several core linguistic processing issues in addition to a number of applications of computational linguistics. A discussion of the current state of the art is then provided along with recommendations for future research in the field. Index Terms-deep learning, neural networks, natural language processing, computational linguistics, machine learning},
archivePrefix = {arXiv},
arxivId = {1807.10854v3},
author = {Otter, Daniel W and Medina, Julian R and Kalita, Jugal K},
booktitle = {IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS},
eprint = {1807.10854v3},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Otter, Medina, Kalita - 2019 - A Survey of the Usages of Deep Learning for Natural Language Processing.pdf:pdf},
title = {{A Survey of the Usages of Deep Learning for Natural Language Processing}},
volume = {XX},
year = {2019}
}
@article{roberts2018magenta,
author = {Roberts, Adam and Hawthorne, Curtis and Simon, Ian},
title = {{Magenta. js: A javascript api for augmenting creativity with deep learning}},
year = {2018}
}
@book{Taylor2017,
author = {Taylor, Michael},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Taylor - 2017 - The Math of Neural Networks.pdf:pdf},
title = {{The Math of Neural Networks}},
year = {2017}
}
@article{FrancoisChollet2017,
abstract = {Summary Deep Learning with Python introduces the field of deep learning using the Python language and the powerful Keras library. Written by Keras creator and Google AI researcher Fran{\c{c}}ois Chollet, this book builds your understanding through intuitive explanations and practical examples. Purchase of the print book includes a free eBook in PDF, Kindle, and ePub formats from Manning Publications. About the Technology Machine learning has made remarkable progress in recent years. We went from near-unusable speech and image recognition, to near-human accuracy. We went from machines that couldn't beat a serious Go player, to defeating a world champion. Behind this progress is deep learning--a combination of engineering advances, best practices, and theory that enables a wealth of previously impossible smart applications. About the Book Deep Learning with Python introduces the field of deep learning using the Python language and the powerful Keras library. Written by Keras creator and Google AI researcher Fran{\c{c}}ois Chollet, this book builds your understanding through intuitive explanations and practical examples. You'll explore challenging concepts and practice with applications in computer vision, natural-language processing, and generative models. By the time you finish, you'll have the knowledge and hands-on skills to apply deep learning in your own projects. What's Inside Deep learning from first principles Setting up your own deep-learning environment Image-classification models Deep learning for text and sequences Neural style transfer, text generation, and image generation About the Reader Readers need intermediate Python skills. No previous experience with Keras, TensorFlow, or machine learning is required. About the Author Fran{\c{c}}ois Chollet works on deep learning at Google in Mountain View, CA. He is the creator of the Keras deep-learning library, as well as a contributor to the TensorFlow machine-learning framework. He also does deep-learning research, with a focus on computer vision and the application of machine learning to formal reasoning. His papers have been published at major conferences in the field, including the Conference on Computer Vision and Pattern Recognition (CVPR), the Conference and Workshop on Neural Information Processing Systems (NIPS), the International Conference on Learning Representations (ICLR), and others. Table of Contents PART 1 - FUNDAMENTALS OF DEEP LEARNING What is deep learning? Before we begin: the mathematical building blocks of neural networks Getting started with neural networks Fundamentals of machine learning PART 2 - DEEP LEARNING IN PRACTICE Deep learning for computer vision Deep learning for text and sequences Advanced deep-learning best practices Generative deep learning Conclusions appendix A - Installing Keras and its dependencies on Ubuntu appendix B - Running Jupyter notebooks on an EC2 GPU instance},
author = {{Fran{\c{c}}ois Chollet}},
journal = {Deep Learning with Python},
title = {{Deep Learning with Python}},
year = {2017}
}
@article{StanfordUniversityCoursecs231n2018,
abstract = {Convolutional Neural Networks are very similar to ordinary Neural Networks from the previous chapter: they are made up of neurons that have learnable weights and biases. Each neuron receives some inputs, performs a dot product and optionally follows it with a non-linearity. The whole network still expresses a single differentiable score function: from the raw image pixels on one end to class scores at the other. And they still have a loss function (e.g. SVM/Softmax) on the last (fully-connected) layer and all the tips/tricks we developed for learning regular Neural Networks still apply. So what does change? ConvNet architectures make the explicit assumption that the inputs are images, which allows us to encode certain properties into the architecture. These then make the forward function more efficient to implement and vastly reduce the amount of parameters in the network.},
author = {{Stanford University Course cs231n}},
journal = {Stanford University Course cs231n},
pages = {30},
title = {{CS231n Convolutional Neural Networks for Visual Recognition}},
url = {https://cs231n.github.io/neural-networks-3/ http://cs231n.github.io/convolutional-networks/{\%}0Ahttp://cs231n.github.io/neural-networks-3/},
year = {2018}
}
@article{Werbos1990,
abstract = {Backpropagation is now the most widely used tool in the field of artificial neural networks. At the core of backpropagation is a method for calculating derivatives exactly and efficiently in any large system made up of elementary subsystems or calculations which are represented by known, differentiable functions; thus, backpropagation has many applications which do not involve neural networks as such. This paper first reviews basic backpropagation, a simple method which is now being widely used in areas like pattern recognition and fault diagnosis. Next, it presents the basic equations for back-propagation through time, and discusses applications to areas like pattern recognition involving dynamic systems, systems identification, and control. Finally, it describes further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations or true recurrent networks, and other practical issues which arise with this method. Pseudocode is provided to clarify the algorithms. The chain rule forordered derivatives—the theorem which underlies backpropagation—is briefly discussed. {\textcopyright} 1990, IEEE},
author = {Werbos, Paul J.},
doi = {10.1109/5.58337},
issn = {15582256},
journal = {Proceedings of the IEEE},
number = {10},
pages = {1550--1560},
title = {{Backpropagation Through Time: What It Does and How to Do It}},
volume = {78},
year = {1990}
}
@article{Blom2015,
abstract = {This is why you should read this article. Although such an opening statement does not make much sense read in isolation, journalists often write headlines like this on news websites. They use the forward-referring technique as a stylistic and narrative luring device trying to induce anticipation and curiosity so the readers click (or tap on) the headline and read on. In this article, we map the use of forward-referring headlines in online news journalism by conducting an analysis of 100,000 headlines from 10 different Danish news websites. The results show that commercialization and tabloidization seem to lead to a recurrent use of forward-reference in Danish online news headlines. In addition, the article contributes to reference theory by expanding previous models on phoricity to include multimodal references on the web.},
author = {Blom, Jonas Nygaard and Hansen, Kenneth Reinecke},
doi = {10.1016/j.pragma.2014.11.010},
issn = {03782166},
journal = {Journal of Pragmatics},
keywords = {Cataphora,Discourse deixis,Forward-reference,Media commercialization,Online news headlines,Tabloidization},
month = {jan},
pages = {87--100},
publisher = {Elsevier},
title = {{Click bait: Forward-reference as lure in online news headlines}},
volume = {76},
year = {2015}
}
@techreport{Almeida,
abstract = {This work lists and describes the main recent strategies for building fixed-length, dense and distributed representations for words, based on the distributional hypothesis. These representations are now commonly called word embeddings and, in addition to encoding surprisingly good syntactic and semantic information, have been proven useful as extra features in many downstream NLP tasks.},
archivePrefix = {arXiv},
arxivId = {1901.09069v1},
author = {Almeida, Felipe and Xex{\'{e}}o, Geraldo},
eprint = {1901.09069v1},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Almeida, Xex{\'{e}}o - Unknown - Word Embeddings A Survey.pdf:pdf},
title = {{Word Embeddings: A Survey}}
}
@inproceedings{zhang2015character,
author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
booktitle = {Advances in neural information processing systems},
pages = {649--657},
title = {{Character-level convolutional networks for text classification}},
year = {2015}
}
@misc{hackernoon,
title = {{Gradient Descent: All You Need to Know | Hacker Noon}},
url = {https://hackernoon.com/gradient-descent-aynk-7cbe95a778da},
urldate = {2020-12-24}
}
@techreport{Srivastava2014,
abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Salakhutdinov, Ruslan},
booktitle = {Journal of Machine Learning Research},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Srivastava et al. - 2014 - Dropout A Simple Way to Prevent Neural Networks from Overfitting.pdf:pdf},
keywords = {deep learning,model combination,neural networks,regularization},
pages = {1929--1958},
title = {{Dropout: A Simple Way to Prevent Neural Networks from Overfitting}},
volume = {15},
year = {2014}
}
@inproceedings{severyn2015unitn,
author = {Severyn, Aliaksei and Moschitti, Alessandro},
booktitle = {Proceedings of the 9th international workshop on semantic evaluation (SemEval 2015)},
pages = {464--469},
title = {{Unitn: Training deep convolutional neural network for twitter sentiment classification}},
year = {2015}
}
@article{gairola2017neural,
author = {Gairola, Siddhartha and Lal, Yash Kumar and Kumar, Vaibhav and Khattar, Dhruv},
journal = {arXiv preprint arXiv:1710.01507},
title = {{A neural clickbait detection engine}},
year = {2017}
}
@techreport{Thomas,
abstract = {This paper presents the results of our participation in the Clickbait Detection Challenge 2017. The system relies on a fusion of neural networks, incorporating different types of available informations. It does not require any linguistic preprocessing, and hence generalizes more easily to new domains and languages. The final combined model achieves a mean squared error of 0.0428, an accuracy of 0.826, and a F1 score of 0.564. According to the official evaluation metric the system ranked 6th of the 13 participating teams.},
archivePrefix = {arXiv},
arxivId = {1710.08721v1},
author = {Thomas, Philippe},
eprint = {1710.08721v1},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Thomas - Unknown - Clickbait Identification using Neural Networks The Whitebait Clickbait Detector at the Clickbait Challenge 2017.pdf:pdf},
title = {{Clickbait Identification using Neural Networks The Whitebait Clickbait Detector at the Clickbait Challenge 2017}}
}
@inproceedings{Ertam2019,
abstract = {It is a huge repository of information used to meet the many needs of Internet users. The collection and processing of data in some internet sites in this information repository has become very important nowadays. In this study, categorical news headlines and summaries in a Turkish news agency site were collected by using web scraping methods and test data were classified by using 'one hot encoding' method with vector learning methods and depth learning methods. A classification accuracy of 90{\%} has been achieved.},
author = {Ertam, Fatih},
booktitle = {2018 International Conference on Artificial Intelligence and Data Processing, IDAP 2018},
doi = {10.1109/IDAP.2018.8620790},
isbn = {9781538668788},
keywords = {Web scraping,deep learning,information extraction,machine learning,text classification},
month = {jan},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Deep learning based text classification with Web Scraping methods}},
year = {2019}
}
@book{And,
author = {Trask, Andrew. W.},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Trask - Unknown - Grokking Deep Learning.pdf:pdf},
isbn = {9781617293702},
title = {{Grokking Deep Learning}}
}
@techreport{Liddy,
author = {Liddy, Elizabeth D},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Liddy - Unknown - SURFACE SURFACE Center for Natural Language Processing School of Information Studies (iSchool) 2001 Natural Language P.pdf:pdf},
title = {{SURFACE SURFACE Center for Natural Language Processing School of Information Studies (iSchool) 2001 Natural Language Processing Natural Language Processing Natural Language Processing 1}},
url = {https://surface.syr.edu/cnlp}
}
@incollection{Neapolitan2018,
abstract = {Neural Networks and Deep Learning is a free online book. The book will teach you about: Neural networks, a beautiful biologically-inspired programming paradigm which enables a computer to learn from observational data Deep learning, a powerful set of techniques for learning in neural networks Neural networks and deep learning currently provide the best solutions to many problems in image recognition, speech recognition, and natural language processing. This book will teach you many of the core concepts behind neural networks and deep learning.},
author = {Neapolitan, Richard E. and Neapolitan, Richard E.},
booktitle = {Artificial Intelligence},
doi = {10.1201/b22400-15},
title = {{Neural Networks and Deep Learning}},
year = {2018}
}
@techreport{Hinton,
abstract = {We show how to use "complementary priors" to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associa-tive memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind.},
author = {Hinton, Geoffrey E and Osindero, Simon},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Hinton, Osindero - Unknown - A Fast Learning Algorithm for Deep Belief Nets Yee-Whye Teh.pdf:pdf},
title = {{A Fast Learning Algorithm for Deep Belief Nets Yee-Whye Teh}}
}
@article{Tenenboim2015,
abstract = {This study examined the relationship between two mechanisms of online participation-clicking and commenting-as well as the characteristics of heavily clicked versus highly commented-upon news items. Based on 15,431 items from a popular Israeli website, correlations between clicking and commenting were calculated for 12 separately analysed months from 2006 to 2011. In addition, overlap rates were determined, showing that 40-59{\%} of the heavily clicked items in any given month were different from the highly commented-upon items. A subsequent content analysis indicated that while sensational topics and curiosity-arousing elements were more prominent among the heavily clicked items than among the highly commented-upon items, political/social topics and controversial elements were more prominent among the highly commented-upon items. The study contributes to deepening our understanding of the role of user comments in constructing social/group identity and offers a new perspective on a prolonged controversy surrounding audiences' news preferences.},
author = {Tenenboim, Ori and Cohen, Akiba A},
doi = {10.1177/1464884913513996},
journal = {Journalism},
keywords = {Israel,Online journalism,audience,interactive,ratings,user comments},
number = {2},
pages = {198--217},
title = {{What prompts users to click and comment: A longitudinal study of online news}},
volume = {16},
year = {2015}
}
@article{korde2012text,
author = {Korde, Vandana and Mahender, C Namrata},
journal = {International Journal of Artificial Intelligence {\&} Applications},
number = {2},
pages = {85},
publisher = {Academy {\&} Industry Research Collaboration Center (AIRCC)},
title = {{Text classification and classifiers: A survey}},
volume = {3},
year = {2012}
}
@techreport{Nordberg2020,
author = {Nordberg, Gustav and Grandien, Jesper},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Nordberg, Grandien - 2020 - The crucial parts of text classification with TensorFlow.js and categorisation of news articles.pdf:pdf},
isbn = {4645538500},
title = {{The crucial parts of text classification with TensorFlow.js and categorisation of news articles}},
url = {www.bth.se},
year = {2020}
}
@techreport{Bengio2003,
abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
author = {Bengio, Yoshua and Ducharme, R{\'{e}}jean and Vincent, Pascal and Jauvin, Christian and Ca, Jauvinc@iro Umontreal and Kandola, Jaz and Hofmann, Thomas and Poggio, Tomaso and Shawe-Taylor, John},
booktitle = {Journal of Machine Learning Research},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Bengio et al. - 2003 - A Neural Probabilistic Language Model.pdf:pdf},
keywords = {Statistical language modeling,artificial neural networks,curse of dimensionality,distributed representation},
pages = {1137--1155},
title = {{A Neural Probabilistic Language Model}},
volume = {3},
year = {2003}
}
@inproceedings{chawda2019novel,
author = {Chawda, Sarjak and Patil, Aditi and Singh, Abhishek and Save, Ashwini},
booktitle = {2019 3rd International Conference on Trends in Electronics and Informatics (ICOEI)},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Chawda et al. - 2019 - A Novel Approach for Clickbait Detection.pdf:pdf},
organization = {IEEE},
pages = {1318--1321},
title = {{A Novel Approach for Clickbait Detection}},
year = {2019}
}
@book{Lane2019,
abstract = {{\{}$\backslash$textquotedbl{\}}Recent advances in deep learning empower applications to understand text and speech with extreme accuracy. The result? Chatbots that can imitate real people, meaningful resume-to-job matches, superb predictive search, and automatically generated document summaries-- all at a low cost. New techniques, along with accessible tools like Keras and TensorFlow, make professional-quality NLP easier than ever before. {\{}$\backslash$textquotedbl{\}}Natural language processing in action{\{}$\backslash$textquotedbl{\}} is your guide to building machines that can read and interpret human language. In it, you'll use readily available Python packages to capture the meaning in text and react accordingly. The book expands traditional NLP approaches to include neural networks, modern deep learning algorithms, and generative techniques as you tackle real-world problems like extracting dates and names, composing text, and answering free-form questions.{\{}$\backslash$textquotedbl{\}} -- Provided by publisher},
author = {Lane, Hobson and Howard, Cole and Hapke, Hannes Max},
file = {:Users/ugurtigu/Downloads/Natural Language Processing in Action by Hobson Lane,Cole Howard,Hannes Hapke (z-lib.org).pdf:pdf},
isbn = {9781617294631},
pages = {139--140},
title = {{Natural Language Processing in Action(Understanding,analyzing, and generating text with python)}},
year = {2019}
}
@techreport{Schmidhuber2014,
abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning {\&} evolutionary computation, and indirect search for short programs encoding deep and large networks. LATEX source: Preface This is the preprint of an invited Deep Learning (DL) overview. One of its goals is to assign credit to those who contributed to the present state of the art. I acknowledge the limitations of attempting to achieve this goal. The DL research community itself may be viewed as a continually evolving, deep network of scientists who have influenced each other in complex ways. Starting from recent DL results, I tried to trace back the origins of relevant ideas through the past half century and beyond, sometimes using "local search" to follow citations of citations backwards in time. Since not all DL publications properly acknowledge earlier relevant work, additional global search strategies were employed , aided by consulting numerous neural network experts. As a result, the present preprint mostly consists of references. Nevertheless, through an expert selection bias I may have missed important work. A related bias was surely introduced by my special familiarity with the work of my own DL research group in the past quarter-century. For these reasons, this work should be viewed as merely a snapshot of an ongoing credit assignment process. To help improve it, please do not hesitate to send corrections and suggestions to juergen@idsia.ch.},
archivePrefix = {arXiv},
arxivId = {1404.7828v4},
author = {Schmidhuber, J{\"{u}}rgen},
eprint = {1404.7828v4},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Schmidhuber - 2014 - Deep Learning in Neural Networks An Overview.pdf:pdf},
title = {{Deep Learning in Neural Networks: An Overview}},
url = {http://www.idsia.ch/˜juergen/DeepLearning8Oct2014.texCompleteBIBTEXfile},
year = {2014}
}
@inproceedings{vorakitphan2018clickbait,
author = {Vorakitphan, Vorakit and Leu, Fang-Yie and Fan, Yao-Chung},
booktitle = {International Conference on Innovative Mobile and Internet Services in Ubiquitous Computing},
organization = {Springer},
pages = {557--564},
title = {{Clickbait detection based on word embedding models}},
year = {2018}
}
@techreport{Pujahari,
abstract = {Clickbaits are online articles with deliberately designed misleading titles for luring more and more readers to open the intended web page. Clickbaits are used to tempted visitors to click on a particular link either to monetize the landing page or to spread the false news for sensationalization. The presence of clickbaits on any news aggregator portal may lead to unpleasant experience to readers. Automatic detection of clickbait headlines from news headlines has been a challenging issue for the machine learning community. A lot of methods have been proposed for preventing clickbait articles in recent past. However, the recent techniques available in detecting clickbaits are not much robust. This paper proposes a hybrid categorization technique for separating clickbait and non-clickbait articles by integrating different features, sentence structure, and clustering. During preliminary categorization, the headlines are separated using eleven features. After that, the headlines are recategorized using sentence formality, syntactic similarity measures. In the last phase, the headlines are again recategorized by applying clustering using word vector similarity based on t-Stochastic Neighbourhood Embedding (t-SNE) approach. After categorization of these headlines, machine learning models are applied to the data set to evaluate machine learning algorithms. The obtained experimental results indicate the proposed hybrid model is more robust, reliable and efficient than any individual categorization techniques for the real-world dataset we used.},
author = {Pujahari, Abinash and {Singh Sisodia}, Dilip},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Pujahari, Singh Sisodia - Unknown - Clickbait Detection using Multiple Categorization Techniques.pdf:pdf},
keywords = {Classification,Clickbait,Clustering,Sentence Structure,Word Vector},
title = {{Clickbait Detection using Multiple Categorization Techniques}},
url = {www.clickbait-challenge.org}
}
@techreport{Collobert2011,
abstract = {We propose a unified neural network architecture and learning algorithm that can be applied to various natural language processing tasks including part-of-speech tagging, chunking, named entity recognition, and semantic role labeling. This versatility is achieved by trying to avoid task-specific engineering and therefore disregarding a lot of prior knowledge. Instead of exploiting man-made input features carefully optimized for each task, our system learns internal representations on the basis of vast amounts of mostly unlabeled training data. This work is then used as a basis for building a freely available tagging system with good performance and minimal computational requirements .},
author = {Collobert, Ronan and Weston, Jason and Com, Jweston@google and Karlen, Michael and Kavukcuoglu, Koray and Kuksa, Pavel},
booktitle = {Journal of Machine Learning Research},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Collobert et al. - 2011 - Natural Language Processing (Almost) from Scratch.pdf:pdf},
keywords = {natural language processing,neural networks},
pages = {2493--2537},
title = {{Natural Language Processing (Almost) from Scratch}},
volume = {12},
year = {2011}
}
@techreport{Anand2019,
abstract = {Online content publishers often use catchy headlines for their articles in order to attract users to their websites. These headlines, popularly known as clickbaits, exploit a user's curiosity gap and lure them to click on links that often disappoint them. Existing methods for automatically detecting clickbaits rely on heavy feature engineering and domain knowledge. Here, we introduce a neural network architecture based on Recurrent Neural Networks for detecting clickbaits. Our model relies on distributed word representations learned from a large unannotated corpora , and character embeddings learned via Convolutional Neural Networks. Experimental results on a dataset of news headlines show that our model outperforms existing techniques for clickbait detection with an accuracy of 0.98 with F1-score of 0.98 and ROC-AUC of 0.99.},
archivePrefix = {arXiv},
arxivId = {1612.01340v2},
author = {Anand, Ankesh and Chakraborty, Tanmoy and Park, Noseong},
eprint = {1612.01340v2},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Anand, Chakraborty, Park - 2019 - We used Neural Networks to Detect Clickbaits You won't believe what happened Next!.pdf:pdf},
keywords = {Clickbait Detection,Deep Learning,Neural Networks},
title = {{We used Neural Networks to Detect Clickbaits: You won't believe what happened Next!}},
year = {2019}
}
@article{zhao2019speech,
author = {Zhao, Jianfeng and Mao, Xia and Chen, Lijiang},
journal = {Biomedical Signal Processing and Control},
pages = {312--323},
publisher = {Elsevier},
title = {{Speech emotion recognition using deep 1D {\&} 2D CNN LSTM networks}},
volume = {47},
year = {2019}
}
@techreport{Glenski,
abstract = {This paper presents the results and conclusions of our participation in the Clickbait Challenge 2017 1 on automatic clickbait detection in social media. We first describe linguistically-infused neural network models and identify informative representations to predict the level of clickbaiting present in Twitter posts. Our models allow to answer the question not only whether a post is a clickbait or not, but to what extent it is a clickbait post e.g., not at all, slightly, considerably, or heavily clickbaity using a score ranging from 0 to 1. We evaluate the predictive power of models trained on varied text and image representations extracted from tweets. Our best performing model that relies on the tweet text and linguistic markers of biased language extracted from the tweet and the corresponding page yields mean squared error (MSE) of 0.04, mean absolute error (MAE) of 0.16 and R2 of 0.43 on the held-out test data. For the binary classification setup (clickbait vs. non-clickbait), our model achieved F1 score of 0.69. We have not found that image representations combined with text yield significant performance improvement yet. Nevertheless, this work is the first to present preliminary analysis of objects extracted using Google Tensorflow object detection API from images in clickbait vs. non-clickbait Twitter posts. Finally, we outline several steps to improve model performance as a part of the future work.},
archivePrefix = {arXiv},
arxivId = {1710.06390v1},
author = {Glenski, Maria and Ayton, Ellyn and Arendt, Dustin and Volkova, Svitlana},
eprint = {1710.06390v1},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Glenski et al. - Unknown - Fishing for Clickbaits in Social Images and Texts with Linguistically-Infused Neural Network Models The Pinea.pdf:pdf},
title = {{Fishing for Clickbaits in Social Images and Texts with Linguistically-Infused Neural Network Models The Pineapplefish Clickbait Detector at the Clickbait Challenge 2017}},
url = {http://www.clickbait-challenge.org/}
}
@techreport{Mikolov2013,
abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
archivePrefix = {arXiv},
arxivId = {1310.4546v1},
author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
eprint = {1310.4546v1},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases and their Compositionality.pdf:pdf},
keywords = {()},
title = {{Distributed Representations of Words and Phrases and their Compositionality}},
year = {2013}
}
@techreport{Potthast,
abstract = {Clickbait has grown to become a nuisance to social media users and social media operators alike. Malicious content publishers misuse social media to manipulate as many users as possible to visit their websites using clickbait messages. Machine learning technology may help to handle this problem, giving rise to automatic clickbait detection. To accelerate progress in this direction, we organized the Clickbait Challenge 2017, a shared task inviting the submission of clickbait detectors for a comparative evaluation. A total of 13 detectors have been submitted, achieving significant improvements over the previous state of the art in terms of detection performance. Also, many of the submitted approaches have been published open source, rendering them reproducible, and a good starting point for newcomers. While the 2017 challenge has passed, we maintain the evaluation system and answer to new registrations in support of the ongoing research on better clickbait detectors.},
archivePrefix = {arXiv},
arxivId = {1812.10847v1},
author = {Potthast, Martin and Gollub, Tim and Hagen, Matthias and Stein, Benno},
eprint = {1812.10847v1},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Potthast et al. - Unknown - The Clickbait Challenge 2017 Towards a Regression Model for Clickbait Strength.pdf:pdf},
title = {{The Clickbait Challenge 2017: Towards a Regression Model for Clickbait Strength}},
url = {https://clickbait-challenge.org}
}
@inproceedings{Park2017,
abstract = {Phishers often exploit users' trust on the appearance of a site by using webpages that are visually similar to an authentic site. In the past, various research studies have tried to identify and classify the factors contributing towards the detection of phishing websites. The focus of this research is to establish a strong relationship between those identified heuristics (content-based) and the legitimacy of a website by analyzing training sets of websites (both phishing and legitimate websites) and in the process analyze new patterns and report findings. Many existing phishing detection tools are often not very accurate as they depend mostly on the old database of previously identified phishing websites. However, there are thousands of new phishing websites appearing every year targeting financial institutions, cloud storage/file hosting sites, government websites, and others. This paper presents a framework called Phishing-Detective that detects phishing websites based on existing and newly found heuristics. For this framework, a web crawler was developed to scrape the contents of phishing and legitimate websites. These contents were analyzed to rate the heuristics and their contribution scale factor towards the illegitimacy of a website. The data set collected from Web Scraper was then analyzed using a data mining tool to find patterns and report findings. A case study shows how this framework can be used to detect a phishing website. This research is still in progress but shows a new way of finding and using heuristics and the sum of their contributing weights to effectively and accurately detect phishing websites. Further development of this framework is discussed at the end of the paper.},
author = {Park, Andrew J. and Quadari, Ruhi Naaz and Tsang, Herbert H.},
booktitle = {2017 8th IEEE Annual Information Technology, Electronics and Mobile Communication Conference, IEMCON 2017},
doi = {10.1109/IEMCON.2017.8117212},
isbn = {9781538633717},
keywords = {Phishing detection,heuristic weights,web crawler},
month = {nov},
pages = {680--684},
publisher = {Institute of Electrical and Electronics Engineers Inc.},
title = {{Phishing website detection framework through web scraping and data mining}},
year = {2017}
}
@article{seopredicting,
author = {Seo, Jae Duk},
title = {{Predicting Keyword Popularity using Tensorflow JS}}
}
@article{smilkov2019tensorflow,
author = {Smilkov, Daniel and Thorat, Nikhil and Assogba, Yannick and Yuan, Ann and Kreeger, Nick and Yu, Ping and Zhang, Kangyi and Cai, Shanqing and Nielsen, Eric and Soergel, David and Others},
journal = {arXiv preprint arXiv:1901.05350},
title = {{Tensorflow. js: Machine learning for the web and beyond}},
year = {2019}
}
@misc{Taylor2017a,
author = {Taylor, Michael},
title = {{The Math of Neural Networks}},
url = {https://dl.acm.org/doi/book/10.5555/3181144 http://himarsh.org/the-math-neural-networks/},
urldate = {2020-12-08},
year = {2017}
}
@book{Aggarwal2018,
abstract = {This book covers both classical and modern models in deep learning. The primary focus is on the theory and algorithms of deep learning. The theory and algorithms of neural networks are particularly important for understanding important concepts, so that one can understand the important design concepts of neural architectures in different applications. Why do neural networks work? When do they work better than off-the-shelf machine-learning models? When is depth useful? Why is training neural networks so hard? What are the pitfalls? The book is also rich in discussing different applications in order to give the practitioner a flavor of how neural architectures are designed for different types of problems. Applications associated with many different areas like recommender systems, machine translation, image captioning, image classification, reinforcement-learning based gaming, and text analytics are covered. The chapters of this book span three categories: The basics of neural networks: Many traditional machine learning models can be understood as special cases of neural networks. An emphasis is placed in the first two chapters on understanding the relationship between traditional machine learning and neural networks. Support vector machines, linear/logistic regression, singular value decomposition, matrix factorization, and recommender systems are shown to be special cases of neural networks. These methods are studied together with recent feature engineering methods like word2vec. Fundamentals of neural networks: A detailed discussion of training and regularization is provided in Chapters 3 and 4. Chapters 5 and 6 present radial-basis function (RBF) networks and restricted Boltzmann machines. Advanced topics in neural networks: Chapters 7 and 8 discuss recurrent neural networks and convolutional neural networks. Several advanced topics like deep reinforcement learning, neural Turing machines, Kohonen self-organizing maps, and generative adversarial networks are introduced in Chapters 9 and 10. The book is written for graduate students, researchers, and practitioners. Numerous exercises are available along with a solution manual to aid in classroom teaching. Where possible, an application-centric view is highlighted in order to provide an understanding of the practical uses of each class of techniques.},
author = {Aggarwal, Charu C.},
booktitle = {Neural Networks and Deep Learning},
doi = {10.1007/978-3-319-94463-0},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Aggarwal - 2018 - Neural Networks and Deep Learning.pdf:pdf},
publisher = {Springer International Publishing},
title = {{Neural Networks and Deep Learning}},
year = {2018}
}
@inproceedings{cho2019shop,
author = {Cho, Jaeyoung and Lee, Sangwon and Chung, Tai Myoung},
booktitle = {Proceedings of the Korean Society of Computer Information Conference},
organization = {Korean Society of Computer Information},
pages = {267--270},
title = {{A shop recommendation learning with Tensorflow. js}},
year = {2019}
}
@misc{tagger,
title = {{German Tagsets | Institut f{\"{u}}r Maschinelle Sprachverarbeitung | Universit{\"{a}}t Stuttgart}},
url = {https://www.ims.uni-stuttgart.de/forschung/ressourcen/lexika/germantagsets/{\#}id-cfcbf0a7-0},
urldate = {2021-01-08}
}
@book{Bruce2013,
abstract = {Predicting the binding mode of flexible polypeptides to proteins is an important task that falls outside the domain of applicability of most small molecule and protein−protein docking tools. Here, we test the small molecule flexible ligand docking program Glide on a set of 19 non-$\alpha$-helical peptides and systematically improve pose prediction accuracy by enhancing Glide sampling for flexible polypeptides. In addition, scoring of the poses was improved by post-processing with physics-based implicit solvent MM- GBSA calculations. Using the best RMSD among the top 10 scoring poses as a metric, the success rate (RMSD ≤ 2.0 {\AA} for the interface backbone atoms) increased from 21{\%} with default Glide SP settings to 58{\%} with the enhanced peptide sampling and scoring protocol in the case of redocking to the native protein structure. This approaches the accuracy of the recently developed Rosetta FlexPepDock method (63{\%} success for these 19 peptides) while being over 100 times faster. Cross-docking was performed for a subset of cases where an unbound receptor structure was available, and in that case, 40{\%} of peptides were docked successfully. We analyze the results and find that the optimized polypeptide protocol is most accurate for extended peptides of limited size and number of formal charges, defining a domain of applicability for this approach.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Bruce, 2011},
booktitle = {Journal of Chemical Information and Modeling},
eprint = {arXiv:1011.1669v3},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Bruce - 2013 - Intro to Deep Learning.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
keywords = {icle},
number = {9},
pages = {1689--1699},
pmid = {25246403},
title = {{Intro to Deep Learning}},
volume = {53},
year = {2013}
}
@techreport{Rong2016,
abstract = {The word2vec model and application by Mikolov et al. have attracted a great amount of attention in recent two years. The vector representations of words learned by word2vec models have been shown to carry semantic meanings and are useful in various NLP tasks. As an increasing number of researchers would like to experiment with word2vec or similar techniques, I notice that there lacks a material that comprehensively explains the parameter learning process of word embedding models in details, thus preventing researchers that are non-experts in neural networks from understanding the working mechanism of such models. This note provides detailed derivations and explanations of the parameter update equations of the word2vec models, including the original continuous bag-of-word (CBOW) and skip-gram (SG) models, as well as advanced optimization techniques, including hierarchical softmax and negative sampling. Intuitive interpretations of the gradient equations are also provided alongside mathematical derivations. In the appendix, a review on the basics of neuron networks and backpropagation is provided. I also created an interactive demo, wevi, to facilitate the intuitive understanding of the model. 1 1 Continuous Bag-of-Word Model 1.1 One-word context We start from the simplest version of the continuous bag-of-word model (CBOW) introduced in Mikolov et al. (2013a). We assume that there is only one word considered per context, which means the model will predict one target word given one context word, which is like a bigram model. For readers who are new to neural networks, it is recommended that one go through Appendix A for a quick review of the important concepts and terminologies before proceeding further. Figure 1 shows the network model under the simplified context definition 2. In our setting, the vocabulary size is V , and the hidden layer size is N. The units on adjacent 1 An online interactive demo is available at: http://bit.ly/wevi-online. 2 In Figures 1, 2, 3, and the rest of this note, W is not the transpose of W, but a different matrix instead.},
archivePrefix = {arXiv},
arxivId = {1411.2738v4},
author = {Rong, Xin},
eprint = {1411.2738v4},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Rong - 2016 - word2vec Parameter Learning Explained.pdf:pdf},
title = {{word2vec Parameter Learning Explained}},
url = {http://bit.ly/wevi-online.},
year = {2016}
}
@article{kowsari2019text,
author = {Kowsari, Kamran and {Jafari Meimandi}, Kiana and Heidarysafa, Mojtaba and Mendu, Sanjana and Barnes, Laura and Brown, Donald},
journal = {Information},
number = {4},
pages = {150},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Text classification algorithms: A survey}},
volume = {10},
year = {2019}
}
@inproceedings{Vorakitphan2019,
abstract = {In recent years, social networking platform serves as a new media of news sharing and information diffusion. Social networking platform has become a part of our daily life. As such, social media advertising budgets have explosively expanded worldwide over the past few years. Due to the huge commercial interest, clickbait behaviors are commonly observed, which use attractive headlines and sensationalized textual description to bait users to visit websites. Clickbaits mainly exploit the users' curiosity's gap by interesting headlines to entice its readers to click an accompanying link to articles often with poor contents. Clickbaits are bothersome either to social media users or platform site owners. In this paper, we propose an approach called Ontology-based LSTM Model (OLSTM) to detect clickbaits. Compared with the existing solutions for clickbait detection, our approach is characterized by the following three components: word embedding model, Recurrent Neural Networks (RNN), and word ontology information. The observation is that preserving semantic relationships is significantly an important factor to be considered in detecting clickbaits. Therefore, we propose to capture semantic relationships between words by word embedding models. In addition, we adopted RNN as our classification models to consider word orders in a sentence. Furthermore, we consider the word ontology relation as another feature set for clickbait classification, as clickbaits often uses words with generalized concepts to induce curiosity. We conduct experiments with real data from Twitter and news websites to validate the effectiveness of the proposed approach, which demonstrates that the employment of the proposed method improves clickbait detection accuracy from 80{\%} to 90{\%} compared with the existing solutions.},
author = {Vorakitphan, Vorakit and Leu, Fang Yie and Fan, Yao Chung},
booktitle = {Advances in Intelligent Systems and Computing},
doi = {10.1007/978-3-319-93554-6_54},
isbn = {9783319935539},
issn = {21945357},
month = {jul},
pages = {557--564},
publisher = {Springer Verlag},
title = {{Clickbait detection based on word embedding models}},
url = {https://link.springer.com/chapter/10.1007/978-3-319-93554-6{\_}54},
volume = {773},
year = {2019}
}
@book{Patterson2019,
abstract = {"Deep learning is a form of machine learning that enables computers to learn from experience and understand the world in terms of a hierarchy of concepts. Because the computer gathers knowledge from experience, there is no need for a human computer operator to formally specify all the knowledge that the computer needs. The hierarchy of concepts allows the computer to learn complicated concepts by building them out of simpler ones; a graph of these hierarchies would be many layers deep. This book introduces a broad range of topics in deep learning. The text offers mathematical and conceptual background, covering relevant concepts in linear algebra, probability theory and information theory, numerical computation, and machine learning. It describes deep learning techniques used by practitioners in industry, including deep feedforward networks, regularization, optimization algorithms, convolutional networks, sequence modeling, and practical methodology; and it surveys such applications as natural language processing, speech recognition, computer vision, online recommendation systems, bioinformatics, and video games. Finally, the book offers research perspectives, covering such theoretical topics as linear factor models, autoencoders, representation learning, structured probabilistic models, Monte Carlo methods, the partition function, approximate inference, and deep generative models. Deep Learning can be used by undergraduate or graduate students planning careers in either industry or research, and by software engineers who want to begin using deep learning in their products or platforms. A website offers supplementary material for both readers and instructors"--Page 4 of cover. Introduction -- Applied math and machine learning basics. Linear algebra -- Probability and information theory -- Numerical computation -- Machine learning basics -- Deep networks: modern practices. Deep feedforward networks -- Regularization for deep learning -- Optimization for training deep models -- Convolutional networks -- Sequence modeling: recurrent and recursive nets -- Practical methodology -- Applications -- Deep learning research. Linear factor models -- Autoencoders -- Representation learning -- Structured probabilistic models for deep learning -- Monte Carlo methods -- Confronting the partition function -- Approximate inference -- Deep generative models.},
author = {Patterson, Josh and Gibson, Adam},
booktitle = {Nature},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Patterson, Gibson - 2019 - Deep Learning a Practitioner'S Approach.pdf:pdf},
isbn = {3463353563306},
number = {7553},
pages = {1--73},
title = {{Deep Learning a Practitioner'S Approach}},
volume = {29},
year = {2019}
}
@techreport{SutskeverGoogle2014,
abstract = {Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT-14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous state of the art. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally , we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.},
author = {{Sutskever Google}, Ilya and {Vinyals Google}, Oriol and {Le Google}, Quoc V},
booktitle = {Advances in Neural Information Processing Systems},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Sutskever Google, Vinyals Google, Le Google - 2014 - Sequence to Sequence Learning with Neural Networks.pdf:pdf},
pages = {3104--3112},
title = {{Sequence to Sequence Learning with Neural Networks}},
volume = {27},
year = {2014}
}
@article{Main2017,
abstract = {The use of tempting headlines (clickbait) to allure readers has become a growing practice nowadays. The widespread use of clickbait risks the reader's trust in media. According to a study performed by Facebook, 80{\%} users "preferred headlines that helped them decide if they wanted to read the full article before they had to click through". In this paper, we present a clickbait detection model which uses distributed subword embeddings and achieves an accuracy of 98.3{\%}. Powered with the model, we build BaitBuster, a solution framework (social bot+browser extension), which not only detects clickbaits floating on the web but also provides brief explanation behind its action. Moreover, we study 1.67 million contents created by 153 media organizations and discover the relation between clickbait usage and media reliability.},
author = {Main, Md and Rony, Uddin and Hassan, Naeemul and Yousuf, Mohammad},
doi = {10.1145/nnnnnnn.nnnnnnn},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Main et al. - 2017 - BaitBuster Destined to Save You Some Clicks.pdf:pdf},
title = {{BaitBuster: Destined to Save You Some Clicks}},
url = {https://doi.org/10.1145/nnnnnnn.nnnnnnn},
year = {2017}
}
@techreport{Chavan2019,
abstract = {Stock also known as equity or shares is a type of security that signifies ownership in a corporation and represents a claim on part of the corporations assets and earnings whereas a stock market or share market is the aggregation of buyers and sellers of stocks. Stock Exchange is defined as a process where the stock brokers can buy as well as sell the shares, bonds or other securities. Many companies regardless of their domains or sectors make their stocks or shares available through Stock Market. Such type of an exchange deals with concentration and a focused mind as the entire process is dealing with valuable assets of a person. Thus to ease the process of stock exchange many stock brokers use the act of predicting the stock prices i.e. trying to determine the whether the financial instruments of a company will go up or down in their values. Prediction is done by fundamental analysis and technical analysis and now using Machine Learning Concepts. In this project we propose a Convolutional Neural Network for predicting the stock price in order to make profit.},
author = {Chavan, Sandeep and Doshi, Harshil and Godbole, Diksha and Parge, Pranav and Gore, Deipali},
booktitle = {International Journal of Innovative Science and Research Technology},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Chavan et al. - 2019 - D Convolutional Neural Network for Stock Market Prediction using Tensorflow.js.pdf:pdf},
keywords = {Convolutional Neural Network,Deep Learning,JavaScript,Ma-chine Learning,Neural Network,Prediction/Forecasting,Stock,Stock Exchange,Stock Market,Tensorflowjs,Time Series Prediction},
number = {6},
title = {{D Convolutional Neural Network for Stock Market Prediction using Tensorflow.js}},
url = {www.ijisrt.com},
volume = {4},
year = {2019}
}
@article{Suah2017,
abstract = {A greener analytical procedure based on automated flow through system with an optical sensor is proposed for determination of Co(II). The flow through system consisted of polymer inclusion membrane (PIM) containing potassium thiocyanate (KSCN) that was placed between the measuring cell and fixed with optical sensor probe as an optical sensor for monitoring of Co(II) at 625 nm. In the presence of Co(II) ions, the colourless membrane changes to blue. The sensing membrane was prepared by incorporating SCN into a non plasticized PIM. The prepared PIM were found to be homogenous, transparent and mechanically stable. The optode shows reversible optical response in the range of 1.00 × 10−6 – 1.00 × 10−3 mol L−1 with detection limit of 6.10 × 10−7 mol L−1. The optode can be regenerated by using 0.1 mol L−1 of ethylenediaminetetraacetic acid (EDTA). The main parameters of the computer controlled flow system incorporating the flow-through optode, a multi-port selection valve and peristaltic pump were optimized too. The calculated Relative Standard Deviation (R.S.D) of the repeatability and reproducibility of the method are 0.76{\%} and 4.73{\%}, respectively. This green system has been applied to the determination of Co(II) in wastewater samples with reduced reagents and samples consumption and minimum waste generation.},
author = {Suah, Faiz Bukhari Mohd},
doi = {10.1016/j.ancr.2017.02.001},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Suah - 2017 - Preparation and characterization of a novel Co(II) optode based on polymer inclusion membrane.pdf:pdf},
issn = {22141812},
journal = {Analytical Chemistry Research},
keywords = {Aliquat 336,Cobalt(II),Flow through system,Green analytical chemistry,Optode,Polymer inclusion membrane},
pages = {40--46},
title = {{Preparation and characterization of a novel Co(II) optode based on polymer inclusion membrane}},
volume = {12},
year = {2017}
}
@techreport{Rohringer2019,
author = {Rohringer, Sophie},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Rohringer - 2019 - Mit diesem Ergebnis hat niemand gerechnet-Chancen und Grenzen von Clickbaiting im Online-Journalismus.pdf:pdf},
title = {{Mit diesem Ergebnis hat niemand gerechnet-Chancen und Grenzen von Clickbaiting im Online-Journalismus}},
year = {2019}
}
@techreport{Potthasta,
abstract = {Clickbait has become a nuisance on social media. To address the urging task of clickbait detection , we constructed a new corpus of 38,517 annotated Twitter tweets, the Webis Clickbait Corpus 2017. To avoid biases in terms of publisher and topic, tweets were sampled from the top 27 most retweeted news publishers, covering a period of 150 days. Each tweet has been annotated on 4-point scale by five annotators recruited at Amazon's Mechanical Turk. The corpus has been employed to evaluate 12 clickbait detectors submitted to the Clickbait Challenge 2017.},
author = {Potthast, Martin and Gollub, Tim and Komlossy, Kristof and Schuster, Sebastian and Wiegmann, Matti and Patricia, Erika and Fernandez, Garces and Hagen, Matthias and Stein, Benno},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Potthast et al. - Unknown - Crowdsourcing a Large Corpus of Clickbait on Twitter.pdf:pdf},
pages = {1498--1507},
title = {{Crowdsourcing a Large Corpus of Clickbait on Twitter}}
}
@inproceedings{Anand2017,
abstract = {Online content publishers often use catchy headlines for their articles in order to attract users to their websites. These headlines, popularly known as clickbaits, exploit a user's curiosity gap and lure them to click on links that often disappoint them. Existing methods for automatically detecting clickbaits rely on heavy feature engineering and domain knowledge. Here, we introduce a neural network architecture based on Recurrent Neural Networks for detecting clickbaits. Our model relies on distributed word representations learned from a large unannotated corpora, and character embeddings learned via Convolutional Neural Networks. Experimental results on a dataset of news headlines show that our model outperforms existing techniques for clickbait detection with an accuracy of 0.98 with F1-score of 0.98 and ROC-AUC of 0.99.},
archivePrefix = {arXiv},
arxivId = {1612.01340},
author = {Anand, Ankesh and Chakraborty, Tanmoy and Park, Noseong},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-319-56608-5_46},
eprint = {1612.01340},
isbn = {9783319566078},
issn = {16113349},
keywords = {Clickbait detection,Deep learning,Neural networks},
pages = {541--547},
publisher = {Springer Verlag},
title = {{We used neural networks to detect clickbaits: You won't believe what happened next!}},
url = {https://link.springer.com/chapter/10.1007/978-3-319-56608-5{\_}46},
volume = {10193 LNCS},
year = {2017}
}
@article{Ruder2016,
abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
archivePrefix = {arXiv},
arxivId = {1609.04747},
author = {Ruder, Sebastian},
eprint = {1609.04747},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Ruder - 2016 - An overview of gradient descent optimization algorithms.pdf:pdf},
month = {sep},
title = {{An overview of gradient descent optimization algorithms}},
url = {http://arxiv.org/abs/1609.04747},
year = {2016}
}
@book{Hrsg2020,
author = {Hrsg, Markus Appel},
booktitle = {Die Psychologie des Postfaktischen: {\"{U}}ber Fake News, „L{\"{u}}genpresse“, Clickbait {\&} Co.},
doi = {10.1007/978-3-662-58695-2},
file = {:Users/ugurtigu/Desktop/Die Psychologie des Postfaktischen {\"{U}}ber Fake News, „L{\"{u}}genpresse“, Clickbait Co. by Markus Appel (z-lib.org).pdf:pdf},
isbn = {9783662586945},
title = {{Die Psychologie des Postfaktischen: {\"{U}}ber Fake News, „L{\"{u}}genpresse“, Clickbait {\&} Co.}},
year = {2020}
}
@misc{StanfordInte,
title = {{Unsupervised Feature Learning and Deep Learning Tutorial}},
url = {http://deeplearning.stanford.edu/tutorial/supervised/FeatureExtractionUsingConvolution/},
urldate = {2020-12-14}
}
@book{Moolayil2019a,
abstract = {Place of publication from publisher website. What's Next for DL Expertise? Learn, understand, and implement deep neural networks in a math- and programming-friendly approach using Keras and Python. The book focuses on an end-to-end approach to developing supervised learning algorithms in regression and classification with practical business-centric use-cases implemented in Keras. The overall book comprises three sections with two chapters in each section. The first section prepares you with all the necessary basics to get started in deep learning. Chapter 1 introduces you to the world of deep learning and its difference from machine learning, the choices of frameworks for deep learning, and the Keras ecosystem. You will cover a real-life business problem that can be solved by supervised learning algorithms with deep neural networks. You'll tackle one use case for regression and another for classification leveraging popular Kaggle datasets. Later, you will see an interesting and challenging part of deep learning: hyperparameter tuning; helping you further improve your models when building robust deep learning applications. Finally, you'll further hone your skills in deep learning and cover areas of active development and research in deep learning. At the end of Learn Keras for Deep Neural Networks, you will have a thorough understanding of deep learning principles and have practical hands-on experience in developing enterprise-grade deep learning solutions in Keras. You will: Master fast-paced practical deep learning concepts with math- and programming-friendly abstractions. Design, develop, train, validate, and deploy deep neural networks using the Keras framework Use best practices for debugging and validating deep learning models Deploy and integrate deep learning as a service into a larger software service or product Extend deep learning principles into other popular frameworks. Intro; Table of Contents; About the Author; About the Technical Reviewer; Acknowledgments; Introduction; Chapter 1: An Introduction to Deep Learning and Keras; Introduction to DL; Demystifying the Buzzwords; What Are Some Classic Problems Solved by DL in Today's Market?; Decomposing a DL Model; Exploring the Popular DL Frameworks; Low-Level DL Frameworks; Theano; Torch; PyTorch; MxNet; TensorFlow; High-Level DL Frameworks; A Sneak Peek into the Keras Framework; Getting the Data Ready; Defining the Model Structure; Training the Model and Making Predictions; Summary; Chapter 2: Keras in Action Setting Up the EnvironmentSelecting the Python Version; Installing Python for Windows, Linux, or macOS; Installing Keras and TensorFlow Back End; Getting Started with DL in Keras; Input Data; Neuron; Activation Function; Sigmoid Activation Function; ReLU Activation Function; Model; Layers; Core Layers; Dense Layer; Dropout Layer; Other Important Layers; The Loss Function; Optimizers; Stochastic Gradient Descent (SGD); Adam; Other Important Optimizers; Metrics; Model Configuration; Model Training; Model Evaluation; Putting All the Building Blocks Together; Summary Chapter 3: Deep Neural Networks for Supervised Learning: RegressionGetting Started; Problem Statement; Why Is Representing a Problem Statement with a Design Principle Important?; Designing an SCQ; Designing the Solution; Exploring the Data; Looking at the Data Dictionary; Finding Data Types; Working with Time; Predicting Sales; Exploring Numeric Columns; Understanding the Categorical Features; Data Engineering; Defining Model Baseline Performance; Designing the DNN; Testing the Model Performance; Improving the Model; Increasing the Number of Neurons; Plotting the Loss Metric Across Epochs Testing the Model ManuallySummary; Chapter 4: Deep Neural Networks for Supervised Learning: Classification; Getting Started; Problem Statement; Designing the SCQ; Designing the Solution; How Can We Identify a Potential Customer?; Exploring the Data; Data Engineering; Defining Model Baseline Accuracy; Designing the DNN for Classification; Revisiting the Data; Standardize, Normalize, or Scale the Data; Transforming the Input Data; DNNs for Classification with Improved Data; Summary; Chapter 5: Tuning and Deploying Deep Neural Networks; The Problem of Overfitting; So, What Is Regularization? L1 RegularizationL2 Regularization; Dropout Regularization; Hyperparameter Tuning; Hyperparameters in DL; Number of Neurons in a Layer; Number of Layers; Number of Epochs; Weight Initialization; Batch Size; Learning Rate; Activation Function; Optimization; Approaches for Hyperparameter Tuning; Manual Search; Grid Search; Random Search; Further Reading; Model Deployment; Tailoring the Test Data; Saving Models to Memory; Retraining the Models with New Data; Online Models; Delivering Your Model As an API; Putting All the Pieces of the Puzzle Together; Summary; Chapter 6: The Path Ahead},
author = {Moolayil, Jojo},
booktitle = {Learn Keras for Deep Neural Networks},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Moolayil - 2019 - Learn Keras for Deep Neural Networks A Fast-Track Approach to Modern Deep Learning with Python.pdf:pdf},
isbn = {978-1-4842-4239-1},
pages = {182},
title = {{Learn Keras for Deep Neural Networks: A Fast-Track Approach to Modern Deep Learning with Python}},
year = {2019}
}
@article{eren2019generic,
author = {Eren, Levent and Ince, Turker and Kiranyaz, Serkan},
journal = {Journal of Signal Processing Systems},
number = {2},
pages = {179--189},
publisher = {Springer},
title = {{A generic intelligent bearing fault diagnosis system using compact adaptive 1D CNN classifier}},
volume = {91},
year = {2019}
}
@techreport{Liao,
abstract = {Online media outlets adopt clickbait techniques to lure readers to click on articles in a bid to expand their reach and subsequently increase revenue through ad mone-tization. As the adverse effects of clickbait attract more and more attention, researchers have started to explore machine learning techniques to automatically detect click-baits. Previous work on clickbait detection assumes that all the training data is available locally during training. In many real-world applications, however, training data is generally distributedly stored by different parties (e.g., different parties maintain data with different feature spaces), and the parties cannot share their data with each other due to data privacy issues. It is challenging to build models of high-quality federally for detecting clickbaits effectively without data sharing. In this paper, we propose a fed-erated training framework, which is called federated hierarchical hybrid networks, to build clickbait detection models, where the titles and contents are stored by different parties, whose relationships must be exploited for clickbait detection. We empirically demonstrate that our approach is effective by comparing our approach to the state-of-the-art approaches using datasets from social media.},
archivePrefix = {arXiv},
arxivId = {1906.00638v1},
author = {Liao, Feng and Zhuo, Hankz Hankui and Huang, Xiaoling and Zhang, Yu},
eprint = {1906.00638v1},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Liao et al. - Unknown - Federated Hierarchical Hybrid Networks for Clickbait Detection.pdf:pdf},
title = {{Federated Hierarchical Hybrid Networks for Clickbait Detection}}
}
@misc{Miller,
author = {Miller, Steven},
title = {{Mind: How to Build a Neural Network (Part One)}},
url = {https://stevenmiller888.github.io/mind-how-to-build-a-neural-network/},
urldate = {2020-12-13}
}
@incollection{rivera2020identifying,
author = {Rivera, Juan De Dios Santos},
booktitle = {Practical TensorFlow. js},
pages = {151--162},
publisher = {Springer},
title = {{Identifying toxic text from a Google Chrome Extension}},
year = {2020}
}
@article{ChehrehChelgani2018,
abstract = {Support vector regression (SVR) modeling was used to predict the coal flotation responses (recovery (R∗) and flotation rate constant (k)) as a function of measured particle properties and hydrodynamic flotation variables. Coal flotation is a complicated multifaceted separation process and many measurable and unmeasurable variables can be considered for its modeling. Therefore, feature selection can be used to save time and cost of measuring irrelevant parameters. Mutual information (MI) as a powerful variable selection tool was used through laboratory measured variables to assess interactions and choose the most effective ones for predictions of R∗ and k. Feature selection by MI through variables indicated that the best arrangements for the R∗ and k predictions are the sets of particle Reynolds number-energy dissipation and particle size-bubble Reynolds number, respectively. Correlation of determination (R2) and difference between laboratory measured and SVR predicted values based on MI selected variables indicated that the SVR can model R∗ and k quite accurately with R2 = 0.93 and R2 = 0.72, respectively. These results demonstrated that the MI-SVR combination can quite satisfactorily measure the importance of variables, increase interpretability, reduce the risk of overfitting, decrease complexity and generate predictive models for high dimension of variables based on selected features for complicated processing systems.},
author = {{Chehreh Chelgani}, S. and Shahbazi, B. and Hadavandi, E.},
doi = {10.1016/j.measurement.2017.09.025},
file = {:Users/ugurtigu/Library/Application Support/Mendeley Desktop/Downloaded/Chehreh Chelgani, Shahbazi, Hadavandi - 2018 - Support vector regression modeling of coal flotation based on variable importance measure.pdf:pdf},
issn = {02632241},
journal = {Measurement: Journal of the International Measurement Confederation},
keywords = {Flotation rate constant,Mutual information,Recovery,Support vector regression,Variable importance measurement},
number = {2},
pages = {102--108},
title = {{Support vector regression modeling of coal flotation based on variable importance measurements by mutual information method}},
volume = {114},
year = {2018}
}
